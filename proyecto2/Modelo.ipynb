{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creaci√≥n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from packaging import version\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los datos\n",
    "df = pd.read_csv(\"./NuevoData.csv\", delimiter=',')\n",
    "\n",
    "df_mdl = df[[\"LIMIT_BAL\", \"SEX\", \"AGE\", \"_1\", \"_2\", \"_3\", \"_4\", \"MARRIAGE_1\", \"MARRIAGE_2\", \"MARRIAGE_3\", \"default payment next month\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_mdl.iloc[:, :-1] \n",
    "Y = tf.keras.utils.to_categorical(df_mdl.iloc[:, -1], num_classes=2)\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18944, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:277: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(5, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67 (268.00 Byte)\n",
      "Trainable params: 67 (268.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "592/592 [==============================] - 2s 2ms/step - loss: 121636.3359 - accuracy: 0.7748 - val_loss: 0.6231 - val_accuracy: 0.7827\n",
      "Epoch 2/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6042 - accuracy: 0.7759 - val_loss: 0.5835 - val_accuracy: 0.7827\n",
      "Epoch 3/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5751 - accuracy: 0.7759 - val_loss: 0.5607 - val_accuracy: 0.7827\n",
      "Epoch 4/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5582 - accuracy: 0.7759 - val_loss: 0.5470 - val_accuracy: 0.7827\n",
      "Epoch 5/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5483 - accuracy: 0.7759 - val_loss: 0.5388 - val_accuracy: 0.7827\n",
      "Epoch 6/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7759 - val_loss: 0.5336 - val_accuracy: 0.7827\n",
      "Epoch 7/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7759 - val_loss: 0.5303 - val_accuracy: 0.7827\n",
      "Epoch 8/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5362 - accuracy: 0.7759 - val_loss: 0.5281 - val_accuracy: 0.7827\n",
      "Epoch 9/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5347 - accuracy: 0.7759 - val_loss: 0.5267 - val_accuracy: 0.7827\n",
      "Epoch 10/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5338 - accuracy: 0.7759 - val_loss: 0.5258 - val_accuracy: 0.7827\n",
      "Epoch 11/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5332 - accuracy: 0.7759 - val_loss: 0.5251 - val_accuracy: 0.7827\n",
      "Epoch 12/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5328 - accuracy: 0.7759 - val_loss: 0.5247 - val_accuracy: 0.7827\n",
      "Epoch 13/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5326 - accuracy: 0.7759 - val_loss: 0.5244 - val_accuracy: 0.7827\n",
      "Epoch 14/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5324 - accuracy: 0.7759 - val_loss: 0.5242 - val_accuracy: 0.7827\n",
      "Epoch 15/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5323 - accuracy: 0.7759 - val_loss: 0.5240 - val_accuracy: 0.7827\n",
      "Epoch 16/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5239 - val_accuracy: 0.7827\n",
      "Epoch 17/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 18/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 19/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 20/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 21/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 22/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 23/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 24/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 25/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 26/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 27/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 28/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 29/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 30/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 31/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 32/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 33/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 34/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 35/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 36/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 37/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 38/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 39/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 40/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 41/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 42/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 43/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 44/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 45/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 46/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 47/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 48/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 49/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 50/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 51/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 52/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 53/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 54/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 55/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 56/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 57/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 58/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 59/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 60/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 61/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 62/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 63/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 64/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 65/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 66/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 67/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 68/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 69/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 70/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 71/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 72/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 73/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 74/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 75/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 76/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 77/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 78/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 79/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 80/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 81/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 82/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 83/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 84/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 85/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 86/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 87/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 88/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 89/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 90/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 91/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 92/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 93/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 94/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 95/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 96/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 97/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 98/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 99/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 100/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 101/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 102/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 103/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 104/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 105/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 106/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 107/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 108/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 109/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 110/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 111/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 112/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 113/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 114/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 115/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 116/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 117/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 118/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 119/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 120/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 121/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 122/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 123/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 124/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 125/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 126/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 127/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 128/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 129/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 130/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 131/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 132/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 133/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 134/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 135/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 136/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 137/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 138/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 139/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 140/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 141/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 142/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 143/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 144/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 145/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 146/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 147/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 148/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 149/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 150/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 151/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 152/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 153/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 154/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 155/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 156/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 157/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 158/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 159/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 160/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 161/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 162/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 163/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 164/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 165/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 166/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 167/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 168/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 169/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 170/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 171/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 172/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 173/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 174/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 175/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 176/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 177/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 178/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 179/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 180/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 181/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 182/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 183/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 184/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 185/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 186/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 187/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 188/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 189/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 190/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 191/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 192/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 193/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 194/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 195/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 196/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 197/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 198/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 199/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 200/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 201/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 202/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 203/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 204/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 205/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 206/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 207/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 208/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 209/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 210/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 211/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 212/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 213/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 214/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 215/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 216/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 217/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 218/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 219/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 220/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 221/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 222/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 223/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 224/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 225/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 226/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 227/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 228/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 229/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 230/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 231/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 232/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 233/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 234/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 235/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 236/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 237/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 238/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 239/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 240/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 241/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 242/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 243/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 244/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 245/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 246/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 247/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 248/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 249/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 250/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 251/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 252/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 253/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 254/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 255/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 256/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 257/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 258/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 259/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 260/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 261/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 262/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 263/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 264/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 265/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 266/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 267/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 268/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 269/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 270/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 271/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 272/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 273/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 274/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 275/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 276/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 277/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 278/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 279/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 280/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 281/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 282/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 283/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 284/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 285/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 286/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 287/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 288/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 289/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 290/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 291/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 292/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 293/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 294/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 295/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 296/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 297/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 298/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 299/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 300/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 301/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 302/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 303/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 304/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 305/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 306/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 307/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 308/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 309/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 310/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 311/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 312/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 313/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 314/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 315/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 316/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 317/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 318/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 319/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 320/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 321/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 322/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 323/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 324/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 325/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 326/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 327/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 328/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 329/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 330/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 331/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 332/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 333/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 334/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 335/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 336/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 337/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 338/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 339/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 340/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 341/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 342/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 343/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 344/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 345/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 346/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 347/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 348/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 349/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 350/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 351/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 352/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 353/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 354/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 355/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 356/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 357/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 358/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 359/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 360/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 361/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 362/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 363/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 364/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 365/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 366/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 367/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 368/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 369/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 370/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 371/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 372/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 373/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 374/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 375/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 376/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 377/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 378/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 379/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 380/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 381/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 382/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 383/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 384/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 385/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 386/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 387/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 388/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 389/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 390/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 391/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 392/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 393/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 394/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 395/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 396/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 397/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 398/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 399/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 400/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 401/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 402/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 403/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 404/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 405/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 406/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 407/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 408/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 409/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 410/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 411/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 412/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 413/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 414/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 415/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 416/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 417/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 418/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 419/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 420/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 421/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 422/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 423/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 424/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 425/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 426/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 427/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 428/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 429/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 430/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 431/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 432/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 433/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 434/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 435/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 436/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 437/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 438/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 439/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 440/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 441/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 442/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 443/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 444/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 445/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 446/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 447/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 448/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 449/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 450/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 451/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 452/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 453/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 454/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 455/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 456/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 457/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 458/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 459/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 460/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 461/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 462/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 463/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 464/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 465/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 466/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 467/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 468/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 469/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 470/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 471/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 472/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 473/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 474/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 475/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 476/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 477/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 478/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 479/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 480/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 481/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 482/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 483/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 484/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 485/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 486/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 487/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 488/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 489/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 490/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 491/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 492/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 493/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 494/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 495/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 496/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 497/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 498/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 499/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 500/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 501/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 502/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 503/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 504/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 505/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 506/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 507/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 508/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 509/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 510/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 511/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 512/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 513/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 514/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 515/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 516/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 517/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 518/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 519/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 520/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 521/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 522/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 523/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 524/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 525/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 526/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 527/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 528/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 529/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 530/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 531/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 532/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 533/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 534/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 535/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 536/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 537/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 538/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 539/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 540/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 541/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 542/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 543/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 544/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 545/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 546/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 547/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 548/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 549/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 550/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 551/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 552/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 553/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 554/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 555/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 556/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 557/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 558/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 559/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 560/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 561/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 562/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 563/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 564/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 565/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 566/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 567/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 568/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 569/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 570/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 571/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 572/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 573/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 574/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 575/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 576/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 577/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 578/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 579/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 580/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 581/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 582/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 583/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 584/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 585/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 586/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 587/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 588/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 589/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 590/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 591/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 592/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 593/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 594/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 595/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 596/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 597/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 598/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 599/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 600/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 601/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 602/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 603/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 604/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 605/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 606/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 607/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 608/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 609/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 610/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 611/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 612/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 613/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 614/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 615/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 616/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 617/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 618/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 619/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 620/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 621/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 622/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 623/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 624/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 625/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 626/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 627/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 628/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 629/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 630/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 631/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 632/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 633/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 634/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 635/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 636/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 637/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 638/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 639/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 640/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 641/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 642/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 643/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 644/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 645/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 646/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 647/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 648/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 649/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 650/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 651/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 652/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 653/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 654/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 655/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 656/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 657/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 658/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 659/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 660/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 661/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 662/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 663/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 664/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 665/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 666/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 667/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 668/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 669/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 670/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 671/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 672/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 673/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 674/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 675/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 676/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 677/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 678/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 679/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 680/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 681/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 682/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 683/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 684/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 685/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 686/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 687/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 688/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 689/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 690/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 691/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 692/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 693/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 694/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 695/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 696/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 697/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 698/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 699/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 700/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 701/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 702/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 703/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 704/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 705/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 706/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 707/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 708/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 709/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 710/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 711/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 712/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 713/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 714/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 715/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 716/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 717/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 718/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 719/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 720/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 721/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 722/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 723/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 724/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 725/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 726/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 727/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 728/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 729/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 730/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 731/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 732/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 733/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 734/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 735/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 736/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 737/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 738/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 739/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 740/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 741/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 742/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 743/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 744/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 745/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 746/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 747/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 748/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 749/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 750/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 751/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 752/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 753/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 754/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 755/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 756/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 757/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 758/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 759/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 760/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 761/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 762/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 763/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 764/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 765/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 766/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 767/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 768/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 769/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 770/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 771/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 772/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 773/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 774/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 775/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 776/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 777/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 778/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 779/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 780/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 781/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 782/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 783/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 784/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 785/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 786/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 787/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 788/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 789/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 790/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 791/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 792/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 793/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 794/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 795/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 796/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 797/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 798/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 799/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 800/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 801/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 802/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 803/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 804/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 805/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 806/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 807/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 808/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 809/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 810/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 811/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 812/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 813/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 814/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 815/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 816/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 817/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 818/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 819/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 820/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 821/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 822/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 823/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 824/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 825/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 826/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 827/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 828/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 829/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 830/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 831/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 832/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 833/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 834/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 835/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 836/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 837/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 838/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 839/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 840/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 841/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 842/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 843/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 844/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 845/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 846/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 847/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 848/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 849/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 850/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 851/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 852/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 853/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 854/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 855/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 856/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 857/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 858/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 859/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 860/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 861/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 862/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 863/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 864/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 865/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 866/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 867/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 868/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 869/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 870/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 871/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 872/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 873/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 874/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 875/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 876/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 877/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 878/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 879/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 880/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 881/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 882/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 883/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 884/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 885/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 886/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 887/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 888/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 889/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 890/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 891/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 892/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 893/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 894/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 895/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 896/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 897/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 898/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 899/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 900/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 901/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 902/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 903/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 904/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 905/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 906/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 907/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 908/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 909/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 910/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 911/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 912/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 913/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 914/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 915/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 916/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 917/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 918/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 919/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 920/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 921/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 922/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 923/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 924/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 925/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 926/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 927/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 928/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 929/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 930/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 931/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 932/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 933/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 934/1000\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 935/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 936/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 937/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 938/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 939/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 940/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 941/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 942/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 943/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 944/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 945/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 946/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 947/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 948/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 949/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 950/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 951/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 952/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 953/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 954/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 955/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 956/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 957/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 958/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 959/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 960/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 961/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 962/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 963/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 964/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 965/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 966/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 967/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 968/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 969/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 970/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 971/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 972/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 973/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 974/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 975/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 976/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 977/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 978/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 979/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 980/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 981/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 982/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 983/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 984/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 985/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 986/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 987/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 988/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 989/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 990/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 991/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 992/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 993/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 994/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 995/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 996/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 997/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 998/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 999/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 1000/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n"
     ]
    }
   ],
   "source": [
    "historySGD = model.fit(X_train, y_train, epochs=1000,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = historySGD.history['loss'] \n",
    "train_accuracy = historySGD.history['accuracy']  \n",
    "valid_loss = historySGD.history['val_loss'] \n",
    "valid_accuracy = historySGD.history['val_accuracy']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Training Loss: 121636.3359375, Training Accuracy: 0.7748099565505981\n",
      "  Validation Loss: 0.6230536103248596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 2:\n",
      "  Training Loss: 0.6041941046714783, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5835213661193848, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 3:\n",
      "  Training Loss: 0.5750857591629028, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5606849193572998, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 4:\n",
      "  Training Loss: 0.5582327842712402, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5470410585403442, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 5:\n",
      "  Training Loss: 0.5482559204101562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5387736558914185, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 6:\n",
      "  Training Loss: 0.5422311425209045, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5335677266120911, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 7:\n",
      "  Training Loss: 0.538513720035553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5302805304527283, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 8:\n",
      "  Training Loss: 0.5361977815628052, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5281283259391785, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 9:\n",
      "  Training Loss: 0.5347303152084351, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5267216563224792, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 10:\n",
      "  Training Loss: 0.533798098564148, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.525774359703064, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 11:\n",
      "  Training Loss: 0.5332017540931702, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5251451730728149, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 12:\n",
      "  Training Loss: 0.5328201055526733, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5247079133987427, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 13:\n",
      "  Training Loss: 0.5325700640678406, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5244002938270569, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 14:\n",
      "  Training Loss: 0.532405436038971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.524183452129364, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 15:\n",
      "  Training Loss: 0.5322994589805603, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5240318179130554, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 16:\n",
      "  Training Loss: 0.5322311520576477, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5239199995994568, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 17:\n",
      "  Training Loss: 0.5321850776672363, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238384008407593, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 18:\n",
      "  Training Loss: 0.5321558117866516, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523779571056366, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 19:\n",
      "  Training Loss: 0.5321364998817444, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237333178520203, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 20:\n",
      "  Training Loss: 0.5321215987205505, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236951112747192, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 21:\n",
      "  Training Loss: 0.5321130752563477, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236680507659912, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 22:\n",
      "  Training Loss: 0.5321069955825806, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236448645591736, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 23:\n",
      "  Training Loss: 0.5321030020713806, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236308574676514, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 24:\n",
      "  Training Loss: 0.5321007370948792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236195921897888, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 25:\n",
      "  Training Loss: 0.5320996046066284, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236119627952576, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 26:\n",
      "  Training Loss: 0.5320990681648254, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236047506332397, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 27:\n",
      "  Training Loss: 0.5320985317230225, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235976576805115, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 28:\n",
      "  Training Loss: 0.5320971012115479, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235921740531921, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 29:\n",
      "  Training Loss: 0.5320972204208374, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235875844955444, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 30:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235834121704102, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 31:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235803723335266, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 32:\n",
      "  Training Loss: 0.5320971608161926, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235796570777893, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 33:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235764980316162, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 34:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235739350318909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 35:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729813575745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 36:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235740542411804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 37:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235720872879028, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 38:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235744714736938, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 39:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732197761536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 40:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738754272461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 41:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235740542411804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 42:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 43:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 44:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 45:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 46:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691666603088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 47:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 48:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 49:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 50:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 51:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 52:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 53:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 54:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235685706138611, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 55:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235692262649536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 56:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 57:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676765441895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 58:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 59:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235692262649536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 60:\n",
      "  Training Loss: 0.5320972204208374, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 61:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 62:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 63:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 64:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697627067566, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 65:\n",
      "  Training Loss: 0.5320952534675598, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 66:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 67:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 68:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 69:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 70:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 71:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 72:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 73:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 74:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 75:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235664248466492, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 76:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 77:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 78:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 79:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 80:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 81:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235699415206909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 82:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 83:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 84:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 85:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 86:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710740089417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 87:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 88:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 89:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235668420791626, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 90:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 91:\n",
      "  Training Loss: 0.5320954918861389, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 92:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 93:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 94:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 95:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 96:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 97:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710144042969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 98:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 99:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 100:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235715508460999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 101:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710144042969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 102:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704779624939, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 103:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729217529297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 104:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738754272461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 105:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235741138458252, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 106:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235726237297058, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 107:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 108:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719680786133, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 109:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235720276832581, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 110:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738754272461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 111:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732197761536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 112:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 113:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235731601715088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 114:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719680786133, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 115:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716700553894, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 116:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713720321655, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 117:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697031021118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 118:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 119:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 120:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235706567764282, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 121:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235699415206909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 122:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 123:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235699415206909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 124:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 125:\n",
      "  Training Loss: 0.5320953726768494, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 126:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235715508460999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 127:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722064971924, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 128:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523572564125061, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 129:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 130:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 131:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235715508460999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 132:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 133:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 134:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 135:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235663652420044, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 136:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235664248466492, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 137:\n",
      "  Training Loss: 0.5320951342582703, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235677361488342, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 138:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 139:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235683917999268, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 140:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 141:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235682129859924, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 142:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 143:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 144:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697627067566, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 145:\n",
      "  Training Loss: 0.5320972204208374, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697031021118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 146:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235721468925476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 147:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738158226013, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 148:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523571252822876, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 149:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235720872879028, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 150:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523571252822876, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 151:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 152:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 153:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 154:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 155:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 156:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 157:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 158:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 159:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 160:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 161:\n",
      "  Training Loss: 0.5320971012115479, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 162:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732197761536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 163:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523574709892273, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 164:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523572564125061, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 165:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235740542411804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 166:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235726237297058, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 167:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235731601715088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 168:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 169:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 170:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235718488693237, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 171:\n",
      "  Training Loss: 0.5320971608161926, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523570716381073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 172:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 173:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 174:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 175:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 176:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 177:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 178:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523571252822876, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 179:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235726237297058, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 180:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729813575745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 181:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235740542411804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 182:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 183:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 184:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 185:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 186:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 187:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 188:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 189:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 190:\n",
      "  Training Loss: 0.5320954322814941, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 191:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 192:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 193:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670804977417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 194:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652327537537, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 195:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235655307769775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 196:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 197:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 198:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704779624939, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 199:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 200:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523571252822876, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 201:\n",
      "  Training Loss: 0.5320953130722046, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 202:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 203:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523571252822876, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 204:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 205:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 206:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 207:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719680786133, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 208:\n",
      "  Training Loss: 0.5320954918861389, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235751867294312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 209:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235734581947327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 210:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523574948310852, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 211:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235736966133118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 212:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711932182312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 213:\n",
      "  Training Loss: 0.532095193862915, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523572564125061, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 214:\n",
      "  Training Loss: 0.5320972204208374, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235728621482849, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 215:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235733389854431, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 216:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 217:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235737562179565, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 218:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235764384269714, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 219:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732197761536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 220:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710144042969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 221:\n",
      "  Training Loss: 0.5320971608161926, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 222:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 223:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704779624939, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 224:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 225:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523570716381073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 226:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 227:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691666603088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 228:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 229:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235735177993774, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 230:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235747694969177, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 231:\n",
      "  Training Loss: 0.5320972800254822, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235745906829834, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 232:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523572564125061, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 233:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 234:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 235:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523571252822876, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 236:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235721468925476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 237:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711932182312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 238:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523572564125061, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 239:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235715508460999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 240:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 241:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 242:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 243:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 244:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704779624939, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 245:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 246:\n",
      "  Training Loss: 0.5320950150489807, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 247:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 248:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 249:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 250:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 251:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 252:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235663056373596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 253:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235657095909119, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 254:\n",
      "  Training Loss: 0.5320953726768494, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235639810562134, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 255:\n",
      "  Training Loss: 0.5320973992347717, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235653519630432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 256:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 257:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 258:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 259:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 260:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567259311676, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 261:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 262:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235692858695984, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 263:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 264:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 265:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235699415206909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 266:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 267:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697031021118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 268:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235701203346252, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 269:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 270:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235715508460999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 271:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702991485596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 272:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 273:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714316368103, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 274:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 275:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697031021118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 276:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 277:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 278:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 279:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 280:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669612884521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 281:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669612884521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 282:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235646963119507, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 283:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235645771026611, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 284:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235647559165955, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 285:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 286:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235656499862671, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 287:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 288:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 289:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235656499862671, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 290:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235639810562134, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 291:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235621333122253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 292:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652923583984, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 293:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523564875125885, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 294:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235650539398193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 295:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235656499862671, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 296:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 297:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 298:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 299:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 300:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235715508460999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 301:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 302:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 303:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 304:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729217529297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 305:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719680786133, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 306:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 307:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711932182312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 308:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235730409622192, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 309:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235728621482849, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 310:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 311:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 312:\n",
      "  Training Loss: 0.5320971608161926, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523570716381073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 313:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 314:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 315:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 316:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235690474510193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 317:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 318:\n",
      "  Training Loss: 0.5320971608161926, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235682725906372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 319:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 320:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 321:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 322:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 323:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 324:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235692858695984, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 325:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235685706138611, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 326:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 327:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 328:\n",
      "  Training Loss: 0.5320954322814941, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 329:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 330:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 331:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 332:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 333:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 334:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669016838074, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 335:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 336:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 337:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 338:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691666603088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 339:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 340:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 341:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 342:\n",
      "  Training Loss: 0.5320954918861389, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652923583984, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 343:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235671997070312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 344:\n",
      "  Training Loss: 0.5320953130722046, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 345:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 346:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 347:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669612884521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 348:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235665440559387, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 349:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523566484451294, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 350:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 351:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 352:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523572564125061, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 353:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 354:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 355:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 356:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711336135864, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 357:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 358:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 359:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 360:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722064971924, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 361:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235740542411804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 362:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729813575745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 363:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710144042969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 364:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 365:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710740089417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 366:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710740089417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 367:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 368:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235721468925476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 369:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738754272461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 370:\n",
      "  Training Loss: 0.5320971012115479, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523573100566864, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 371:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738754272461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 372:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 373:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710740089417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 374:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704779624939, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 375:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 376:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697031021118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 377:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568332195282, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 378:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 379:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235665440559387, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 380:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676765441895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 381:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 382:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 383:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670804977417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 384:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 385:\n",
      "  Training Loss: 0.5320952534675598, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235671997070312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 386:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 387:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 388:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676765441895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 389:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235665440559387, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 390:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235661268234253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 391:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235690474510193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 392:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 393:\n",
      "  Training Loss: 0.5320951342582703, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 394:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697031021118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 395:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 396:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 397:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 398:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 399:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669612884521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 400:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235656499862671, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 401:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235671997070312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 402:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 403:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235684514045715, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 404:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 405:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 406:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 407:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 408:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 409:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 410:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235674977302551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 411:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235692262649536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 412:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 413:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710144042969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 414:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 415:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 416:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 417:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235679149627686, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 418:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 419:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 420:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 421:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669016838074, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 422:\n",
      "  Training Loss: 0.5320954918861389, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235698223114014, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 423:\n",
      "  Training Loss: 0.532097339630127, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 424:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 425:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710740089417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 426:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700011253357, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 427:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 428:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 429:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 430:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 431:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713720321655, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 432:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716700553894, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 433:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235733985900879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 434:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235737562179565, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 435:\n",
      "  Training Loss: 0.5320953726768494, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722064971924, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 436:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235747694969177, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 437:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235748291015625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 438:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235742330551147, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 439:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738754272461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 440:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235734581947327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 441:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 442:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 443:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235720276832581, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 444:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 445:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 446:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567259311676, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 447:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235665440559387, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 448:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235654711723328, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 449:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652327537537, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 450:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652327537537, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 451:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 452:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 453:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 454:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 455:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 456:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235715508460999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 457:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697627067566, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 458:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235682725906372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 459:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 460:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 461:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704779624939, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 462:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 463:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 464:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235705375671387, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 465:\n",
      "  Training Loss: 0.5320971608161926, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 466:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 467:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523572564125061, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 468:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523570716381073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 469:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 470:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 471:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 472:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 473:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711336135864, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 474:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 475:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 476:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523570716381073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 477:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 478:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729217529297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 479:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523573100566864, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 480:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 481:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 482:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 483:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 484:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 485:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235662460327148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 486:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235647559165955, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 487:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235663056373596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 488:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235646367073059, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 489:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235654711723328, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 490:\n",
      "  Training Loss: 0.5320949554443359, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 491:\n",
      "  Training Loss: 0.5320971608161926, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 492:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235664248466492, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 493:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235666036605835, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 494:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235654711723328, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 495:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235666036605835, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 496:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523564338684082, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 497:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235642790794373, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 498:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235635042190552, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 499:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235653519630432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 500:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235644578933716, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 501:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235635042190552, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 502:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235624313354492, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 503:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652923583984, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 504:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235662460327148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 505:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669016838074, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 506:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235644578933716, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 507:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 508:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 509:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 510:\n",
      "  Training Loss: 0.5320954322814941, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 511:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 512:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 513:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235677361488342, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 514:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235674977302551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 515:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 516:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 517:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 518:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 519:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235677361488342, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 520:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 521:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 522:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691666603088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 523:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 524:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 525:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 526:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 527:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 528:\n",
      "  Training Loss: 0.5320954918861389, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235731601715088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 529:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 530:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714316368103, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 531:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523574709892273, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 532:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738754272461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 533:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235740542411804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 534:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235731601715088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 535:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711932182312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 536:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523573100566864, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 537:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719680786133, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 538:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 539:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523570716381073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 540:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 541:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 542:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235679149627686, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 543:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 544:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 545:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567259311676, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 546:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523566484451294, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 547:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670804977417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 548:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235654711723328, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 549:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568332195282, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 550:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 551:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 552:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 553:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 554:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 555:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 556:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 557:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 558:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 559:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 560:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 561:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 562:\n",
      "  Training Loss: 0.5320954918861389, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 563:\n",
      "  Training Loss: 0.5320953130722046, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711932182312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 564:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711932182312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 565:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711932182312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 566:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 567:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 568:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 569:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 570:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738754272461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 571:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235740542411804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 572:\n",
      "  Training Loss: 0.5320974588394165, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235726237297058, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 573:\n",
      "  Training Loss: 0.5320971012115479, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 574:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235705971717834, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 575:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713720321655, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 576:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235699415206909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 577:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 578:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 579:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 580:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 581:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 582:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235739350318909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 583:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719680786133, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 584:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 585:\n",
      "  Training Loss: 0.5320952534675598, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 586:\n",
      "  Training Loss: 0.5320949554443359, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235677361488342, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 587:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235682129859924, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 588:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697031021118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 589:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235690474510193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 590:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716700553894, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 591:\n",
      "  Training Loss: 0.5320973992347717, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729217529297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 592:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 593:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 594:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 595:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 596:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704779624939, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 597:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235699415206909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 598:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 599:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 600:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 601:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235690474510193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 602:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 603:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235677361488342, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 604:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 605:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 606:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235685706138611, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 607:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235684514045715, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 608:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 609:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235674977302551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 610:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 611:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 612:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 613:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 614:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523564875125885, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 615:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235663056373596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 616:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 617:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235679149627686, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 618:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 619:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 620:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 621:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 622:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523571252822876, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 623:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235724449157715, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 624:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235739350318909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 625:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235744118690491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 626:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738754272461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 627:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732197761536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 628:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 629:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235731601715088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 630:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 631:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 632:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 633:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691666603088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 634:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 635:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 636:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 637:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235679149627686, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 638:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 639:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 640:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235744118690491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 641:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235731601715088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 642:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 643:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 644:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670804977417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 645:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670804977417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 646:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 647:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 648:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 649:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 650:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 651:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 652:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 653:\n",
      "  Training Loss: 0.5320953130722046, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 654:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729813575745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 655:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 656:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 657:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732197761536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 658:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716700553894, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 659:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235720276832581, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 660:\n",
      "  Training Loss: 0.5320971608161926, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235720276832581, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 661:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 662:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 663:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 664:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 665:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235677361488342, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 666:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 667:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 668:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 669:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 670:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235701203346252, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 671:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 672:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 673:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235690474510193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 674:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 675:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 676:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 677:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 678:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 679:\n",
      "  Training Loss: 0.5320954918861389, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523572564125061, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 680:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719084739685, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 681:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 682:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697627067566, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 683:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 684:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691666603088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 685:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 686:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 687:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 688:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 689:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235683917999268, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 690:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 691:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 692:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 693:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235682725906372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 694:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 695:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 696:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 697:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 698:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 699:\n",
      "  Training Loss: 0.5320954918861389, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567259311676, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 700:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235648155212402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 701:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652327537537, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 702:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235660672187805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 703:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669016838074, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 704:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 705:\n",
      "  Training Loss: 0.5320954322814941, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 706:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 707:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 708:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 709:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 710:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 711:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235720276832581, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 712:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 713:\n",
      "  Training Loss: 0.5320954322814941, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235740542411804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 714:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235731601715088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 715:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723853111267, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 716:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235720276832581, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 717:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235721468925476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 718:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 719:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 720:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 721:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 722:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686302185059, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 723:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235679149627686, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 724:\n",
      "  Training Loss: 0.5320971012115479, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 725:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 726:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 727:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 728:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 729:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710144042969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 730:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 731:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722064971924, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 732:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 733:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 734:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 735:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 736:\n",
      "  Training Loss: 0.5320953130722046, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 737:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 738:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 739:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711336135864, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 740:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235701203346252, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 741:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235720276832581, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 742:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 743:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235715508460999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 744:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 745:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 746:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235706567764282, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 747:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 748:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235677361488342, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 749:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 750:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691666603088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 751:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 752:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 753:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235677361488342, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 754:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 755:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 756:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 757:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 758:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714316368103, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 759:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 760:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729813575745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 761:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 762:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 763:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 764:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 765:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713720321655, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 766:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697031021118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 767:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 768:\n",
      "  Training Loss: 0.5320969820022583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 769:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 770:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235661268234253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 771:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669016838074, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 772:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523566484451294, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 773:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669016838074, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 774:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523566484451294, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 775:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669016838074, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 776:\n",
      "  Training Loss: 0.5320972204208374, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669016838074, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 777:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 778:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235658288002014, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 779:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235663652420044, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 780:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 781:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 782:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 783:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 784:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686302185059, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 785:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719084739685, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 786:\n",
      "  Training Loss: 0.532097339630127, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 787:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697031021118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 788:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676765441895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 789:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 790:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676765441895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 791:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652923583984, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 792:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235647559165955, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 793:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235657095909119, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 794:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 795:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 796:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 797:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 798:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 799:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 800:\n",
      "  Training Loss: 0.5320949554443359, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 801:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235721468925476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 802:\n",
      "  Training Loss: 0.5320971608161926, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 803:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 804:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 805:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 806:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 807:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 808:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235715508460999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 809:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 810:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 811:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 812:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 813:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691666603088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 814:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 815:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 816:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 817:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 818:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722064971924, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 819:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719084739685, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 820:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 821:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719680786133, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 822:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235721468925476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 823:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235728621482849, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 824:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235747694969177, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 825:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235720276832581, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 826:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235750079154968, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 827:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235739946365356, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 828:\n",
      "  Training Loss: 0.5320972800254822, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719084739685, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 829:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719084739685, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 830:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732197761536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 831:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 832:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235692262649536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 833:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686302185059, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 834:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568332195282, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 835:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 836:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700011253357, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 837:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235715508460999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 838:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704779624939, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 839:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 840:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 841:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 842:\n",
      "  Training Loss: 0.5320951342582703, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 843:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235719680786133, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 844:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 845:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713720321655, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 846:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 847:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 848:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 849:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 850:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 851:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 852:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235731601715088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 853:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235734581947327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 854:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235726237297058, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 855:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732197761536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 856:\n",
      "  Training Loss: 0.5320945978164673, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235767364501953, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 857:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235761404037476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 858:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235744118690491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 859:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732197761536, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 860:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 861:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235736966133118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 862:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 863:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 864:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 865:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 866:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 867:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 868:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 869:\n",
      "  Training Loss: 0.5320971012115479, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 870:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669016838074, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 871:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670804977417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 872:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 873:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 874:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 875:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 876:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 877:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686302185059, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 878:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 879:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 880:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 881:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 882:\n",
      "  Training Loss: 0.5320952534675598, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 883:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 884:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 885:\n",
      "  Training Loss: 0.5320970416069031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 886:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 887:\n",
      "  Training Loss: 0.532097339630127, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 888:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 889:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 890:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 891:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 892:\n",
      "  Training Loss: 0.5320954322814941, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 893:\n",
      "  Training Loss: 0.5320953130722046, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357417345047, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 894:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 895:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 896:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 897:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 898:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 899:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732793807983, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 900:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 901:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235705971717834, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 902:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 903:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 904:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235733985900879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 905:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 906:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 907:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 908:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711932182312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 909:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 910:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235721468925476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 911:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 912:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 913:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 914:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 915:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 916:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 917:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523564875125885, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 918:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652923583984, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 919:\n",
      "  Training Loss: 0.5320969223976135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235646963119507, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 920:\n",
      "  Training Loss: 0.5320973992347717, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652923583984, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 921:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235642194747925, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 922:\n",
      "  Training Loss: 0.5320956110954285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235652327537537, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 923:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523564875125885, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 924:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235639810562134, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 925:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676765441895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 926:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235678553581238, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 927:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 928:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 929:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 930:\n",
      "  Training Loss: 0.5320960879325867, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 931:\n",
      "  Training Loss: 0.5320968627929688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 932:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 933:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 934:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 935:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 936:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676765441895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 937:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676169395447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 938:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235667824745178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 939:\n",
      "  Training Loss: 0.5320958495140076, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 940:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235661268234253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 941:\n",
      "  Training Loss: 0.532095193862915, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 942:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 943:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235661268234253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 944:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235661268234253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 945:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235644578933716, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 946:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235646367073059, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 947:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 948:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 949:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 950:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702991485596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 951:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 952:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 953:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235690474510193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 954:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235690474510193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 955:\n",
      "  Training Loss: 0.5320959687232971, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 956:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697031021118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 957:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235682129859924, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 958:\n",
      "  Training Loss: 0.5320954322814941, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 959:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235693454742432, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 960:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 961:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235661268234253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 962:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 963:\n",
      "  Training Loss: 0.5320963859558105, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673785209656, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 964:\n",
      "  Training Loss: 0.5320954322814941, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 965:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235687494277954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 966:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 967:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 968:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 969:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235677361488342, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 970:\n",
      "  Training Loss: 0.5320957899093628, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 971:\n",
      "  Training Loss: 0.5320951342582703, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 972:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523571252822876, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 973:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 974:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 975:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235701203346252, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 976:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 977:\n",
      "  Training Loss: 0.5320960283279419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 978:\n",
      "  Training Loss: 0.5320959091186523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523570716381073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 979:\n",
      "  Training Loss: 0.5320954322814941, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 980:\n",
      "  Training Loss: 0.5320964455604553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 981:\n",
      "  Training Loss: 0.5320956707000732, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235686898231506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 982:\n",
      "  Training Loss: 0.5320961475372314, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 983:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235698223114014, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 984:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52357017993927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 985:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 986:\n",
      "  Training Loss: 0.5320954322814941, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235658288002014, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 987:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235682129859924, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 988:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 989:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 990:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710740089417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 991:\n",
      "  Training Loss: 0.5320966839790344, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235705971717834, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 992:\n",
      "  Training Loss: 0.532096803188324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 993:\n",
      "  Training Loss: 0.5320962071418762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 994:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 995:\n",
      "  Training Loss: 0.5320966243743896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 996:\n",
      "  Training Loss: 0.5320965647697449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 997:\n",
      "  Training Loss: 0.5320954918861389, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 998:\n",
      "  Training Loss: 0.5320963263511658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 999:\n",
      "  Training Loss: 0.532095730304718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235660076141357, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 1000:\n",
      "  Training Loss: 0.5320967435836792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523566484451294, Validation Accuracy: 0.7827280163764954\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(len(train_loss)):\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Training Loss: {train_loss[epoch]}, Training Accuracy: {train_accuracy[epoch]}\")\n",
    "    print(f\"  Validation Loss: {valid_loss[epoch]}, Validation Accuracy: {valid_accuracy[epoch]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(5, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "592/592 [==============================] - 2s 2ms/step - loss: 42884.1953 - accuracy: 0.7748 - val_loss: 0.5252 - val_accuracy: 0.7827\n",
      "Epoch 2/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5324 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 3/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 4/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 5/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 6/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 7/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 8/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 9/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 10/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 11/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 12/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 13/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 14/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 15/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 16/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 17/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 18/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 19/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 20/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 21/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 22/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 23/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 24/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 25/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 26/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 27/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 28/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 29/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 30/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 31/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 32/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 33/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 34/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 35/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 36/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 37/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 38/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 39/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 40/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 41/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 42/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 43/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 44/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 45/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 46/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 47/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 48/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 49/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 50/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 51/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 52/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 53/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 54/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 55/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 56/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 57/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 58/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 59/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 60/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 61/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 62/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 63/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 64/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 65/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 66/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 67/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 68/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 69/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 70/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 71/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 72/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 73/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 74/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 75/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 76/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 77/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 78/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 79/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 80/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 81/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 82/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 83/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 84/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 85/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 86/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 87/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 88/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 89/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 90/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 91/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 92/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 93/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 94/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 95/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 96/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 97/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 98/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 99/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 100/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 101/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 102/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 103/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 104/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 105/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 106/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 107/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 108/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 109/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 110/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 111/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 112/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 113/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 114/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 115/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 116/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 117/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 118/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 119/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 120/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 121/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 122/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 123/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 124/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 125/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 126/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 127/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 128/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 129/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 130/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 131/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 132/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 133/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 134/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 135/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 136/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 137/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 138/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 139/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 140/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 141/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 142/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 143/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 144/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 145/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 146/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 147/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 148/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 149/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 150/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 151/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 152/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 153/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 154/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 155/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 156/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 157/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 158/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 159/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 160/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 161/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 162/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5239 - val_accuracy: 0.7827\n",
      "Epoch 163/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 164/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 165/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 166/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 167/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 168/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 169/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 170/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 171/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 172/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 173/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 174/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 175/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 176/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 177/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 178/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 179/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 180/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 181/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 182/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 183/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 184/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 185/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 186/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 187/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 188/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 189/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 190/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 191/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 192/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 193/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 194/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 195/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 196/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 197/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 198/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 199/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 200/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 201/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 202/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 203/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 204/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 205/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 206/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 207/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 208/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 209/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 210/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 211/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 212/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 213/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 214/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 215/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 216/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 217/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 218/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 219/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 220/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 221/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 222/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 223/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 224/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 225/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 226/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 227/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 228/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 229/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 230/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 231/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 232/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 233/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 234/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 235/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 236/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 237/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 238/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 239/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 240/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 241/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 242/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 243/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 244/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 245/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 246/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 247/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 248/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 249/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 250/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 251/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 252/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 253/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 254/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 255/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 256/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 257/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 258/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 259/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 260/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 261/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 262/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 263/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 264/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 265/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 266/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 267/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 268/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 269/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 270/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 271/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 272/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 273/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 274/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 275/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 276/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 277/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 278/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 279/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 280/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 281/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 282/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 283/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 284/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 285/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 286/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 287/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 288/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 289/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 290/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 291/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 292/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 293/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 294/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 295/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 296/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 297/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 298/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 299/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 300/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 301/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 302/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 303/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 304/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 305/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 306/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 307/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 308/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 309/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 310/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 311/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 312/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 313/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 314/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 315/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 316/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 317/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 318/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 319/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 320/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 321/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 322/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 323/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 324/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 325/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 326/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 327/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 328/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 329/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 330/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 331/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 332/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 333/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 334/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 335/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 336/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 337/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 338/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 339/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 340/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 341/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 342/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 343/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 344/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 345/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 346/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 347/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 348/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 349/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 350/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 351/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 352/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 353/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 354/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 355/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 356/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 357/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 358/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 359/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 360/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 361/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 362/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 363/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 364/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 365/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 366/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 367/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 368/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 369/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 370/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 371/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 372/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 373/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 374/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 375/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 376/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 377/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 378/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 379/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 380/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 381/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 382/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 383/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 384/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 385/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 386/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 387/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 388/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 389/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 390/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 391/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 392/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 393/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 394/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 395/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 396/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 397/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 398/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 399/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 400/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 401/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 402/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 403/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 404/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 405/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 406/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 407/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 408/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 409/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 410/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 411/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 412/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 413/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 414/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 415/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 416/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 417/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 418/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 419/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 420/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 421/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 422/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 423/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 424/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 425/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 426/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 427/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 428/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 429/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 430/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 431/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 432/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 433/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 434/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 435/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 436/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 437/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 438/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 439/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 440/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 441/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 442/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 443/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 444/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 445/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 446/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 447/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 448/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 449/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 450/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 451/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 452/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 453/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 454/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 455/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 456/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 457/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 458/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 459/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 460/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 461/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 462/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 463/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 464/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 465/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 466/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 467/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 468/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 469/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 470/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 471/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 472/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 473/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 474/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 475/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 476/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 477/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 478/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 479/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 480/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 481/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 482/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 483/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 484/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 485/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 486/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 487/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 488/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 489/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 490/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 491/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 492/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 493/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 494/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 495/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 496/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 497/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 498/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 499/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 500/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 501/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 502/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 503/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 504/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 505/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 506/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 507/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 508/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 509/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 510/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 511/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 512/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 513/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 514/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 515/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 516/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 517/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 518/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 519/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 520/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 521/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 522/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 523/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 524/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 525/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 526/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 527/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 528/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 529/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 530/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 531/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 532/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 533/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 534/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 535/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 536/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 537/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 538/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 539/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 540/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 541/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 542/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 543/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 544/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 545/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 546/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 547/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 548/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 549/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 550/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 551/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 552/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 553/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 554/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 555/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 556/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 557/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 558/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 559/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 560/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 561/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 562/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 563/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 564/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 565/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 566/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 567/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 568/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 569/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 570/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 571/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 572/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 573/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 574/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 575/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 576/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 577/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 578/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 579/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 580/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 581/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 582/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 583/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 584/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 585/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 586/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 587/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 588/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 589/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 590/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 591/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 592/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 593/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 594/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 595/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 596/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 597/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 598/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 599/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 600/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 601/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 602/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 603/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 604/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 605/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 606/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 607/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 608/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 609/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 610/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 611/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 612/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 613/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 614/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 615/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 616/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 617/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 618/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 619/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 620/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 621/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 622/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 623/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 624/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 625/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 626/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 627/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 628/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 629/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 630/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 631/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 632/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 633/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 634/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 635/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 636/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 637/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 638/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 639/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 640/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 641/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 642/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 643/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 644/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 645/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 646/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 647/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 648/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 649/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 650/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 651/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 652/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 653/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 654/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 655/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 656/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 657/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 658/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 659/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 660/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 661/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 662/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 663/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 664/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 665/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 666/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 667/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 668/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 669/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 670/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 671/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 672/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 673/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 674/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 675/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 676/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 677/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 678/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 679/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 680/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 681/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 682/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 683/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 684/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 685/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 686/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 687/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 688/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 689/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 690/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 691/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 692/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 693/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 694/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 695/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 696/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 697/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 698/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 699/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 700/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 701/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 702/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 703/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 704/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 705/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 706/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 707/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 708/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 709/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 710/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 711/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 712/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 713/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 714/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 715/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 716/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 717/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 718/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 719/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 720/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 721/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 722/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 723/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 724/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 725/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 726/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 727/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 728/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 729/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 730/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 731/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 732/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 733/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 734/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 735/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 736/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 737/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 738/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 739/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 740/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 741/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 742/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 743/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 744/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 745/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 746/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 747/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 748/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 749/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 750/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 751/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 752/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 753/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 754/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 755/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 756/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 757/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 758/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 759/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 760/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 761/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 762/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 763/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 764/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 765/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 766/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 767/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 768/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 769/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 770/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 771/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 772/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 773/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 774/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 775/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 776/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 777/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 778/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 779/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 780/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 781/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 782/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 783/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 784/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 785/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 786/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 787/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 788/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 789/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 790/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 791/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 792/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 793/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 794/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 795/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 796/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 797/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 798/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 799/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 800/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 801/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 802/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 803/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 804/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 805/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 806/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 807/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 808/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 809/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 810/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 811/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 812/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 813/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 814/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 815/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 816/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 817/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 818/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 819/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 820/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 821/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 822/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 823/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 824/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 825/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 826/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 827/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 828/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 829/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 830/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 831/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 832/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 833/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 834/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 835/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 836/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 837/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 838/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 839/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 840/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 841/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 842/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 843/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 844/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 845/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 846/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 847/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 848/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 849/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 850/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 851/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 852/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 853/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 854/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 855/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 856/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 857/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 858/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 859/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 860/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 861/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 862/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 863/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 864/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 865/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 866/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 867/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 868/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 869/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 870/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 871/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 872/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 873/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 874/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 875/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 876/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 877/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 878/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 879/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 880/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 881/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 882/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 883/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 884/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 885/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 886/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 887/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 888/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 889/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 890/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 891/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 892/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 893/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 894/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 895/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 896/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 897/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 898/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 899/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 900/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 901/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 902/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 903/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 904/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 905/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 906/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 907/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 908/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 909/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 910/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 911/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 912/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 913/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 914/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 915/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 916/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 917/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 918/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 919/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 920/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 921/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 922/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 923/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 924/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 925/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 926/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 927/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 928/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 929/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 930/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 931/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 932/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 933/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 934/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 935/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 936/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 937/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 938/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 939/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 940/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 941/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 942/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 943/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 944/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 945/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 946/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 947/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 948/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 949/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 950/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 951/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 952/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 953/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 954/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 955/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 956/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 957/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 958/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 959/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 960/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 961/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 962/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 963/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 964/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 965/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 966/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 967/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 968/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 969/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 970/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 971/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 972/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 973/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 974/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 975/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 976/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 977/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 978/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 979/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 980/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 981/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 982/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 983/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 984/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 985/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 986/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 987/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 988/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 989/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 990/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 991/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 992/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 993/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 994/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 995/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 996/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 997/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 998/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 999/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 1000/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n"
     ]
    }
   ],
   "source": [
    "historySGDM = model.fit(X_train, y_train, epochs=1000,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = historySGDM.history['loss'] \n",
    "train_accuracy = historySGDM.history['accuracy']  \n",
    "valid_loss = historySGDM.history['val_loss'] \n",
    "valid_accuracy = historySGDM.history['val_accuracy']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Training Loss: 42884.1953125, Training Accuracy: 0.7748099565505981\n",
      "  Validation Loss: 0.5251954793930054, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 2:\n",
      "  Training Loss: 0.5323540568351746, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236326456069946, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 3:\n",
      "  Training Loss: 0.5321263670921326, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523642897605896, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 4:\n",
      "  Training Loss: 0.5321317315101624, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235294699668884, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 5:\n",
      "  Training Loss: 0.5321221947669983, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236878991127014, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 6:\n",
      "  Training Loss: 0.5321329236030579, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52354496717453, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 7:\n",
      "  Training Loss: 0.5321378707885742, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236078500747681, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 8:\n",
      "  Training Loss: 0.5321279168128967, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235268473625183, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 9:\n",
      "  Training Loss: 0.5321537852287292, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523566722869873, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 10:\n",
      "  Training Loss: 0.532133162021637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235036611557007, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 11:\n",
      "  Training Loss: 0.5321173667907715, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236080288887024, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 12:\n",
      "  Training Loss: 0.532139003276825, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236182808876038, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 13:\n",
      "  Training Loss: 0.5321439504623413, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 14:\n",
      "  Training Loss: 0.5321394205093384, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235574841499329, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 15:\n",
      "  Training Loss: 0.5321305990219116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236608386039734, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 16:\n",
      "  Training Loss: 0.5321288704872131, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235851407051086, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 17:\n",
      "  Training Loss: 0.5321313738822937, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235899090766907, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 18:\n",
      "  Training Loss: 0.5321334600448608, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52364581823349, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 19:\n",
      "  Training Loss: 0.5321443676948547, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 20:\n",
      "  Training Loss: 0.5321174263954163, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235105752944946, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 21:\n",
      "  Training Loss: 0.5321605801582336, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235658288002014, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 22:\n",
      "  Training Loss: 0.5321189761161804, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234876871109009, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 23:\n",
      "  Training Loss: 0.5321297645568848, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236126780509949, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 24:\n",
      "  Training Loss: 0.5321270227432251, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236425399780273, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 25:\n",
      "  Training Loss: 0.5321618914604187, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236834287643433, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 26:\n",
      "  Training Loss: 0.5321329236030579, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523630678653717, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 27:\n",
      "  Training Loss: 0.5321434736251831, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235462188720703, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 28:\n",
      "  Training Loss: 0.5321297645568848, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523590087890625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 29:\n",
      "  Training Loss: 0.5321335196495056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235654711723328, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 30:\n",
      "  Training Loss: 0.5321189165115356, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235479474067688, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 31:\n",
      "  Training Loss: 0.5321446061134338, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235639810562134, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 32:\n",
      "  Training Loss: 0.5321359634399414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236416459083557, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 33:\n",
      "  Training Loss: 0.5321188569068909, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235179662704468, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 34:\n",
      "  Training Loss: 0.5321384072303772, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235203504562378, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 35:\n",
      "  Training Loss: 0.5321577191352844, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235334634780884, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 36:\n",
      "  Training Loss: 0.5321157574653625, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236183404922485, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 37:\n",
      "  Training Loss: 0.5321386456489563, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235476493835449, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 38:\n",
      "  Training Loss: 0.532107412815094, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236929059028625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 39:\n",
      "  Training Loss: 0.5321320295333862, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 40:\n",
      "  Training Loss: 0.532131016254425, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236349105834961, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 41:\n",
      "  Training Loss: 0.5321366786956787, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236182808876038, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 42:\n",
      "  Training Loss: 0.5321628451347351, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235793590545654, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 43:\n",
      "  Training Loss: 0.5320980548858643, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234553217887878, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 44:\n",
      "  Training Loss: 0.5321443676948547, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235859751701355, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 45:\n",
      "  Training Loss: 0.5321256518363953, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235151648521423, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 46:\n",
      "  Training Loss: 0.5321496725082397, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235568284988403, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 47:\n",
      "  Training Loss: 0.5321169495582581, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235273241996765, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 48:\n",
      "  Training Loss: 0.5321299433708191, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235256552696228, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 49:\n",
      "  Training Loss: 0.5321341753005981, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235580205917358, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 50:\n",
      "  Training Loss: 0.5321394205093384, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691666603088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 51:\n",
      "  Training Loss: 0.5321317911148071, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235368609428406, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 52:\n",
      "  Training Loss: 0.5321298837661743, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523590087890625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 53:\n",
      "  Training Loss: 0.5321334600448608, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 54:\n",
      "  Training Loss: 0.5321523547172546, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235422253608704, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 55:\n",
      "  Training Loss: 0.5321227312088013, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235690474510193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 56:\n",
      "  Training Loss: 0.5321295857429504, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235826373100281, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 57:\n",
      "  Training Loss: 0.5321177244186401, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523524284362793, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 58:\n",
      "  Training Loss: 0.5321345329284668, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235241651535034, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 59:\n",
      "  Training Loss: 0.5321339964866638, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236107707023621, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 60:\n",
      "  Training Loss: 0.5321588516235352, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236627459526062, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 61:\n",
      "  Training Loss: 0.5321336388587952, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236226916313171, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 62:\n",
      "  Training Loss: 0.5321252942085266, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235623717308044, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 63:\n",
      "  Training Loss: 0.5321369767189026, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236237645149231, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 64:\n",
      "  Training Loss: 0.5321305990219116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235081315040588, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 65:\n",
      "  Training Loss: 0.5321336984634399, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523576557636261, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 66:\n",
      "  Training Loss: 0.5321451425552368, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236197710037231, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 67:\n",
      "  Training Loss: 0.5321229100227356, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235181450843811, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 68:\n",
      "  Training Loss: 0.5321238040924072, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237001180648804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 69:\n",
      "  Training Loss: 0.5321505665779114, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235413908958435, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 70:\n",
      "  Training Loss: 0.5321104526519775, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234880447387695, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 71:\n",
      "  Training Loss: 0.5321496725082397, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235528349876404, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 72:\n",
      "  Training Loss: 0.5321435928344727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523515522480011, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 73:\n",
      "  Training Loss: 0.5321292281150818, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235899090766907, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 74:\n",
      "  Training Loss: 0.5321326851844788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236135721206665, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 75:\n",
      "  Training Loss: 0.5320791006088257, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523465096950531, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 76:\n",
      "  Training Loss: 0.5321473479270935, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235648155212402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 77:\n",
      "  Training Loss: 0.5321364998817444, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236321687698364, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 78:\n",
      "  Training Loss: 0.5321328043937683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235639810562134, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 79:\n",
      "  Training Loss: 0.532139003276825, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236601233482361, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 80:\n",
      "  Training Loss: 0.5321426391601562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523695170879364, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 81:\n",
      "  Training Loss: 0.5321422815322876, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234957933425903, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 82:\n",
      "  Training Loss: 0.5321515798568726, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235536694526672, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 83:\n",
      "  Training Loss: 0.532143235206604, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702395439148, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 84:\n",
      "  Training Loss: 0.5321222543716431, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523576021194458, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 85:\n",
      "  Training Loss: 0.5321275591850281, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523569643497467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 86:\n",
      "  Training Loss: 0.5321170687675476, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236662030220032, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 87:\n",
      "  Training Loss: 0.5321426391601562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235185623168945, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 88:\n",
      "  Training Loss: 0.5321401357650757, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234904885292053, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 89:\n",
      "  Training Loss: 0.5321161150932312, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234775543212891, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 90:\n",
      "  Training Loss: 0.5321313738822937, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235960483551025, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 91:\n",
      "  Training Loss: 0.5321267247200012, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236040353775024, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 92:\n",
      "  Training Loss: 0.5321403741836548, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235753655433655, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 93:\n",
      "  Training Loss: 0.532131016254425, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235663056373596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 94:\n",
      "  Training Loss: 0.5321145057678223, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235030055046082, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 95:\n",
      "  Training Loss: 0.5321376919746399, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676765441895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 96:\n",
      "  Training Loss: 0.5321500897407532, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236005783081055, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 97:\n",
      "  Training Loss: 0.5321345925331116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236736536026001, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 98:\n",
      "  Training Loss: 0.5321448445320129, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236204862594604, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 99:\n",
      "  Training Loss: 0.532127320766449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235950946807861, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 100:\n",
      "  Training Loss: 0.5321460962295532, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235990285873413, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 101:\n",
      "  Training Loss: 0.5321397185325623, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235834717750549, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 102:\n",
      "  Training Loss: 0.5321403741836548, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235544443130493, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 103:\n",
      "  Training Loss: 0.532110869884491, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236895084381104, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 104:\n",
      "  Training Loss: 0.5321468114852905, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237140655517578, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 105:\n",
      "  Training Loss: 0.5321265459060669, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236425399780273, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 106:\n",
      "  Training Loss: 0.5321415662765503, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235620141029358, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 107:\n",
      "  Training Loss: 0.5321246981620789, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235345959663391, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 108:\n",
      "  Training Loss: 0.5321367383003235, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236331224441528, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 109:\n",
      "  Training Loss: 0.5321228504180908, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236653685569763, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 110:\n",
      "  Training Loss: 0.5321400165557861, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523638904094696, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 111:\n",
      "  Training Loss: 0.5321417450904846, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236265063285828, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 112:\n",
      "  Training Loss: 0.5321453809738159, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235463976860046, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 113:\n",
      "  Training Loss: 0.5321168303489685, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236071348190308, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 114:\n",
      "  Training Loss: 0.5321256518363953, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235453248023987, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 115:\n",
      "  Training Loss: 0.5321422219276428, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236154198646545, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 116:\n",
      "  Training Loss: 0.5321322083473206, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235966444015503, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 117:\n",
      "  Training Loss: 0.5321073532104492, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235024690628052, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 118:\n",
      "  Training Loss: 0.5321325063705444, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235477089881897, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 119:\n",
      "  Training Loss: 0.5321456789970398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235324501991272, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 120:\n",
      "  Training Loss: 0.5321494936943054, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236214399337769, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 121:\n",
      "  Training Loss: 0.5321535468101501, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235467553138733, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 122:\n",
      "  Training Loss: 0.5321282148361206, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236432552337646, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 123:\n",
      "  Training Loss: 0.5321358442306519, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235269665718079, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 124:\n",
      "  Training Loss: 0.5321494340896606, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235201716423035, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 125:\n",
      "  Training Loss: 0.5321226119995117, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236682295799255, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 126:\n",
      "  Training Loss: 0.5321299433708191, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236480236053467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 127:\n",
      "  Training Loss: 0.5321356654167175, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236145853996277, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 128:\n",
      "  Training Loss: 0.532151460647583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236175656318665, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 129:\n",
      "  Training Loss: 0.5321411490440369, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235413312911987, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 130:\n",
      "  Training Loss: 0.5321322679519653, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235761404037476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 131:\n",
      "  Training Loss: 0.5321329832077026, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235660672187805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 132:\n",
      "  Training Loss: 0.5321165323257446, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235228538513184, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 133:\n",
      "  Training Loss: 0.5321384072303772, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235105752944946, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 134:\n",
      "  Training Loss: 0.5321360230445862, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235929489135742, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 135:\n",
      "  Training Loss: 0.5321110486984253, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234860777854919, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 136:\n",
      "  Training Loss: 0.5321536660194397, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235145092010498, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 137:\n",
      "  Training Loss: 0.5321307182312012, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236348509788513, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 138:\n",
      "  Training Loss: 0.5321578979492188, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 139:\n",
      "  Training Loss: 0.5321412086486816, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235647559165955, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 140:\n",
      "  Training Loss: 0.5321378111839294, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523542582988739, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 141:\n",
      "  Training Loss: 0.5321493148803711, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235226154327393, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 142:\n",
      "  Training Loss: 0.5321365594863892, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236069560050964, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 143:\n",
      "  Training Loss: 0.5321416258811951, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235467553138733, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 144:\n",
      "  Training Loss: 0.5321489572525024, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523581326007843, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 145:\n",
      "  Training Loss: 0.5321398973464966, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235349535942078, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 146:\n",
      "  Training Loss: 0.5321128368377686, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237124562263489, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 147:\n",
      "  Training Loss: 0.5321617126464844, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236480236053467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 148:\n",
      "  Training Loss: 0.5321270227432251, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235092639923096, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 149:\n",
      "  Training Loss: 0.532131016254425, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523634672164917, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 150:\n",
      "  Training Loss: 0.5321386456489563, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235640406608582, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 151:\n",
      "  Training Loss: 0.5321283340454102, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235572457313538, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 152:\n",
      "  Training Loss: 0.5321252942085266, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235454440116882, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 153:\n",
      "  Training Loss: 0.5321269631385803, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235059261322021, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 154:\n",
      "  Training Loss: 0.5321325063705444, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235083103179932, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 155:\n",
      "  Training Loss: 0.5321360230445862, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235870480537415, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 156:\n",
      "  Training Loss: 0.5321456789970398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235949754714966, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 157:\n",
      "  Training Loss: 0.5321329236030579, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523561418056488, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 158:\n",
      "  Training Loss: 0.5321418642997742, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235131978988647, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 159:\n",
      "  Training Loss: 0.5321400761604309, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523590087890625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 160:\n",
      "  Training Loss: 0.5321638584136963, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235639810562134, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 161:\n",
      "  Training Loss: 0.5321374535560608, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236107707023621, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 162:\n",
      "  Training Loss: 0.5320850610733032, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238751173019409, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 163:\n",
      "  Training Loss: 0.5321572422981262, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237215161323547, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 164:\n",
      "  Training Loss: 0.532132625579834, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234987139701843, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 165:\n",
      "  Training Loss: 0.5321184396743774, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236520767211914, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 166:\n",
      "  Training Loss: 0.5321316719055176, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235469341278076, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 167:\n",
      "  Training Loss: 0.5321324467658997, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236035585403442, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 168:\n",
      "  Training Loss: 0.532151997089386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714316368103, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 169:\n",
      "  Training Loss: 0.5321240425109863, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235140919685364, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 170:\n",
      "  Training Loss: 0.5321306586265564, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236234068870544, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 171:\n",
      "  Training Loss: 0.5321316123008728, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235573053359985, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 172:\n",
      "  Training Loss: 0.5321398973464966, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235529541969299, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 173:\n",
      "  Training Loss: 0.5321383476257324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235379934310913, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 174:\n",
      "  Training Loss: 0.5321380496025085, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235790610313416, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 175:\n",
      "  Training Loss: 0.5321386456489563, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235326886177063, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 176:\n",
      "  Training Loss: 0.5321410894393921, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236214399337769, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 177:\n",
      "  Training Loss: 0.5321256518363953, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235169529914856, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 178:\n",
      "  Training Loss: 0.5320988893508911, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236893892288208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 179:\n",
      "  Training Loss: 0.5321391820907593, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236379504203796, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 180:\n",
      "  Training Loss: 0.5321364402770996, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236015319824219, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 181:\n",
      "  Training Loss: 0.5321322679519653, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236638784408569, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 182:\n",
      "  Training Loss: 0.5321056246757507, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234962105751038, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 183:\n",
      "  Training Loss: 0.5321448445320129, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235323309898376, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 184:\n",
      "  Training Loss: 0.5321549773216248, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235567688941956, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 185:\n",
      "  Training Loss: 0.5321265459060669, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235863924026489, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 186:\n",
      "  Training Loss: 0.5321258902549744, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235339403152466, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 187:\n",
      "  Training Loss: 0.5321289896965027, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235056281089783, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 188:\n",
      "  Training Loss: 0.5321256518363953, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236280560493469, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 189:\n",
      "  Training Loss: 0.5321452617645264, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236237645149231, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 190:\n",
      "  Training Loss: 0.5321211814880371, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235001444816589, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 191:\n",
      "  Training Loss: 0.532139003276825, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523503303527832, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 192:\n",
      "  Training Loss: 0.5321375131607056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235410332679749, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 193:\n",
      "  Training Loss: 0.5321314334869385, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235294699668884, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 194:\n",
      "  Training Loss: 0.5321282744407654, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234953165054321, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 195:\n",
      "  Training Loss: 0.5321359634399414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235241651535034, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 196:\n",
      "  Training Loss: 0.5321245789527893, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236045718193054, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 197:\n",
      "  Training Loss: 0.5321531295776367, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235994458198547, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 198:\n",
      "  Training Loss: 0.5321309566497803, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236306190490723, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 199:\n",
      "  Training Loss: 0.5321528911590576, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738158226013, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 200:\n",
      "  Training Loss: 0.5321300625801086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236563086509705, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 201:\n",
      "  Training Loss: 0.5321310758590698, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235322713851929, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 202:\n",
      "  Training Loss: 0.5321323275566101, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236104130744934, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 203:\n",
      "  Training Loss: 0.5321521759033203, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523617684841156, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 204:\n",
      "  Training Loss: 0.5321358442306519, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235809087753296, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 205:\n",
      "  Training Loss: 0.5321110486984253, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234943628311157, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 206:\n",
      "  Training Loss: 0.5321191549301147, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236505270004272, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 207:\n",
      "  Training Loss: 0.5321463942527771, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236587524414062, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 208:\n",
      "  Training Loss: 0.5321187973022461, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237923860549927, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 209:\n",
      "  Training Loss: 0.5321488976478577, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235612988471985, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 210:\n",
      "  Training Loss: 0.5321309566497803, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236386060714722, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 211:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235547423362732, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 212:\n",
      "  Training Loss: 0.532115638256073, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235288143157959, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 213:\n",
      "  Training Loss: 0.5321211218833923, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236339569091797, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 214:\n",
      "  Training Loss: 0.5321499109268188, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523628830909729, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 215:\n",
      "  Training Loss: 0.5321256518363953, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523604154586792, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 216:\n",
      "  Training Loss: 0.5321263670921326, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235422849655151, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 217:\n",
      "  Training Loss: 0.5321272015571594, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236449241638184, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 218:\n",
      "  Training Loss: 0.5320979952812195, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237983465194702, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 219:\n",
      "  Training Loss: 0.532130241394043, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234973430633545, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 220:\n",
      "  Training Loss: 0.5321569442749023, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235465168952942, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 221:\n",
      "  Training Loss: 0.5321378111839294, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235223770141602, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 222:\n",
      "  Training Loss: 0.5321314334869385, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235991477966309, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 223:\n",
      "  Training Loss: 0.5321319699287415, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235550403594971, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 224:\n",
      "  Training Loss: 0.5321432948112488, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236048698425293, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 225:\n",
      "  Training Loss: 0.5321438908576965, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235908627510071, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 226:\n",
      "  Training Loss: 0.5321381688117981, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235204696655273, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 227:\n",
      "  Training Loss: 0.5321366786956787, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235444903373718, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 228:\n",
      "  Training Loss: 0.532126247882843, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236631631851196, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 229:\n",
      "  Training Loss: 0.5321131348609924, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237443447113037, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 230:\n",
      "  Training Loss: 0.5321512222290039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236960053443909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 231:\n",
      "  Training Loss: 0.5321422219276428, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523603618144989, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 232:\n",
      "  Training Loss: 0.5321425199508667, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235199332237244, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 233:\n",
      "  Training Loss: 0.5321428179740906, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235427021980286, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 234:\n",
      "  Training Loss: 0.5321366786956787, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235248804092407, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 235:\n",
      "  Training Loss: 0.532130241394043, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236153602600098, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 236:\n",
      "  Training Loss: 0.5321257710456848, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236294269561768, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 237:\n",
      "  Training Loss: 0.5321479439735413, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235776305198669, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 238:\n",
      "  Training Loss: 0.5321493148803711, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236377120018005, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 239:\n",
      "  Training Loss: 0.5321282744407654, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523590624332428, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 240:\n",
      "  Training Loss: 0.5321168899536133, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235177874565125, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 241:\n",
      "  Training Loss: 0.532127320766449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236141681671143, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 242:\n",
      "  Training Loss: 0.5321428179740906, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235986113548279, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 243:\n",
      "  Training Loss: 0.5321260094642639, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235427021980286, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 244:\n",
      "  Training Loss: 0.5321226119995117, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235892534255981, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 245:\n",
      "  Training Loss: 0.5321371555328369, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235978364944458, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 246:\n",
      "  Training Loss: 0.5321235656738281, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235353112220764, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 247:\n",
      "  Training Loss: 0.5321233868598938, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234960317611694, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 248:\n",
      "  Training Loss: 0.5321233868598938, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523625373840332, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 249:\n",
      "  Training Loss: 0.5321412682533264, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236069560050964, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 250:\n",
      "  Training Loss: 0.5321287512779236, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235402584075928, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 251:\n",
      "  Training Loss: 0.5321307182312012, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523545503616333, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 252:\n",
      "  Training Loss: 0.5321205258369446, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234929919242859, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 253:\n",
      "  Training Loss: 0.5321323275566101, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234916806221008, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 254:\n",
      "  Training Loss: 0.5321632027626038, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234756469726562, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 255:\n",
      "  Training Loss: 0.5321402549743652, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236037969589233, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 256:\n",
      "  Training Loss: 0.5321407914161682, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236181020736694, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 257:\n",
      "  Training Loss: 0.5321409106254578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237125158309937, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 258:\n",
      "  Training Loss: 0.5321375131607056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235214829444885, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 259:\n",
      "  Training Loss: 0.5321450233459473, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235203504562378, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 260:\n",
      "  Training Loss: 0.5321507453918457, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235403180122375, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 261:\n",
      "  Training Loss: 0.5321340560913086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235492587089539, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 262:\n",
      "  Training Loss: 0.5321354866027832, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236051678657532, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 263:\n",
      "  Training Loss: 0.5321460366249084, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235707759857178, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 264:\n",
      "  Training Loss: 0.532138466835022, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236047506332397, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 265:\n",
      "  Training Loss: 0.5321242213249207, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235927104949951, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 266:\n",
      "  Training Loss: 0.532136857509613, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235610604286194, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 267:\n",
      "  Training Loss: 0.5321217775344849, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235577821731567, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 268:\n",
      "  Training Loss: 0.5321352481842041, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235617160797119, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 269:\n",
      "  Training Loss: 0.5321305990219116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236334800720215, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 270:\n",
      "  Training Loss: 0.5321290493011475, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235964059829712, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 271:\n",
      "  Training Loss: 0.5321229696273804, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235208868980408, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 272:\n",
      "  Training Loss: 0.5321394205093384, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235373377799988, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 273:\n",
      "  Training Loss: 0.5321359634399414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236216187477112, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 274:\n",
      "  Training Loss: 0.5321135520935059, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234947800636292, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 275:\n",
      "  Training Loss: 0.5321382284164429, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235254168510437, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 276:\n",
      "  Training Loss: 0.532143235206604, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235915184020996, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 277:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235322713851929, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 278:\n",
      "  Training Loss: 0.5321178436279297, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234898328781128, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 279:\n",
      "  Training Loss: 0.5321326851844788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 280:\n",
      "  Training Loss: 0.53214430809021, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235036015510559, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 281:\n",
      "  Training Loss: 0.5321534872055054, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235332250595093, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 282:\n",
      "  Training Loss: 0.5321103930473328, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234627723693848, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 283:\n",
      "  Training Loss: 0.5321526527404785, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235204696655273, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 284:\n",
      "  Training Loss: 0.53212970495224, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235362648963928, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 285:\n",
      "  Training Loss: 0.5321287512779236, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236111283302307, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 286:\n",
      "  Training Loss: 0.5321255922317505, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235189199447632, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 287:\n",
      "  Training Loss: 0.5321045517921448, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237398147583008, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 288:\n",
      "  Training Loss: 0.5321070551872253, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235005021095276, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 289:\n",
      "  Training Loss: 0.5321263670921326, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234716534614563, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 290:\n",
      "  Training Loss: 0.5321608185768127, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234996676445007, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 291:\n",
      "  Training Loss: 0.5321235060691833, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234884023666382, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 292:\n",
      "  Training Loss: 0.5321199297904968, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235735177993774, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 293:\n",
      "  Training Loss: 0.5321298837661743, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235326290130615, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 294:\n",
      "  Training Loss: 0.5321289300918579, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235269665718079, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 295:\n",
      "  Training Loss: 0.5321425795555115, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523554265499115, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 296:\n",
      "  Training Loss: 0.5321503281593323, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236380696296692, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 297:\n",
      "  Training Loss: 0.5321322083473206, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236215591430664, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 298:\n",
      "  Training Loss: 0.5321243405342102, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236364603042603, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 299:\n",
      "  Training Loss: 0.5321378111839294, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235527753829956, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 300:\n",
      "  Training Loss: 0.5321264266967773, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236607193946838, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 301:\n",
      "  Training Loss: 0.5321371555328369, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235574841499329, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 302:\n",
      "  Training Loss: 0.5321363806724548, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235340595245361, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 303:\n",
      "  Training Loss: 0.532126784324646, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523694634437561, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 304:\n",
      "  Training Loss: 0.5321238040924072, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236713886260986, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 305:\n",
      "  Training Loss: 0.5321524143218994, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235579609870911, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 306:\n",
      "  Training Loss: 0.5321280360221863, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236295461654663, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 307:\n",
      "  Training Loss: 0.53214430809021, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235421657562256, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 308:\n",
      "  Training Loss: 0.532131016254425, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236369967460632, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 309:\n",
      "  Training Loss: 0.5321343541145325, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236024856567383, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 310:\n",
      "  Training Loss: 0.5321124196052551, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235109329223633, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 311:\n",
      "  Training Loss: 0.5321237444877625, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236278176307678, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 312:\n",
      "  Training Loss: 0.5321457386016846, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235626697540283, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 313:\n",
      "  Training Loss: 0.5321243405342102, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234922170639038, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 314:\n",
      "  Training Loss: 0.5321267247200012, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236417055130005, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 315:\n",
      "  Training Loss: 0.5321293473243713, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234943628311157, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 316:\n",
      "  Training Loss: 0.5321479439735413, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235876441001892, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 317:\n",
      "  Training Loss: 0.5321357250213623, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235467553138733, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 318:\n",
      "  Training Loss: 0.532145082950592, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235650539398193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 319:\n",
      "  Training Loss: 0.5321370363235474, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52358078956604, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 320:\n",
      "  Training Loss: 0.5321410894393921, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235210061073303, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 321:\n",
      "  Training Loss: 0.5321375131607056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235811471939087, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 322:\n",
      "  Training Loss: 0.532135009765625, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235598087310791, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 323:\n",
      "  Training Loss: 0.5321330428123474, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236541628837585, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 324:\n",
      "  Training Loss: 0.5321375727653503, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235213041305542, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 325:\n",
      "  Training Loss: 0.532139241695404, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235546231269836, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 326:\n",
      "  Training Loss: 0.532126247882843, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235759615898132, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 327:\n",
      "  Training Loss: 0.5321220755577087, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235187411308289, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 328:\n",
      "  Training Loss: 0.5321352481842041, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235463976860046, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 329:\n",
      "  Training Loss: 0.5321245193481445, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235933065414429, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 330:\n",
      "  Training Loss: 0.5321279168128967, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236085057258606, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 331:\n",
      "  Training Loss: 0.5321286916732788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236616134643555, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 332:\n",
      "  Training Loss: 0.5321144461631775, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235108137130737, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 333:\n",
      "  Training Loss: 0.5321474075317383, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234917998313904, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 334:\n",
      "  Training Loss: 0.5321373343467712, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234922170639038, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 335:\n",
      "  Training Loss: 0.5321381688117981, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235702991485596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 336:\n",
      "  Training Loss: 0.5321203470230103, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235910415649414, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 337:\n",
      "  Training Loss: 0.5321431159973145, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235214233398438, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 338:\n",
      "  Training Loss: 0.5321261286735535, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235573053359985, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 339:\n",
      "  Training Loss: 0.5321354866027832, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235997438430786, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 340:\n",
      "  Training Loss: 0.5321356058120728, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235403180122375, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 341:\n",
      "  Training Loss: 0.5321359038352966, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235744118690491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 342:\n",
      "  Training Loss: 0.5320992469787598, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234529376029968, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 343:\n",
      "  Training Loss: 0.5321365594863892, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235913991928101, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 344:\n",
      "  Training Loss: 0.5321303606033325, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236550569534302, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 345:\n",
      "  Training Loss: 0.5321393609046936, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235728621482849, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 346:\n",
      "  Training Loss: 0.5321203470230103, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235267877578735, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 347:\n",
      "  Training Loss: 0.5321328043937683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52351313829422, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 348:\n",
      "  Training Loss: 0.5321286916732788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235205292701721, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 349:\n",
      "  Training Loss: 0.532135009765625, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235429406166077, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 350:\n",
      "  Training Loss: 0.5321228504180908, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237138867378235, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 351:\n",
      "  Training Loss: 0.5321267247200012, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235990285873413, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 352:\n",
      "  Training Loss: 0.5321018099784851, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237641334533691, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 353:\n",
      "  Training Loss: 0.5321475267410278, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235130190849304, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 354:\n",
      "  Training Loss: 0.5321410894393921, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235072374343872, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 355:\n",
      "  Training Loss: 0.5321329832077026, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236379504203796, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 356:\n",
      "  Training Loss: 0.5321341156959534, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523630678653717, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 357:\n",
      "  Training Loss: 0.5321333408355713, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235469341278076, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 358:\n",
      "  Training Loss: 0.5321238040924072, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236616134643555, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 359:\n",
      "  Training Loss: 0.5321505069732666, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235415101051331, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 360:\n",
      "  Training Loss: 0.5321187376976013, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236220955848694, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 361:\n",
      "  Training Loss: 0.5321357250213623, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236832499504089, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 362:\n",
      "  Training Loss: 0.5321300029754639, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236024260520935, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 363:\n",
      "  Training Loss: 0.5321424603462219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235140323638916, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 364:\n",
      "  Training Loss: 0.5321334004402161, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235415101051331, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 365:\n",
      "  Training Loss: 0.5321317911148071, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523612916469574, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 366:\n",
      "  Training Loss: 0.5321322679519653, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235971808433533, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 367:\n",
      "  Training Loss: 0.5321343541145325, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236240029335022, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 368:\n",
      "  Training Loss: 0.5321337580680847, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523597776889801, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 369:\n",
      "  Training Loss: 0.5321271419525146, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236296057701111, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 370:\n",
      "  Training Loss: 0.5321428179740906, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235985517501831, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 371:\n",
      "  Training Loss: 0.5321434140205383, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236143469810486, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 372:\n",
      "  Training Loss: 0.5321277976036072, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235301852226257, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 373:\n",
      "  Training Loss: 0.5321308374404907, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235925316810608, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 374:\n",
      "  Training Loss: 0.5321372151374817, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235536694526672, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 375:\n",
      "  Training Loss: 0.5321369171142578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234836935997009, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 376:\n",
      "  Training Loss: 0.532134473323822, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523592472076416, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 377:\n",
      "  Training Loss: 0.5321369767189026, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235307812690735, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 378:\n",
      "  Training Loss: 0.5321367979049683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235448479652405, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 379:\n",
      "  Training Loss: 0.5321218967437744, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234985947608948, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 380:\n",
      "  Training Loss: 0.5321348309516907, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235826373100281, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 381:\n",
      "  Training Loss: 0.5321347713470459, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235493779182434, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 382:\n",
      "  Training Loss: 0.5321270227432251, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235093832015991, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 383:\n",
      "  Training Loss: 0.5321336388587952, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235117077827454, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 384:\n",
      "  Training Loss: 0.5321356058120728, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235602259635925, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 385:\n",
      "  Training Loss: 0.5321272611618042, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235492587089539, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 386:\n",
      "  Training Loss: 0.5321393013000488, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236359238624573, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 387:\n",
      "  Training Loss: 0.5321502089500427, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236034989356995, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 388:\n",
      "  Training Loss: 0.5321314334869385, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235385894775391, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 389:\n",
      "  Training Loss: 0.532126784324646, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234980583190918, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 390:\n",
      "  Training Loss: 0.5321494936943054, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235120058059692, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 391:\n",
      "  Training Loss: 0.5321094393730164, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236551761627197, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 392:\n",
      "  Training Loss: 0.5321345925331116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729813575745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 393:\n",
      "  Training Loss: 0.5321177244186401, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234851837158203, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 394:\n",
      "  Training Loss: 0.5321286916732788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523649275302887, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 395:\n",
      "  Training Loss: 0.5321397185325623, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235353708267212, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 396:\n",
      "  Training Loss: 0.5321325659751892, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235375761985779, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 397:\n",
      "  Training Loss: 0.5321475863456726, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235823392868042, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 398:\n",
      "  Training Loss: 0.5321322083473206, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236104726791382, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 399:\n",
      "  Training Loss: 0.5321176648139954, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52349454164505, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 400:\n",
      "  Training Loss: 0.5321370959281921, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235236287117004, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 401:\n",
      "  Training Loss: 0.5321440100669861, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235985517501831, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 402:\n",
      "  Training Loss: 0.5321286916732788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236101150512695, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 403:\n",
      "  Training Loss: 0.5321534872055054, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235429406166077, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 404:\n",
      "  Training Loss: 0.5321334600448608, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236005783081055, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 405:\n",
      "  Training Loss: 0.5321203470230103, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234976410865784, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 406:\n",
      "  Training Loss: 0.532148540019989, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236019492149353, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 407:\n",
      "  Training Loss: 0.5321359634399414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235748887062073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 408:\n",
      "  Training Loss: 0.5321311354637146, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235428214073181, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 409:\n",
      "  Training Loss: 0.5321340560913086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234928131103516, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 410:\n",
      "  Training Loss: 0.5321457982063293, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52354896068573, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 411:\n",
      "  Training Loss: 0.5321434140205383, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236353874206543, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 412:\n",
      "  Training Loss: 0.5321230292320251, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236607193946838, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 413:\n",
      "  Training Loss: 0.5321500301361084, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236387848854065, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 414:\n",
      "  Training Loss: 0.5321494936943054, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235654711723328, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 415:\n",
      "  Training Loss: 0.5321298837661743, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235639810562134, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 416:\n",
      "  Training Loss: 0.5321310758590698, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235263705253601, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 417:\n",
      "  Training Loss: 0.532124400138855, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234994888305664, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 418:\n",
      "  Training Loss: 0.5321354269981384, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 419:\n",
      "  Training Loss: 0.5321406722068787, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235640406608582, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 420:\n",
      "  Training Loss: 0.5321335196495056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235255360603333, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 421:\n",
      "  Training Loss: 0.5321143865585327, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234844088554382, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 422:\n",
      "  Training Loss: 0.5321164131164551, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236908197402954, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 423:\n",
      "  Training Loss: 0.5321810245513916, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236407518386841, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 424:\n",
      "  Training Loss: 0.5321135520935059, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236313939094543, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 425:\n",
      "  Training Loss: 0.5321329236030579, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235319137573242, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 426:\n",
      "  Training Loss: 0.5321243405342102, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235359072685242, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 427:\n",
      "  Training Loss: 0.5321370959281921, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523552656173706, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 428:\n",
      "  Training Loss: 0.5321459770202637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235967636108398, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 429:\n",
      "  Training Loss: 0.5321270227432251, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235105752944946, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 430:\n",
      "  Training Loss: 0.5321256518363953, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235992074012756, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 431:\n",
      "  Training Loss: 0.5321412086486816, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235861539840698, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 432:\n",
      "  Training Loss: 0.5321312546730042, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236280560493469, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 433:\n",
      "  Training Loss: 0.532127857208252, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236698389053345, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 434:\n",
      "  Training Loss: 0.5321398377418518, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235779285430908, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 435:\n",
      "  Training Loss: 0.5321289300918579, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235273241996765, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 436:\n",
      "  Training Loss: 0.532092273235321, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237257480621338, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 437:\n",
      "  Training Loss: 0.5321506261825562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236216187477112, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 438:\n",
      "  Training Loss: 0.5321359634399414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235782265663147, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 439:\n",
      "  Training Loss: 0.5321364402770996, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235874056816101, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 440:\n",
      "  Training Loss: 0.5321230292320251, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236070156097412, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 441:\n",
      "  Training Loss: 0.5321255326271057, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523487389087677, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 442:\n",
      "  Training Loss: 0.5321232676506042, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236837267875671, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 443:\n",
      "  Training Loss: 0.5321405529975891, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236061811447144, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 444:\n",
      "  Training Loss: 0.5321328043937683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 445:\n",
      "  Training Loss: 0.5321138501167297, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234735608100891, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 446:\n",
      "  Training Loss: 0.5321371555328369, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234996676445007, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 447:\n",
      "  Training Loss: 0.5321370959281921, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235093832015991, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 448:\n",
      "  Training Loss: 0.5321451425552368, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235116481781006, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 449:\n",
      "  Training Loss: 0.5321484208106995, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235574841499329, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 450:\n",
      "  Training Loss: 0.5321489572525024, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523505687713623, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 451:\n",
      "  Training Loss: 0.5321331024169922, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236439108848572, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 452:\n",
      "  Training Loss: 0.5321279168128967, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236493945121765, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 453:\n",
      "  Training Loss: 0.5321386456489563, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236198306083679, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 454:\n",
      "  Training Loss: 0.5321230888366699, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235255360603333, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 455:\n",
      "  Training Loss: 0.5321188569068909, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236250162124634, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 456:\n",
      "  Training Loss: 0.5321212410926819, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236234068870544, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 457:\n",
      "  Training Loss: 0.5321348905563354, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235165357589722, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 458:\n",
      "  Training Loss: 0.5321246385574341, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234894752502441, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 459:\n",
      "  Training Loss: 0.5321454405784607, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235478281974792, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 460:\n",
      "  Training Loss: 0.532139778137207, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236181020736694, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 461:\n",
      "  Training Loss: 0.5321289896965027, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235787630081177, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 462:\n",
      "  Training Loss: 0.5321285724639893, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235308408737183, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 463:\n",
      "  Training Loss: 0.5321330428123474, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236318111419678, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 464:\n",
      "  Training Loss: 0.532137930393219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235986709594727, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 465:\n",
      "  Training Loss: 0.5321343541145325, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235030055046082, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 466:\n",
      "  Training Loss: 0.5321255326271057, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236085057258606, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 467:\n",
      "  Training Loss: 0.5321111083030701, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236882567405701, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 468:\n",
      "  Training Loss: 0.5321269035339355, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235403180122375, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 469:\n",
      "  Training Loss: 0.5321385264396667, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235162377357483, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 470:\n",
      "  Training Loss: 0.5321086049079895, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234811902046204, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 471:\n",
      "  Training Loss: 0.5321366786956787, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523563027381897, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 472:\n",
      "  Training Loss: 0.5321314334869385, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235512852668762, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 473:\n",
      "  Training Loss: 0.5321091413497925, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236927270889282, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 474:\n",
      "  Training Loss: 0.5321608781814575, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523566484451294, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 475:\n",
      "  Training Loss: 0.5321319103240967, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235868096351624, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 476:\n",
      "  Training Loss: 0.5321452021598816, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235791206359863, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 477:\n",
      "  Training Loss: 0.5321462750434875, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523600697517395, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 478:\n",
      "  Training Loss: 0.5321326851844788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236341953277588, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 479:\n",
      "  Training Loss: 0.5321495532989502, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523594081401825, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 480:\n",
      "  Training Loss: 0.5321386456489563, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235899090766907, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 481:\n",
      "  Training Loss: 0.5321359038352966, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523566484451294, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 482:\n",
      "  Training Loss: 0.5321215987205505, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236049294471741, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 483:\n",
      "  Training Loss: 0.5321324467658997, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235305428504944, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 484:\n",
      "  Training Loss: 0.5321379899978638, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523552417755127, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 485:\n",
      "  Training Loss: 0.5321250557899475, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234731435775757, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 486:\n",
      "  Training Loss: 0.5321398973464966, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234731435775757, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 487:\n",
      "  Training Loss: 0.5321353077888489, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235882997512817, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 488:\n",
      "  Training Loss: 0.5321113467216492, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235112905502319, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 489:\n",
      "  Training Loss: 0.532127320766449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235345959663391, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 490:\n",
      "  Training Loss: 0.5321031808853149, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236856341362, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 491:\n",
      "  Training Loss: 0.5321424603462219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235196352005005, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 492:\n",
      "  Training Loss: 0.532145619392395, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234963297843933, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 493:\n",
      "  Training Loss: 0.532136857509613, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235588550567627, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 494:\n",
      "  Training Loss: 0.5321261286735535, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234947204589844, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 495:\n",
      "  Training Loss: 0.5321470499038696, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235863924026489, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 496:\n",
      "  Training Loss: 0.5321046113967896, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234749913215637, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 497:\n",
      "  Training Loss: 0.532164990901947, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523515522480011, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 498:\n",
      "  Training Loss: 0.5321338176727295, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235100388526917, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 499:\n",
      "  Training Loss: 0.5321502685546875, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 500:\n",
      "  Training Loss: 0.5321153402328491, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235012769699097, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 501:\n",
      "  Training Loss: 0.5321500897407532, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234926342964172, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 502:\n",
      "  Training Loss: 0.5321056842803955, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523467481136322, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 503:\n",
      "  Training Loss: 0.5321416854858398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236585736274719, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 504:\n",
      "  Training Loss: 0.532145082950592, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235857367515564, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 505:\n",
      "  Training Loss: 0.5321474671363831, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235489010810852, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 506:\n",
      "  Training Loss: 0.5321058034896851, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234929919242859, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 507:\n",
      "  Training Loss: 0.5321223735809326, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236223936080933, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 508:\n",
      "  Training Loss: 0.5321511626243591, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236181020736694, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 509:\n",
      "  Training Loss: 0.5321341753005981, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236002802848816, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 510:\n",
      "  Training Loss: 0.5321230888366699, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235307216644287, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 511:\n",
      "  Training Loss: 0.5321317315101624, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236287117004395, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 512:\n",
      "  Training Loss: 0.5321358442306519, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235642194747925, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 513:\n",
      "  Training Loss: 0.5321282148361206, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235166549682617, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 514:\n",
      "  Training Loss: 0.5321413278579712, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235486626625061, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 515:\n",
      "  Training Loss: 0.5321276187896729, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236381888389587, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 516:\n",
      "  Training Loss: 0.5321479439735413, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236356854438782, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 517:\n",
      "  Training Loss: 0.5321412682533264, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235392451286316, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 518:\n",
      "  Training Loss: 0.5321291089057922, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235173106193542, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 519:\n",
      "  Training Loss: 0.5321252942085266, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234991312026978, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 520:\n",
      "  Training Loss: 0.5321268439292908, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236384272575378, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 521:\n",
      "  Training Loss: 0.5321364402770996, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235628485679626, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 522:\n",
      "  Training Loss: 0.532146155834198, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235580801963806, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 523:\n",
      "  Training Loss: 0.5321304202079773, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236093401908875, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 524:\n",
      "  Training Loss: 0.5321416854858398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236165523529053, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 525:\n",
      "  Training Loss: 0.5321347713470459, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235479474067688, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 526:\n",
      "  Training Loss: 0.532136082649231, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235380530357361, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 527:\n",
      "  Training Loss: 0.5321288704872131, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236197710037231, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 528:\n",
      "  Training Loss: 0.5321389436721802, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236654281616211, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 529:\n",
      "  Training Loss: 0.5321364402770996, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235471725463867, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 530:\n",
      "  Training Loss: 0.5321344137191772, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235899090766907, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 531:\n",
      "  Training Loss: 0.5321024060249329, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237634778022766, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 532:\n",
      "  Training Loss: 0.5321588516235352, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523590087890625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 533:\n",
      "  Training Loss: 0.5321208238601685, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236577391624451, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 534:\n",
      "  Training Loss: 0.5321337580680847, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235843062400818, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 535:\n",
      "  Training Loss: 0.5321308374404907, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235351920127869, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 536:\n",
      "  Training Loss: 0.5321136116981506, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236717462539673, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 537:\n",
      "  Training Loss: 0.5321403741836548, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235642194747925, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 538:\n",
      "  Training Loss: 0.5321313738822937, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235204696655273, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 539:\n",
      "  Training Loss: 0.5321285128593445, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235468745231628, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 540:\n",
      "  Training Loss: 0.5321390628814697, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235624313354492, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 541:\n",
      "  Training Loss: 0.5321237444877625, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234772562980652, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 542:\n",
      "  Training Loss: 0.5321627259254456, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235572457313538, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 543:\n",
      "  Training Loss: 0.5321332812309265, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236204862594604, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 544:\n",
      "  Training Loss: 0.532124936580658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697627067566, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 545:\n",
      "  Training Loss: 0.5321331024169922, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235064625740051, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 546:\n",
      "  Training Loss: 0.5321175456047058, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234970450401306, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 547:\n",
      "  Training Loss: 0.5321405529975891, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523549497127533, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 548:\n",
      "  Training Loss: 0.5321239829063416, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234947204589844, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 549:\n",
      "  Training Loss: 0.5321152210235596, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236480236053467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 550:\n",
      "  Training Loss: 0.5321347117424011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235807299613953, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 551:\n",
      "  Training Loss: 0.5321313738822937, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235552787780762, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 552:\n",
      "  Training Loss: 0.5321325659751892, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236040353775024, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 553:\n",
      "  Training Loss: 0.5321613550186157, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235878825187683, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 554:\n",
      "  Training Loss: 0.5321305990219116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236155986785889, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 555:\n",
      "  Training Loss: 0.5321435928344727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235635042190552, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 556:\n",
      "  Training Loss: 0.532128632068634, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235468745231628, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 557:\n",
      "  Training Loss: 0.5321252942085266, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235986709594727, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 558:\n",
      "  Training Loss: 0.5321254730224609, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235623121261597, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 559:\n",
      "  Training Loss: 0.5321283340454102, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235345959663391, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 560:\n",
      "  Training Loss: 0.5321292877197266, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235614776611328, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 561:\n",
      "  Training Loss: 0.5321378111839294, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235187411308289, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 562:\n",
      "  Training Loss: 0.5321522951126099, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235037207603455, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 563:\n",
      "  Training Loss: 0.5320607423782349, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237438082695007, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 564:\n",
      "  Training Loss: 0.5321687459945679, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236161351203918, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 565:\n",
      "  Training Loss: 0.5321376323699951, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523541271686554, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 566:\n",
      "  Training Loss: 0.5321338772773743, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236361026763916, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 567:\n",
      "  Training Loss: 0.5321341156959534, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235782861709595, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 568:\n",
      "  Training Loss: 0.5321283340454102, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235748291015625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 569:\n",
      "  Training Loss: 0.5321168899536133, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523611843585968, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 570:\n",
      "  Training Loss: 0.5321379899978638, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236492156982422, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 571:\n",
      "  Training Loss: 0.5321521759033203, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236201286315918, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 572:\n",
      "  Training Loss: 0.5321372151374817, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523577868938446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 573:\n",
      "  Training Loss: 0.5321557521820068, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234977006912231, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 574:\n",
      "  Training Loss: 0.5321483612060547, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235321521759033, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 575:\n",
      "  Training Loss: 0.5321369767189026, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236226916313171, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 576:\n",
      "  Training Loss: 0.5321111083030701, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234899520874023, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 577:\n",
      "  Training Loss: 0.5321187973022461, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236409902572632, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 578:\n",
      "  Training Loss: 0.5321319103240967, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235173106193542, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 579:\n",
      "  Training Loss: 0.5321428179740906, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235478281974792, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 580:\n",
      "  Training Loss: 0.5321212410926819, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235875844955444, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 581:\n",
      "  Training Loss: 0.5321229696273804, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236250162124634, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 582:\n",
      "  Training Loss: 0.5321157574653625, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523742139339447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 583:\n",
      "  Training Loss: 0.5321367383003235, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 584:\n",
      "  Training Loss: 0.5321322083473206, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235880613327026, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 585:\n",
      "  Training Loss: 0.5321234464645386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235176682472229, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 586:\n",
      "  Training Loss: 0.5321145057678223, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234799981117249, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 587:\n",
      "  Training Loss: 0.5321426391601562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235544443130493, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 588:\n",
      "  Training Loss: 0.5321455597877502, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236195921897888, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 589:\n",
      "  Training Loss: 0.5321313738822937, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235698223114014, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 590:\n",
      "  Training Loss: 0.5321113467216492, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237061977386475, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 591:\n",
      "  Training Loss: 0.5321687459945679, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236188769340515, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 592:\n",
      "  Training Loss: 0.5321239233016968, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523505449295044, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 593:\n",
      "  Training Loss: 0.5321224927902222, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234884023666382, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 594:\n",
      "  Training Loss: 0.5321321487426758, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236341953277588, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 595:\n",
      "  Training Loss: 0.5321442484855652, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236037969589233, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 596:\n",
      "  Training Loss: 0.5321483612060547, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236058235168457, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 597:\n",
      "  Training Loss: 0.5321319699287415, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235323309898376, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 598:\n",
      "  Training Loss: 0.5321215391159058, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523686945438385, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 599:\n",
      "  Training Loss: 0.532177746295929, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235573053359985, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 600:\n",
      "  Training Loss: 0.532129168510437, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52351313829422, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 601:\n",
      "  Training Loss: 0.5321528911590576, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235661268234253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 602:\n",
      "  Training Loss: 0.5321286916732788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236306190490723, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 603:\n",
      "  Training Loss: 0.5321146249771118, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234978199005127, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 604:\n",
      "  Training Loss: 0.5321442484855652, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235541462898254, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 605:\n",
      "  Training Loss: 0.5321367383003235, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 606:\n",
      "  Training Loss: 0.5321250557899475, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235668420791626, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 607:\n",
      "  Training Loss: 0.5321609377861023, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235270261764526, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 608:\n",
      "  Training Loss: 0.5321258306503296, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235288739204407, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 609:\n",
      "  Training Loss: 0.5321468114852905, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235588550567627, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 610:\n",
      "  Training Loss: 0.5321284532546997, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235546827316284, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 611:\n",
      "  Training Loss: 0.5321270227432251, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235899090766907, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 612:\n",
      "  Training Loss: 0.5321367383003235, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235888957977295, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 613:\n",
      "  Training Loss: 0.5321328043937683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235196352005005, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 614:\n",
      "  Training Loss: 0.5321212410926819, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234934687614441, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 615:\n",
      "  Training Loss: 0.5321363806724548, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235589146614075, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 616:\n",
      "  Training Loss: 0.5321237444877625, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235835909843445, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 617:\n",
      "  Training Loss: 0.532133936882019, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236053466796875, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 618:\n",
      "  Training Loss: 0.5321303009986877, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235902667045593, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 619:\n",
      "  Training Loss: 0.5321169495582581, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236629843711853, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 620:\n",
      "  Training Loss: 0.5321322679519653, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235885381698608, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 621:\n",
      "  Training Loss: 0.5321211814880371, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236388444900513, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 622:\n",
      "  Training Loss: 0.5321521162986755, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235812664031982, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 623:\n",
      "  Training Loss: 0.5321270823478699, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236607789993286, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 624:\n",
      "  Training Loss: 0.5321264863014221, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236868262290955, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 625:\n",
      "  Training Loss: 0.5321298837661743, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236305594444275, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 626:\n",
      "  Training Loss: 0.5321394801139832, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235803127288818, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 627:\n",
      "  Training Loss: 0.5321335196495056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236186981201172, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 628:\n",
      "  Training Loss: 0.5321409106254578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235368609428406, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 629:\n",
      "  Training Loss: 0.5321291089057922, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236029028892517, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 630:\n",
      "  Training Loss: 0.5321398377418518, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235408544540405, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 631:\n",
      "  Training Loss: 0.5321462750434875, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236315131187439, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 632:\n",
      "  Training Loss: 0.5321371555328369, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235388875007629, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 633:\n",
      "  Training Loss: 0.5321264266967773, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235021710395813, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 634:\n",
      "  Training Loss: 0.532150387763977, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235494375228882, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 635:\n",
      "  Training Loss: 0.5321325063705444, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235915780067444, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 636:\n",
      "  Training Loss: 0.532140851020813, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235834717750549, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 637:\n",
      "  Training Loss: 0.532118022441864, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235010385513306, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 638:\n",
      "  Training Loss: 0.5321373343467712, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236805081367493, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 639:\n",
      "  Training Loss: 0.5321331024169922, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236616134643555, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 640:\n",
      "  Training Loss: 0.5321098566055298, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237650871276855, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 641:\n",
      "  Training Loss: 0.5321522951126099, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235605835914612, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 642:\n",
      "  Training Loss: 0.5321417450904846, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235914587974548, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 643:\n",
      "  Training Loss: 0.5321303606033325, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235172510147095, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 644:\n",
      "  Training Loss: 0.5321073532104492, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234614610671997, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 645:\n",
      "  Training Loss: 0.5321577787399292, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235614776611328, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 646:\n",
      "  Training Loss: 0.5321389436721802, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235834121704102, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 647:\n",
      "  Training Loss: 0.5321311354637146, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235867500305176, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 648:\n",
      "  Training Loss: 0.5321351289749146, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235273241996765, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 649:\n",
      "  Training Loss: 0.5321272015571594, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523571252822876, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 650:\n",
      "  Training Loss: 0.5321308374404907, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236565470695496, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 651:\n",
      "  Training Loss: 0.5321411490440369, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236013531684875, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 652:\n",
      "  Training Loss: 0.5321272015571594, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235169529914856, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 653:\n",
      "  Training Loss: 0.5321181416511536, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236386060714722, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 654:\n",
      "  Training Loss: 0.5321222543716431, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237576365470886, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 655:\n",
      "  Training Loss: 0.532146155834198, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235845446586609, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 656:\n",
      "  Training Loss: 0.5321425795555115, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235508680343628, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 657:\n",
      "  Training Loss: 0.5321382284164429, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236243605613708, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 658:\n",
      "  Training Loss: 0.5321252942085266, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235645771026611, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 659:\n",
      "  Training Loss: 0.5321468114852905, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236397385597229, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 660:\n",
      "  Training Loss: 0.5321397185325623, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235824584960938, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 661:\n",
      "  Training Loss: 0.5321251749992371, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234996676445007, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 662:\n",
      "  Training Loss: 0.532128632068634, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236181616783142, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 663:\n",
      "  Training Loss: 0.5321356058120728, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236032009124756, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 664:\n",
      "  Training Loss: 0.53214031457901, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235327482223511, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 665:\n",
      "  Training Loss: 0.5321458578109741, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523529052734375, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 666:\n",
      "  Training Loss: 0.5321412682533264, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 667:\n",
      "  Training Loss: 0.5321323871612549, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235942006111145, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 668:\n",
      "  Training Loss: 0.5321298241615295, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523591160774231, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 669:\n",
      "  Training Loss: 0.5321350693702698, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236966609954834, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 670:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235479474067688, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 671:\n",
      "  Training Loss: 0.53215092420578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235165953636169, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 672:\n",
      "  Training Loss: 0.5321394205093384, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235390067100525, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 673:\n",
      "  Training Loss: 0.5321279168128967, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235449075698853, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 674:\n",
      "  Training Loss: 0.5321241617202759, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236495733261108, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 675:\n",
      "  Training Loss: 0.5321345329284668, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713720321655, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 676:\n",
      "  Training Loss: 0.5321304202079773, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236174464225769, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 677:\n",
      "  Training Loss: 0.5321292281150818, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235278606414795, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 678:\n",
      "  Training Loss: 0.5321183800697327, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236222147941589, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 679:\n",
      "  Training Loss: 0.5321284532546997, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236539840698242, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 680:\n",
      "  Training Loss: 0.5321424603462219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235711336135864, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 681:\n",
      "  Training Loss: 0.5321445465087891, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523539125919342, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 682:\n",
      "  Training Loss: 0.5321220755577087, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234880447387695, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 683:\n",
      "  Training Loss: 0.5321346521377563, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523621678352356, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 684:\n",
      "  Training Loss: 0.5321308374404907, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235349535942078, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 685:\n",
      "  Training Loss: 0.5321250557899475, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235280394554138, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 686:\n",
      "  Training Loss: 0.5321386456489563, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235520601272583, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 687:\n",
      "  Training Loss: 0.5321141481399536, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234928131103516, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 688:\n",
      "  Training Loss: 0.5321351289749146, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235647559165955, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 689:\n",
      "  Training Loss: 0.5321238040924072, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235905647277832, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 690:\n",
      "  Training Loss: 0.5321394205093384, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235389471054077, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 691:\n",
      "  Training Loss: 0.532137393951416, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235980153083801, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 692:\n",
      "  Training Loss: 0.5321469902992249, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235571265220642, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 693:\n",
      "  Training Loss: 0.532132089138031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234975814819336, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 694:\n",
      "  Training Loss: 0.5321313142776489, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235169529914856, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 695:\n",
      "  Training Loss: 0.5321522951126099, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235527753829956, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 696:\n",
      "  Training Loss: 0.5321321487426758, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236069560050964, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 697:\n",
      "  Training Loss: 0.532133162021637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236313343048096, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 698:\n",
      "  Training Loss: 0.5321149826049805, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523475706577301, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 699:\n",
      "  Training Loss: 0.5321370959281921, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235390067100525, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 700:\n",
      "  Training Loss: 0.5321167707443237, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234652757644653, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 701:\n",
      "  Training Loss: 0.5321395993232727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235396027565002, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 702:\n",
      "  Training Loss: 0.532163679599762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235690474510193, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 703:\n",
      "  Training Loss: 0.5321291089057922, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235762000083923, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 704:\n",
      "  Training Loss: 0.5321313142776489, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235700607299805, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 705:\n",
      "  Training Loss: 0.5321258306503296, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 706:\n",
      "  Training Loss: 0.5321340560913086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235444903373718, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 707:\n",
      "  Training Loss: 0.5321412086486816, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236138701438904, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 708:\n",
      "  Training Loss: 0.5321102738380432, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236775279045105, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 709:\n",
      "  Training Loss: 0.5321457386016846, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236189365386963, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 710:\n",
      "  Training Loss: 0.5321285724639893, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235894322395325, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 711:\n",
      "  Training Loss: 0.532143235206604, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235967040061951, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 712:\n",
      "  Training Loss: 0.5321245193481445, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523544430732727, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 713:\n",
      "  Training Loss: 0.5320934057235718, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237688422203064, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 714:\n",
      "  Training Loss: 0.5321484804153442, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235906839370728, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 715:\n",
      "  Training Loss: 0.5321382284164429, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235604047775269, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 716:\n",
      "  Training Loss: 0.5321326851844788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235797166824341, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 717:\n",
      "  Training Loss: 0.5321531295776367, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523577868938446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 718:\n",
      "  Training Loss: 0.5321459174156189, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236179232597351, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 719:\n",
      "  Training Loss: 0.5321107506752014, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234873294830322, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 720:\n",
      "  Training Loss: 0.5321307182312012, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235514640808105, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 721:\n",
      "  Training Loss: 0.5321367979049683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235721468925476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 722:\n",
      "  Training Loss: 0.5321304798126221, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235413908958435, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 723:\n",
      "  Training Loss: 0.5321400165557861, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235568284988403, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 724:\n",
      "  Training Loss: 0.532146155834198, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235872864723206, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 725:\n",
      "  Training Loss: 0.5321298837661743, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235669612884521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 726:\n",
      "  Training Loss: 0.5321238040924072, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52362459897995, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 727:\n",
      "  Training Loss: 0.5321474671363831, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235797762870789, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 728:\n",
      "  Training Loss: 0.5321345925331116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235057473182678, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 729:\n",
      "  Training Loss: 0.532113790512085, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236574411392212, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 730:\n",
      "  Training Loss: 0.5321451425552368, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236513018608093, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 731:\n",
      "  Training Loss: 0.5321282148361206, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236069560050964, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 732:\n",
      "  Training Loss: 0.5321559309959412, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235764384269714, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 733:\n",
      "  Training Loss: 0.5321369171142578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235426425933838, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 734:\n",
      "  Training Loss: 0.5321335196495056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235567092895508, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 735:\n",
      "  Training Loss: 0.5321182608604431, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234872698783875, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 736:\n",
      "  Training Loss: 0.5321282744407654, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235926508903503, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 737:\n",
      "  Training Loss: 0.532139003276825, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523551881313324, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 738:\n",
      "  Training Loss: 0.5321332812309265, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523612916469574, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 739:\n",
      "  Training Loss: 0.5321273803710938, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236592292785645, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 740:\n",
      "  Training Loss: 0.5321369767189026, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235731601715088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 741:\n",
      "  Training Loss: 0.5321255922317505, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237334966659546, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 742:\n",
      "  Training Loss: 0.5321325063705444, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235973000526428, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 743:\n",
      "  Training Loss: 0.5321400761604309, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235641002655029, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 744:\n",
      "  Training Loss: 0.5321143269538879, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236129760742188, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 745:\n",
      "  Training Loss: 0.5321488976478577, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235520005226135, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 746:\n",
      "  Training Loss: 0.5321301221847534, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235116481781006, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 747:\n",
      "  Training Loss: 0.5321402549743652, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234774351119995, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 748:\n",
      "  Training Loss: 0.5321458578109741, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235794186592102, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 749:\n",
      "  Training Loss: 0.5321115255355835, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523708701133728, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 750:\n",
      "  Training Loss: 0.5321458578109741, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235544443130493, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 751:\n",
      "  Training Loss: 0.5321288704872131, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236092209815979, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 752:\n",
      "  Training Loss: 0.5321375131607056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234975814819336, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 753:\n",
      "  Training Loss: 0.5321466326713562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235162377357483, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 754:\n",
      "  Training Loss: 0.5321388244628906, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235756635665894, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 755:\n",
      "  Training Loss: 0.5321442484855652, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235640406608582, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 756:\n",
      "  Training Loss: 0.5321381688117981, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235646367073059, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 757:\n",
      "  Training Loss: 0.5321367979049683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523630678653717, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 758:\n",
      "  Training Loss: 0.5321399569511414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236201882362366, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 759:\n",
      "  Training Loss: 0.5321340560913086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523615300655365, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 760:\n",
      "  Training Loss: 0.5321385860443115, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236501097679138, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 761:\n",
      "  Training Loss: 0.5321398377418518, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235697627067566, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 762:\n",
      "  Training Loss: 0.5321264266967773, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235278010368347, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 763:\n",
      "  Training Loss: 0.5321336388587952, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235251784324646, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 764:\n",
      "  Training Loss: 0.532139003276825, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235571265220642, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 765:\n",
      "  Training Loss: 0.5321085453033447, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237060785293579, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 766:\n",
      "  Training Loss: 0.5321047306060791, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235002636909485, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 767:\n",
      "  Training Loss: 0.5321382284164429, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236059427261353, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 768:\n",
      "  Training Loss: 0.5321381092071533, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235564708709717, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 769:\n",
      "  Training Loss: 0.532150387763977, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235030651092529, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 770:\n",
      "  Training Loss: 0.532139003276825, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235130786895752, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 771:\n",
      "  Training Loss: 0.5321363806724548, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235771536827087, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 772:\n",
      "  Training Loss: 0.5321353077888489, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235713124275208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 773:\n",
      "  Training Loss: 0.532153844833374, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236154794692993, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 774:\n",
      "  Training Loss: 0.532122015953064, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235479474067688, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 775:\n",
      "  Training Loss: 0.5321483016014099, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235826373100281, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 776:\n",
      "  Training Loss: 0.532148003578186, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234989523887634, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 777:\n",
      "  Training Loss: 0.5321348309516907, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236120223999023, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 778:\n",
      "  Training Loss: 0.5320993661880493, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234794616699219, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 779:\n",
      "  Training Loss: 0.5321359634399414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235070586204529, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 780:\n",
      "  Training Loss: 0.532124936580658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523618757724762, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 781:\n",
      "  Training Loss: 0.5321245193481445, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237548351287842, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 782:\n",
      "  Training Loss: 0.5321409106254578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235710740089417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 783:\n",
      "  Training Loss: 0.5321459174156189, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523612380027771, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 784:\n",
      "  Training Loss: 0.5321012735366821, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235110521316528, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 785:\n",
      "  Training Loss: 0.5320915579795837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236653685569763, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 786:\n",
      "  Training Loss: 0.5321367979049683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235958695411682, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 787:\n",
      "  Training Loss: 0.5321149826049805, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235010385513306, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 788:\n",
      "  Training Loss: 0.5321422815322876, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523495078086853, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 789:\n",
      "  Training Loss: 0.5321414470672607, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523532509803772, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 790:\n",
      "  Training Loss: 0.5321433544158936, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235657095909119, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 791:\n",
      "  Training Loss: 0.5321155786514282, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234832763671875, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 792:\n",
      "  Training Loss: 0.532140851020813, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235083699226379, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 793:\n",
      "  Training Loss: 0.5321511030197144, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 794:\n",
      "  Training Loss: 0.5321201086044312, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236027240753174, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 795:\n",
      "  Training Loss: 0.5321117043495178, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235627293586731, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 796:\n",
      "  Training Loss: 0.532131552696228, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235772132873535, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 797:\n",
      "  Training Loss: 0.5321395397186279, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234944820404053, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 798:\n",
      "  Training Loss: 0.532146155834198, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236101150512695, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 799:\n",
      "  Training Loss: 0.5321416258811951, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235933065414429, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 800:\n",
      "  Training Loss: 0.5321295261383057, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236654281616211, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 801:\n",
      "  Training Loss: 0.5321485996246338, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523630678653717, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 802:\n",
      "  Training Loss: 0.5321382284164429, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523625373840332, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 803:\n",
      "  Training Loss: 0.5321332812309265, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235363245010376, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 804:\n",
      "  Training Loss: 0.532139003276825, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235921740531921, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 805:\n",
      "  Training Loss: 0.5321353077888489, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235443115234375, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 806:\n",
      "  Training Loss: 0.5321336984634399, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 807:\n",
      "  Training Loss: 0.5321195721626282, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236383080482483, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 808:\n",
      "  Training Loss: 0.532138466835022, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236402153968811, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 809:\n",
      "  Training Loss: 0.5321285128593445, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235062837600708, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 810:\n",
      "  Training Loss: 0.5321473479270935, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235605239868164, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 811:\n",
      "  Training Loss: 0.5321283936500549, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235559940338135, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 812:\n",
      "  Training Loss: 0.5321367979049683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235825181007385, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 813:\n",
      "  Training Loss: 0.5321242809295654, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235732793807983, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 814:\n",
      "  Training Loss: 0.5321456789970398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523562490940094, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 815:\n",
      "  Training Loss: 0.5321409702301025, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235190987586975, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 816:\n",
      "  Training Loss: 0.5321258902549744, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235119462013245, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 817:\n",
      "  Training Loss: 0.5321310758590698, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236070156097412, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 818:\n",
      "  Training Loss: 0.5321133732795715, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523746907711029, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 819:\n",
      "  Training Loss: 0.5321370959281921, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236080884933472, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 820:\n",
      "  Training Loss: 0.5321428179740906, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235670208930969, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 821:\n",
      "  Training Loss: 0.5321316719055176, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236071348190308, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 822:\n",
      "  Training Loss: 0.5321470499038696, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235483646392822, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 823:\n",
      "  Training Loss: 0.5321242809295654, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236012935638428, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 824:\n",
      "  Training Loss: 0.532134473323822, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237181782722473, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 825:\n",
      "  Training Loss: 0.532133162021637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235320925712585, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 826:\n",
      "  Training Loss: 0.5321140289306641, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237013101577759, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 827:\n",
      "  Training Loss: 0.5321468114852905, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235772132873535, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 828:\n",
      "  Training Loss: 0.5321352481842041, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235494375228882, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 829:\n",
      "  Training Loss: 0.5321323275566101, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235810875892639, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 830:\n",
      "  Training Loss: 0.5321237444877625, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236876010894775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 831:\n",
      "  Training Loss: 0.5321378707885742, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235115885734558, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 832:\n",
      "  Training Loss: 0.5321250557899475, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234807729721069, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 833:\n",
      "  Training Loss: 0.5321515798568726, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235339999198914, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 834:\n",
      "  Training Loss: 0.5321313738822937, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235478281974792, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 835:\n",
      "  Training Loss: 0.5321242213249207, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236817598342896, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 836:\n",
      "  Training Loss: 0.5321370959281921, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235223770141602, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 837:\n",
      "  Training Loss: 0.5321205258369446, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236440896987915, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 838:\n",
      "  Training Loss: 0.532124936580658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 839:\n",
      "  Training Loss: 0.5321189165115356, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236727595329285, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 840:\n",
      "  Training Loss: 0.5321577191352844, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235795974731445, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 841:\n",
      "  Training Loss: 0.5321326851844788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523529052734375, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 842:\n",
      "  Training Loss: 0.5321232080459595, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235289335250854, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 843:\n",
      "  Training Loss: 0.5321248173713684, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236568450927734, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 844:\n",
      "  Training Loss: 0.532149076461792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236379504203796, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 845:\n",
      "  Training Loss: 0.5321369171142578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523542582988739, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 846:\n",
      "  Training Loss: 0.5321322083473206, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235733985900879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 847:\n",
      "  Training Loss: 0.5321252346038818, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523504912853241, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 848:\n",
      "  Training Loss: 0.5321381688117981, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235368609428406, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 849:\n",
      "  Training Loss: 0.5321286916732788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236446857452393, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 850:\n",
      "  Training Loss: 0.5321234464645386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235477685928345, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 851:\n",
      "  Training Loss: 0.5321372747421265, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235809683799744, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 852:\n",
      "  Training Loss: 0.5321057438850403, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237308144569397, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 853:\n",
      "  Training Loss: 0.5321587324142456, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236251354217529, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 854:\n",
      "  Training Loss: 0.5321512222290039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708951950073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 855:\n",
      "  Training Loss: 0.5321329236030579, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236029028892517, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 856:\n",
      "  Training Loss: 0.5321028828620911, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237485766410828, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 857:\n",
      "  Training Loss: 0.5321633815765381, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236233472824097, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 858:\n",
      "  Training Loss: 0.5321329832077026, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235560536384583, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 859:\n",
      "  Training Loss: 0.5321426391601562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235463976860046, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 860:\n",
      "  Training Loss: 0.5321508049964905, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235516428947449, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 861:\n",
      "  Training Loss: 0.5321276187896729, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235940217971802, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 862:\n",
      "  Training Loss: 0.5321008563041687, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234881639480591, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 863:\n",
      "  Training Loss: 0.5321395993232727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235379934310913, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 864:\n",
      "  Training Loss: 0.5321405529975891, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523679792881012, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 865:\n",
      "  Training Loss: 0.5321370363235474, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235444903373718, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 866:\n",
      "  Training Loss: 0.5321345329284668, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235022902488708, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 867:\n",
      "  Training Loss: 0.5321396589279175, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235536694526672, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 868:\n",
      "  Training Loss: 0.5321217179298401, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236069560050964, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 869:\n",
      "  Training Loss: 0.5321339964866638, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235236883163452, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 870:\n",
      "  Training Loss: 0.5321233868598938, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235010981559753, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 871:\n",
      "  Training Loss: 0.5321508646011353, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235379934310913, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 872:\n",
      "  Training Loss: 0.5321407914161682, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235755443572998, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 873:\n",
      "  Training Loss: 0.5321288704872131, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235465168952942, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 874:\n",
      "  Training Loss: 0.5321321487426758, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235591530799866, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 875:\n",
      "  Training Loss: 0.5321335792541504, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235966444015503, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 876:\n",
      "  Training Loss: 0.5321357846260071, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235642194747925, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 877:\n",
      "  Training Loss: 0.5321300625801086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695838928223, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 878:\n",
      "  Training Loss: 0.5321234464645386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235985517501831, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 879:\n",
      "  Training Loss: 0.5321446061134338, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523632824420929, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 880:\n",
      "  Training Loss: 0.5321226716041565, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236272811889648, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 881:\n",
      "  Training Loss: 0.5321125984191895, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234873294830322, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 882:\n",
      "  Training Loss: 0.5321252942085266, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236377716064453, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 883:\n",
      "  Training Loss: 0.5321369171142578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235068202018738, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 884:\n",
      "  Training Loss: 0.5321402549743652, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235884785652161, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 885:\n",
      "  Training Loss: 0.5321363210678101, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235528349876404, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 886:\n",
      "  Training Loss: 0.532143771648407, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235236883163452, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 887:\n",
      "  Training Loss: 0.532136857509613, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235764980316162, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 888:\n",
      "  Training Loss: 0.5321386456489563, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235840678215027, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 889:\n",
      "  Training Loss: 0.5321313738822937, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235339403152466, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 890:\n",
      "  Training Loss: 0.5321305990219116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235959887504578, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 891:\n",
      "  Training Loss: 0.5321151614189148, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234867334365845, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 892:\n",
      "  Training Loss: 0.5321025848388672, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237208604812622, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 893:\n",
      "  Training Loss: 0.5321272611618042, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237239599227905, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 894:\n",
      "  Training Loss: 0.5321325063705444, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235311388969421, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 895:\n",
      "  Training Loss: 0.5321421027183533, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235136151313782, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 896:\n",
      "  Training Loss: 0.5321326851844788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234798789024353, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 897:\n",
      "  Training Loss: 0.5321357250213623, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236828327178955, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 898:\n",
      "  Training Loss: 0.5321447849273682, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523594319820404, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 899:\n",
      "  Training Loss: 0.532132089138031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236409306526184, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 900:\n",
      "  Training Loss: 0.5321183204650879, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523496687412262, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 901:\n",
      "  Training Loss: 0.5321370959281921, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235576033592224, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 902:\n",
      "  Training Loss: 0.5321234464645386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235864520072937, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 903:\n",
      "  Training Loss: 0.5321244597434998, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236285328865051, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 904:\n",
      "  Training Loss: 0.5321322679519653, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236515402793884, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 905:\n",
      "  Training Loss: 0.5321353673934937, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234900712966919, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 906:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235011577606201, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 907:\n",
      "  Training Loss: 0.5321296453475952, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236692428588867, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 908:\n",
      "  Training Loss: 0.5321452617645264, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691666603088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 909:\n",
      "  Training Loss: 0.532126247882843, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235574841499329, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 910:\n",
      "  Training Loss: 0.5321229696273804, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236631631851196, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 911:\n",
      "  Training Loss: 0.5321313142776489, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234998464584351, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 912:\n",
      "  Training Loss: 0.5321345925331116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235211849212646, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 913:\n",
      "  Training Loss: 0.5321393013000488, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235632061958313, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 914:\n",
      "  Training Loss: 0.5321090817451477, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234765410423279, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 915:\n",
      "  Training Loss: 0.5321649312973022, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235325694084167, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 916:\n",
      "  Training Loss: 0.5321344137191772, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235958099365234, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 917:\n",
      "  Training Loss: 0.5321124792098999, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234799981117249, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 918:\n",
      "  Training Loss: 0.5321351289749146, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235136151313782, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 919:\n",
      "  Training Loss: 0.532145082950592, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235193371772766, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 920:\n",
      "  Training Loss: 0.5321448445320129, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235646963119507, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 921:\n",
      "  Training Loss: 0.5321375727653503, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523490309715271, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 922:\n",
      "  Training Loss: 0.5321527123451233, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235810279846191, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 923:\n",
      "  Training Loss: 0.5321400165557861, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235303640365601, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 924:\n",
      "  Training Loss: 0.532138466835022, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234994888305664, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 925:\n",
      "  Training Loss: 0.5321066379547119, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236974954605103, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 926:\n",
      "  Training Loss: 0.5321578979492188, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236024260520935, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 927:\n",
      "  Training Loss: 0.5321321487426758, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235841274261475, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 928:\n",
      "  Training Loss: 0.5321226716041565, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236419439315796, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 929:\n",
      "  Training Loss: 0.5321237444877625, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234852433204651, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 930:\n",
      "  Training Loss: 0.5321491360664368, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235579013824463, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 931:\n",
      "  Training Loss: 0.5321338772773743, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236166715621948, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 932:\n",
      "  Training Loss: 0.5321335196495056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235345959663391, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 933:\n",
      "  Training Loss: 0.5321395993232727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235026478767395, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 934:\n",
      "  Training Loss: 0.5321383476257324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236196517944336, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 935:\n",
      "  Training Loss: 0.5321199297904968, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234898924827576, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 936:\n",
      "  Training Loss: 0.5321336984634399, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235617160797119, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 937:\n",
      "  Training Loss: 0.5321360230445862, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235772132873535, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 938:\n",
      "  Training Loss: 0.5321236252784729, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234861373901367, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 939:\n",
      "  Training Loss: 0.532139003276825, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235742330551147, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 940:\n",
      "  Training Loss: 0.5321275591850281, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235030055046082, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 941:\n",
      "  Training Loss: 0.5321062207221985, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236600041389465, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 942:\n",
      "  Training Loss: 0.5321340560913086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52350252866745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 943:\n",
      "  Training Loss: 0.5321223735809326, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234885811805725, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 944:\n",
      "  Training Loss: 0.5321508049964905, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235272645950317, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 945:\n",
      "  Training Loss: 0.5321309566497803, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235083699226379, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 946:\n",
      "  Training Loss: 0.5321385860443115, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235174298286438, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 947:\n",
      "  Training Loss: 0.5321347713470459, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236619114875793, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 948:\n",
      "  Training Loss: 0.5321425795555115, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236199498176575, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 949:\n",
      "  Training Loss: 0.5321405529975891, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523632824420929, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 950:\n",
      "  Training Loss: 0.5321252346038818, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236767530441284, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 951:\n",
      "  Training Loss: 0.5321261286735535, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235341787338257, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 952:\n",
      "  Training Loss: 0.532140851020813, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235369205474854, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 953:\n",
      "  Training Loss: 0.5321269035339355, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235803127288818, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 954:\n",
      "  Training Loss: 0.532133162021637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235581398010254, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 955:\n",
      "  Training Loss: 0.5321279764175415, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235458016395569, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 956:\n",
      "  Training Loss: 0.5321251749992371, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235840678215027, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 957:\n",
      "  Training Loss: 0.532133162021637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235176086425781, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 958:\n",
      "  Training Loss: 0.5321171283721924, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236182808876038, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 959:\n",
      "  Training Loss: 0.5321326851844788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235305428504944, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 960:\n",
      "  Training Loss: 0.5321398377418518, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234935283660889, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 961:\n",
      "  Training Loss: 0.5321497917175293, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234985947608948, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 962:\n",
      "  Training Loss: 0.5321508049964905, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236281156539917, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 963:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52354896068573, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 964:\n",
      "  Training Loss: 0.5321319103240967, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235379934310913, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 965:\n",
      "  Training Loss: 0.5321266055107117, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235812664031982, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 966:\n",
      "  Training Loss: 0.5321263670921326, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236435532569885, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 967:\n",
      "  Training Loss: 0.5321300625801086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235251784324646, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 968:\n",
      "  Training Loss: 0.5321425199508667, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234973430633545, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 969:\n",
      "  Training Loss: 0.5321490168571472, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235577821731567, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 970:\n",
      "  Training Loss: 0.5321458578109741, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236087441444397, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 971:\n",
      "  Training Loss: 0.532127320766449, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523631751537323, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 972:\n",
      "  Training Loss: 0.5321251749992371, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236234664916992, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 973:\n",
      "  Training Loss: 0.5321354866027832, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235574841499329, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 974:\n",
      "  Training Loss: 0.5321281552314758, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236029028892517, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 975:\n",
      "  Training Loss: 0.532131552696228, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235363841056824, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 976:\n",
      "  Training Loss: 0.5321265459060669, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235317349433899, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 977:\n",
      "  Training Loss: 0.5321366190910339, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236654281616211, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 978:\n",
      "  Training Loss: 0.532146155834198, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235801339149475, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 979:\n",
      "  Training Loss: 0.5321325063705444, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235673189163208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 980:\n",
      "  Training Loss: 0.5321287512779236, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235351324081421, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 981:\n",
      "  Training Loss: 0.5321296453475952, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235095024108887, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 982:\n",
      "  Training Loss: 0.5321375131607056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236501097679138, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 983:\n",
      "  Training Loss: 0.5321312546730042, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235419869422913, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 984:\n",
      "  Training Loss: 0.5321246981620789, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52359938621521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 985:\n",
      "  Training Loss: 0.5321313142776489, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235841274261475, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 986:\n",
      "  Training Loss: 0.5320612192153931, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234500169754028, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 987:\n",
      "  Training Loss: 0.5321708917617798, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236129760742188, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 988:\n",
      "  Training Loss: 0.532137393951416, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236091613769531, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 989:\n",
      "  Training Loss: 0.532137930393219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523582398891449, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 990:\n",
      "  Training Loss: 0.5321409106254578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236479640007019, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 991:\n",
      "  Training Loss: 0.5321239233016968, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235225558280945, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 992:\n",
      "  Training Loss: 0.532139778137207, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235521197319031, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 993:\n",
      "  Training Loss: 0.5321177840232849, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236704349517822, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 994:\n",
      "  Training Loss: 0.5321290493011475, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235889554023743, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 995:\n",
      "  Training Loss: 0.5321264266967773, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523517370223999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 996:\n",
      "  Training Loss: 0.532131016254425, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235086679458618, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 997:\n",
      "  Training Loss: 0.5321477651596069, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235463976860046, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 998:\n",
      "  Training Loss: 0.5321249961853027, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235530138015747, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 999:\n",
      "  Training Loss: 0.5321362018585205, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234764814376831, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 1000:\n",
      "  Training Loss: 0.5321475267410278, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235359072685242, Validation Accuracy: 0.7827280163764954\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(len(train_loss)):\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Training Loss: {train_loss[epoch]}, Training Accuracy: {train_accuracy[epoch]}\")\n",
    "    print(f\"  Validation Loss: {valid_loss[epoch]}, Validation Accuracy: {valid_accuracy[epoch]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADAGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(5, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592/592 [==============================] - 2s 2ms/step - loss: 60.8746 - accuracy: 0.6995 - val_loss: 6.9404 - val_accuracy: 0.7827\n",
      "Epoch 2/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 7.6568 - accuracy: 0.6965 - val_loss: 1.6676 - val_accuracy: 0.7827\n",
      "Epoch 3/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 6.0516 - accuracy: 0.6985 - val_loss: 7.5523 - val_accuracy: 0.7827\n",
      "Epoch 4/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 5.0816 - accuracy: 0.6972 - val_loss: 5.3166 - val_accuracy: 0.7827\n",
      "Epoch 5/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 4.2941 - accuracy: 0.7012 - val_loss: 7.3315 - val_accuracy: 0.7827\n",
      "Epoch 6/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 3.9940 - accuracy: 0.7065 - val_loss: 1.1122 - val_accuracy: 0.6674\n",
      "Epoch 7/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 3.7461 - accuracy: 0.7022 - val_loss: 1.0647 - val_accuracy: 0.6603\n",
      "Epoch 8/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 3.4319 - accuracy: 0.6989 - val_loss: 1.1886 - val_accuracy: 0.7827\n",
      "Epoch 9/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 3.1758 - accuracy: 0.7020 - val_loss: 1.9360 - val_accuracy: 0.4523\n",
      "Epoch 10/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 2.9971 - accuracy: 0.6989 - val_loss: 2.2611 - val_accuracy: 0.7827\n",
      "Epoch 11/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 2.9332 - accuracy: 0.6989 - val_loss: 3.6379 - val_accuracy: 0.7827\n",
      "Epoch 12/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 2.6546 - accuracy: 0.7026 - val_loss: 1.1436 - val_accuracy: 0.5207\n",
      "Epoch 13/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 2.5623 - accuracy: 0.7007 - val_loss: 0.6916 - val_accuracy: 0.7692\n",
      "Epoch 14/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 2.4263 - accuracy: 0.7029 - val_loss: 0.7926 - val_accuracy: 0.7827\n",
      "Epoch 15/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 2.4179 - accuracy: 0.6976 - val_loss: 1.4189 - val_accuracy: 0.7827\n",
      "Epoch 16/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 2.3316 - accuracy: 0.6982 - val_loss: 0.6302 - val_accuracy: 0.7827\n",
      "Epoch 17/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 2.1905 - accuracy: 0.6988 - val_loss: 0.6078 - val_accuracy: 0.7827\n",
      "Epoch 18/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 2.0263 - accuracy: 0.7024 - val_loss: 2.2669 - val_accuracy: 0.7827\n",
      "Epoch 19/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 2.1315 - accuracy: 0.6924 - val_loss: 0.6096 - val_accuracy: 0.7827\n",
      "Epoch 20/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.9916 - accuracy: 0.6963 - val_loss: 0.5721 - val_accuracy: 0.7802\n",
      "Epoch 21/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.9856 - accuracy: 0.6961 - val_loss: 1.6586 - val_accuracy: 0.7827\n",
      "Epoch 22/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.8461 - accuracy: 0.6962 - val_loss: 0.5676 - val_accuracy: 0.7751\n",
      "Epoch 23/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.8288 - accuracy: 0.6927 - val_loss: 1.3179 - val_accuracy: 0.7827\n",
      "Epoch 24/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.7580 - accuracy: 0.6929 - val_loss: 1.7257 - val_accuracy: 0.7827\n",
      "Epoch 25/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.7499 - accuracy: 0.6906 - val_loss: 0.9286 - val_accuracy: 0.7810\n",
      "Epoch 26/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.6459 - accuracy: 0.6905 - val_loss: 0.6530 - val_accuracy: 0.6223\n",
      "Epoch 27/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.6948 - accuracy: 0.6897 - val_loss: 3.1174 - val_accuracy: 0.7827\n",
      "Epoch 28/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.5912 - accuracy: 0.6939 - val_loss: 1.8478 - val_accuracy: 0.7825\n",
      "Epoch 29/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.5067 - accuracy: 0.6903 - val_loss: 0.6171 - val_accuracy: 0.7696\n",
      "Epoch 30/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.4548 - accuracy: 0.6806 - val_loss: 2.1928 - val_accuracy: 0.7823\n",
      "Epoch 31/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.5092 - accuracy: 0.6834 - val_loss: 0.9044 - val_accuracy: 0.7760\n",
      "Epoch 32/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.5077 - accuracy: 0.6856 - val_loss: 1.2715 - val_accuracy: 0.7806\n",
      "Epoch 33/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.4205 - accuracy: 0.6861 - val_loss: 0.5563 - val_accuracy: 0.7451\n",
      "Epoch 34/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.4065 - accuracy: 0.6831 - val_loss: 2.0728 - val_accuracy: 0.2242\n",
      "Epoch 35/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.3674 - accuracy: 0.6861 - val_loss: 1.2871 - val_accuracy: 0.7804\n",
      "Epoch 36/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.4046 - accuracy: 0.6802 - val_loss: 0.8597 - val_accuracy: 0.7745\n",
      "Epoch 37/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.4755 - accuracy: 0.6776 - val_loss: 0.5848 - val_accuracy: 0.6968\n",
      "Epoch 38/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.3834 - accuracy: 0.6859 - val_loss: 1.4577 - val_accuracy: 0.7817\n",
      "Epoch 39/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.3271 - accuracy: 0.6801 - val_loss: 0.7637 - val_accuracy: 0.7722\n",
      "Epoch 40/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.2912 - accuracy: 0.6800 - val_loss: 0.5869 - val_accuracy: 0.7527\n",
      "Epoch 41/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.3511 - accuracy: 0.6789 - val_loss: 2.6056 - val_accuracy: 0.2175\n",
      "Epoch 42/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.2258 - accuracy: 0.6840 - val_loss: 1.3876 - val_accuracy: 0.2291\n",
      "Epoch 43/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.2905 - accuracy: 0.6804 - val_loss: 0.6393 - val_accuracy: 0.7639\n",
      "Epoch 44/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.3188 - accuracy: 0.6751 - val_loss: 1.3414 - val_accuracy: 0.2299\n",
      "Epoch 45/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.2403 - accuracy: 0.6817 - val_loss: 0.5686 - val_accuracy: 0.7394\n",
      "Epoch 46/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.2612 - accuracy: 0.6819 - val_loss: 0.7014 - val_accuracy: 0.7715\n",
      "Epoch 47/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.2004 - accuracy: 0.6822 - val_loss: 0.5875 - val_accuracy: 0.6856\n",
      "Epoch 48/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.2193 - accuracy: 0.6816 - val_loss: 0.5684 - val_accuracy: 0.7156\n",
      "Epoch 49/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1452 - accuracy: 0.6813 - val_loss: 0.8142 - val_accuracy: 0.7722\n",
      "Epoch 50/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1529 - accuracy: 0.6810 - val_loss: 1.2305 - val_accuracy: 0.7781\n",
      "Epoch 51/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1599 - accuracy: 0.6813 - val_loss: 0.6361 - val_accuracy: 0.6246\n",
      "Epoch 52/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1638 - accuracy: 0.6765 - val_loss: 1.5004 - val_accuracy: 0.7815\n",
      "Epoch 53/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1344 - accuracy: 0.6768 - val_loss: 2.2779 - val_accuracy: 0.7819\n",
      "Epoch 54/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1342 - accuracy: 0.6811 - val_loss: 1.2806 - val_accuracy: 0.7789\n",
      "Epoch 55/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0803 - accuracy: 0.6842 - val_loss: 0.5616 - val_accuracy: 0.7342\n",
      "Epoch 56/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0799 - accuracy: 0.6791 - val_loss: 0.7152 - val_accuracy: 0.7715\n",
      "Epoch 57/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1477 - accuracy: 0.6834 - val_loss: 0.7635 - val_accuracy: 0.7715\n",
      "Epoch 58/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1270 - accuracy: 0.6849 - val_loss: 0.6438 - val_accuracy: 0.7658\n",
      "Epoch 59/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1455 - accuracy: 0.6777 - val_loss: 1.4463 - val_accuracy: 0.2240\n",
      "Epoch 60/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1580 - accuracy: 0.6802 - val_loss: 1.4572 - val_accuracy: 0.7815\n",
      "Epoch 61/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1378 - accuracy: 0.6822 - val_loss: 0.6119 - val_accuracy: 0.7595\n",
      "Epoch 62/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0740 - accuracy: 0.6825 - val_loss: 0.8296 - val_accuracy: 0.7739\n",
      "Epoch 63/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.1032 - accuracy: 0.6802 - val_loss: 0.6296 - val_accuracy: 0.7644\n",
      "Epoch 64/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0495 - accuracy: 0.6783 - val_loss: 0.9136 - val_accuracy: 0.7764\n",
      "Epoch 65/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0357 - accuracy: 0.6829 - val_loss: 0.6640 - val_accuracy: 0.7705\n",
      "Epoch 66/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0578 - accuracy: 0.6858 - val_loss: 0.9195 - val_accuracy: 0.2660\n",
      "Epoch 67/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0433 - accuracy: 0.6807 - val_loss: 0.5604 - val_accuracy: 0.7251\n",
      "Epoch 68/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0482 - accuracy: 0.6876 - val_loss: 0.7276 - val_accuracy: 0.7715\n",
      "Epoch 69/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9990 - accuracy: 0.6878 - val_loss: 0.8996 - val_accuracy: 0.7766\n",
      "Epoch 70/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9310 - accuracy: 0.6930 - val_loss: 0.7701 - val_accuracy: 0.7726\n",
      "Epoch 71/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0500 - accuracy: 0.6843 - val_loss: 1.6253 - val_accuracy: 0.7812\n",
      "Epoch 72/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0439 - accuracy: 0.6886 - val_loss: 0.6081 - val_accuracy: 0.7646\n",
      "Epoch 73/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9736 - accuracy: 0.6882 - val_loss: 0.7926 - val_accuracy: 0.3649\n",
      "Epoch 74/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0031 - accuracy: 0.6898 - val_loss: 1.4120 - val_accuracy: 0.2261\n",
      "Epoch 75/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0242 - accuracy: 0.6864 - val_loss: 0.5577 - val_accuracy: 0.7462\n",
      "Epoch 76/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9748 - accuracy: 0.6917 - val_loss: 0.6097 - val_accuracy: 0.7663\n",
      "Epoch 77/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0012 - accuracy: 0.6878 - val_loss: 0.6833 - val_accuracy: 0.5492\n",
      "Epoch 78/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0447 - accuracy: 0.6801 - val_loss: 0.5905 - val_accuracy: 0.7612\n",
      "Epoch 79/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0022 - accuracy: 0.6883 - val_loss: 0.5719 - val_accuracy: 0.7578\n",
      "Epoch 80/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9928 - accuracy: 0.6908 - val_loss: 0.5608 - val_accuracy: 0.7261\n",
      "Epoch 81/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9704 - accuracy: 0.6918 - val_loss: 0.5685 - val_accuracy: 0.7572\n",
      "Epoch 82/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9915 - accuracy: 0.6883 - val_loss: 0.7468 - val_accuracy: 0.7730\n",
      "Epoch 83/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9394 - accuracy: 0.6919 - val_loss: 1.0392 - val_accuracy: 0.7781\n",
      "Epoch 84/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9423 - accuracy: 0.6904 - val_loss: 0.6182 - val_accuracy: 0.7696\n",
      "Epoch 85/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 1.0214 - accuracy: 0.6871 - val_loss: 1.4753 - val_accuracy: 0.7812\n",
      "Epoch 86/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9354 - accuracy: 0.6878 - val_loss: 0.5685 - val_accuracy: 0.7595\n",
      "Epoch 87/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9694 - accuracy: 0.6890 - val_loss: 0.6244 - val_accuracy: 0.7722\n",
      "Epoch 88/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9680 - accuracy: 0.6920 - val_loss: 0.7920 - val_accuracy: 0.3566\n",
      "Epoch 89/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9105 - accuracy: 0.6921 - val_loss: 0.5561 - val_accuracy: 0.7557\n",
      "Epoch 90/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9229 - accuracy: 0.6933 - val_loss: 0.6523 - val_accuracy: 0.7717\n",
      "Epoch 91/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9225 - accuracy: 0.6973 - val_loss: 0.5531 - val_accuracy: 0.7549\n",
      "Epoch 92/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9087 - accuracy: 0.6895 - val_loss: 0.7152 - val_accuracy: 0.7734\n",
      "Epoch 93/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9729 - accuracy: 0.6914 - val_loss: 0.5949 - val_accuracy: 0.6985\n",
      "Epoch 94/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9325 - accuracy: 0.6931 - val_loss: 0.6073 - val_accuracy: 0.6805\n",
      "Epoch 95/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9412 - accuracy: 0.6904 - val_loss: 0.5570 - val_accuracy: 0.7308\n",
      "Epoch 96/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9430 - accuracy: 0.6929 - val_loss: 0.8497 - val_accuracy: 0.7766\n",
      "Epoch 97/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8925 - accuracy: 0.6988 - val_loss: 0.9591 - val_accuracy: 0.7783\n",
      "Epoch 98/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8689 - accuracy: 0.7008 - val_loss: 0.7677 - val_accuracy: 0.7762\n",
      "Epoch 99/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8924 - accuracy: 0.6990 - val_loss: 1.0573 - val_accuracy: 0.7804\n",
      "Epoch 100/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9847 - accuracy: 0.6955 - val_loss: 0.5988 - val_accuracy: 0.7701\n",
      "Epoch 101/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9003 - accuracy: 0.6928 - val_loss: 1.4462 - val_accuracy: 0.2276\n",
      "Epoch 102/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8729 - accuracy: 0.6981 - val_loss: 0.6713 - val_accuracy: 0.7732\n",
      "Epoch 103/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.9427 - accuracy: 0.6949 - val_loss: 0.6442 - val_accuracy: 0.7715\n",
      "Epoch 104/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8732 - accuracy: 0.6973 - val_loss: 0.7335 - val_accuracy: 0.7749\n",
      "Epoch 105/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8825 - accuracy: 0.6984 - val_loss: 0.5919 - val_accuracy: 0.7703\n",
      "Epoch 106/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8519 - accuracy: 0.6975 - val_loss: 0.7159 - val_accuracy: 0.4901\n",
      "Epoch 107/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8800 - accuracy: 0.6934 - val_loss: 0.7465 - val_accuracy: 0.7762\n",
      "Epoch 108/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8981 - accuracy: 0.6963 - val_loss: 0.5487 - val_accuracy: 0.7597\n",
      "Epoch 109/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8870 - accuracy: 0.6995 - val_loss: 0.6367 - val_accuracy: 0.7726\n",
      "Epoch 110/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8685 - accuracy: 0.6996 - val_loss: 0.6580 - val_accuracy: 0.6043\n",
      "Epoch 111/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8574 - accuracy: 0.6967 - val_loss: 0.8626 - val_accuracy: 0.7785\n",
      "Epoch 112/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8373 - accuracy: 0.6986 - val_loss: 0.5806 - val_accuracy: 0.7709\n",
      "Epoch 113/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8341 - accuracy: 0.7020 - val_loss: 0.5577 - val_accuracy: 0.7656\n",
      "Epoch 114/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8685 - accuracy: 0.6989 - val_loss: 0.9750 - val_accuracy: 0.7804\n",
      "Epoch 115/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8251 - accuracy: 0.7037 - val_loss: 0.6501 - val_accuracy: 0.7730\n",
      "Epoch 116/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8541 - accuracy: 0.7015 - val_loss: 0.5517 - val_accuracy: 0.7637\n",
      "Epoch 117/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8657 - accuracy: 0.7003 - val_loss: 0.5467 - val_accuracy: 0.7620\n",
      "Epoch 118/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8667 - accuracy: 0.7028 - val_loss: 0.6743 - val_accuracy: 0.7739\n",
      "Epoch 119/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8645 - accuracy: 0.7035 - val_loss: 0.5428 - val_accuracy: 0.7502\n",
      "Epoch 120/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8301 - accuracy: 0.6997 - val_loss: 0.8377 - val_accuracy: 0.7781\n",
      "Epoch 121/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8566 - accuracy: 0.6976 - val_loss: 0.9357 - val_accuracy: 0.7804\n",
      "Epoch 122/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8366 - accuracy: 0.7003 - val_loss: 0.5703 - val_accuracy: 0.7316\n",
      "Epoch 123/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8068 - accuracy: 0.7060 - val_loss: 0.5538 - val_accuracy: 0.7422\n",
      "Epoch 124/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8230 - accuracy: 0.7031 - val_loss: 0.7905 - val_accuracy: 0.3535\n",
      "Epoch 125/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8165 - accuracy: 0.7075 - val_loss: 0.5464 - val_accuracy: 0.7473\n",
      "Epoch 126/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8326 - accuracy: 0.7039 - val_loss: 0.6263 - val_accuracy: 0.7736\n",
      "Epoch 127/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8299 - accuracy: 0.7022 - val_loss: 0.6154 - val_accuracy: 0.6883\n",
      "Epoch 128/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8564 - accuracy: 0.7010 - val_loss: 0.7111 - val_accuracy: 0.4943\n",
      "Epoch 129/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8080 - accuracy: 0.7068 - val_loss: 0.5415 - val_accuracy: 0.7625\n",
      "Epoch 130/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8236 - accuracy: 0.7053 - val_loss: 0.5529 - val_accuracy: 0.7684\n",
      "Epoch 131/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8239 - accuracy: 0.7077 - val_loss: 0.6477 - val_accuracy: 0.7743\n",
      "Epoch 132/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8157 - accuracy: 0.7025 - val_loss: 0.8636 - val_accuracy: 0.7804\n",
      "Epoch 133/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8311 - accuracy: 0.7059 - val_loss: 0.7160 - val_accuracy: 0.7768\n",
      "Epoch 134/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8032 - accuracy: 0.7096 - val_loss: 0.5546 - val_accuracy: 0.7441\n",
      "Epoch 135/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8349 - accuracy: 0.7084 - val_loss: 0.5747 - val_accuracy: 0.7715\n",
      "Epoch 136/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8153 - accuracy: 0.7069 - val_loss: 0.5421 - val_accuracy: 0.7532\n",
      "Epoch 137/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7626 - accuracy: 0.7102 - val_loss: 0.6736 - val_accuracy: 0.5695\n",
      "Epoch 138/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8075 - accuracy: 0.7061 - val_loss: 0.6144 - val_accuracy: 0.7745\n",
      "Epoch 139/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8301 - accuracy: 0.7080 - val_loss: 1.0699 - val_accuracy: 0.7815\n",
      "Epoch 140/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8252 - accuracy: 0.7004 - val_loss: 0.7080 - val_accuracy: 0.7774\n",
      "Epoch 141/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8389 - accuracy: 0.7056 - val_loss: 0.9565 - val_accuracy: 0.7815\n",
      "Epoch 142/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7728 - accuracy: 0.7131 - val_loss: 0.9629 - val_accuracy: 0.7815\n",
      "Epoch 143/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8083 - accuracy: 0.7095 - val_loss: 0.6020 - val_accuracy: 0.7130\n",
      "Epoch 144/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8218 - accuracy: 0.7082 - val_loss: 0.5379 - val_accuracy: 0.7589\n",
      "Epoch 145/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7828 - accuracy: 0.7079 - val_loss: 1.1711 - val_accuracy: 0.7821\n",
      "Epoch 146/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8236 - accuracy: 0.7055 - val_loss: 0.6021 - val_accuracy: 0.7743\n",
      "Epoch 147/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8024 - accuracy: 0.7144 - val_loss: 0.5765 - val_accuracy: 0.7321\n",
      "Epoch 148/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7863 - accuracy: 0.7093 - val_loss: 0.5471 - val_accuracy: 0.7511\n",
      "Epoch 149/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8146 - accuracy: 0.7084 - val_loss: 0.6047 - val_accuracy: 0.7120\n",
      "Epoch 150/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7949 - accuracy: 0.7087 - val_loss: 1.0510 - val_accuracy: 0.7819\n",
      "Epoch 151/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8200 - accuracy: 0.7072 - val_loss: 0.5457 - val_accuracy: 0.7701\n",
      "Epoch 152/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8135 - accuracy: 0.7080 - val_loss: 0.5813 - val_accuracy: 0.7312\n",
      "Epoch 153/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8210 - accuracy: 0.7108 - val_loss: 0.5379 - val_accuracy: 0.7667\n",
      "Epoch 154/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7890 - accuracy: 0.7079 - val_loss: 0.8389 - val_accuracy: 0.7812\n",
      "Epoch 155/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7794 - accuracy: 0.7133 - val_loss: 0.6092 - val_accuracy: 0.7743\n",
      "Epoch 156/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8067 - accuracy: 0.7131 - val_loss: 0.5461 - val_accuracy: 0.7720\n",
      "Epoch 157/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8057 - accuracy: 0.7095 - val_loss: 0.5696 - val_accuracy: 0.7728\n",
      "Epoch 158/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7902 - accuracy: 0.7118 - val_loss: 0.5418 - val_accuracy: 0.7705\n",
      "Epoch 159/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7670 - accuracy: 0.7094 - val_loss: 0.5920 - val_accuracy: 0.7741\n",
      "Epoch 160/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7774 - accuracy: 0.7129 - val_loss: 0.6265 - val_accuracy: 0.7751\n",
      "Epoch 161/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7832 - accuracy: 0.7072 - val_loss: 0.8307 - val_accuracy: 0.7815\n",
      "Epoch 162/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7861 - accuracy: 0.7090 - val_loss: 0.8522 - val_accuracy: 0.7815\n",
      "Epoch 163/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7495 - accuracy: 0.7119 - val_loss: 0.7259 - val_accuracy: 0.7793\n",
      "Epoch 164/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8072 - accuracy: 0.7054 - val_loss: 0.5607 - val_accuracy: 0.7451\n",
      "Epoch 165/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7945 - accuracy: 0.7139 - val_loss: 0.6586 - val_accuracy: 0.7777\n",
      "Epoch 166/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8135 - accuracy: 0.7079 - val_loss: 0.5474 - val_accuracy: 0.7523\n",
      "Epoch 167/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7831 - accuracy: 0.7065 - val_loss: 1.0453 - val_accuracy: 0.2665\n",
      "Epoch 168/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7590 - accuracy: 0.7141 - val_loss: 1.5756 - val_accuracy: 0.7827\n",
      "Epoch 169/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7851 - accuracy: 0.7141 - val_loss: 0.9301 - val_accuracy: 0.7819\n",
      "Epoch 170/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7827 - accuracy: 0.7103 - val_loss: 0.5349 - val_accuracy: 0.7696\n",
      "Epoch 171/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7497 - accuracy: 0.7182 - val_loss: 0.5420 - val_accuracy: 0.7720\n",
      "Epoch 172/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7542 - accuracy: 0.7135 - val_loss: 0.5503 - val_accuracy: 0.7728\n",
      "Epoch 173/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7558 - accuracy: 0.7155 - val_loss: 0.5493 - val_accuracy: 0.7728\n",
      "Epoch 174/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7709 - accuracy: 0.7124 - val_loss: 0.7277 - val_accuracy: 0.7804\n",
      "Epoch 175/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7878 - accuracy: 0.7137 - val_loss: 0.6804 - val_accuracy: 0.7791\n",
      "Epoch 176/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7643 - accuracy: 0.7164 - val_loss: 0.5326 - val_accuracy: 0.7673\n",
      "Epoch 177/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7631 - accuracy: 0.7152 - val_loss: 0.6339 - val_accuracy: 0.6803\n",
      "Epoch 178/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7729 - accuracy: 0.7107 - val_loss: 0.6373 - val_accuracy: 0.6750\n",
      "Epoch 179/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7558 - accuracy: 0.7187 - val_loss: 1.1478 - val_accuracy: 0.2561\n",
      "Epoch 180/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.8046 - accuracy: 0.7097 - val_loss: 0.5439 - val_accuracy: 0.7568\n",
      "Epoch 181/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7556 - accuracy: 0.7118 - val_loss: 0.7694 - val_accuracy: 0.7815\n",
      "Epoch 182/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7618 - accuracy: 0.7121 - val_loss: 1.0458 - val_accuracy: 0.7827\n",
      "Epoch 183/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7620 - accuracy: 0.7155 - val_loss: 0.5396 - val_accuracy: 0.7722\n",
      "Epoch 184/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7450 - accuracy: 0.7198 - val_loss: 0.5459 - val_accuracy: 0.7734\n",
      "Epoch 185/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7806 - accuracy: 0.7092 - val_loss: 0.6690 - val_accuracy: 0.7791\n",
      "Epoch 186/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7735 - accuracy: 0.7204 - val_loss: 0.9726 - val_accuracy: 0.7825\n",
      "Epoch 187/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7561 - accuracy: 0.7156 - val_loss: 0.5621 - val_accuracy: 0.7479\n",
      "Epoch 188/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7557 - accuracy: 0.7110 - val_loss: 0.5608 - val_accuracy: 0.7745\n",
      "Epoch 189/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7505 - accuracy: 0.7168 - val_loss: 0.5327 - val_accuracy: 0.7652\n",
      "Epoch 190/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7447 - accuracy: 0.7145 - val_loss: 0.6480 - val_accuracy: 0.7789\n",
      "Epoch 191/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7672 - accuracy: 0.7155 - val_loss: 0.5313 - val_accuracy: 0.7698\n",
      "Epoch 192/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7212 - accuracy: 0.7212 - val_loss: 0.6769 - val_accuracy: 0.5657\n",
      "Epoch 193/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7073 - accuracy: 0.7262 - val_loss: 0.5736 - val_accuracy: 0.7755\n",
      "Epoch 194/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7450 - accuracy: 0.7198 - val_loss: 0.5695 - val_accuracy: 0.7758\n",
      "Epoch 195/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7119 - accuracy: 0.7219 - val_loss: 1.6442 - val_accuracy: 0.2342\n",
      "Epoch 196/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7701 - accuracy: 0.7151 - val_loss: 0.5307 - val_accuracy: 0.7705\n",
      "Epoch 197/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7535 - accuracy: 0.7158 - val_loss: 0.5496 - val_accuracy: 0.7743\n",
      "Epoch 198/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7171 - accuracy: 0.7236 - val_loss: 0.5337 - val_accuracy: 0.7722\n",
      "Epoch 199/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7349 - accuracy: 0.7212 - val_loss: 0.5434 - val_accuracy: 0.7741\n",
      "Epoch 200/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7320 - accuracy: 0.7234 - val_loss: 0.6634 - val_accuracy: 0.5944\n",
      "Epoch 201/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6896 - accuracy: 0.7285 - val_loss: 0.5836 - val_accuracy: 0.7774\n",
      "Epoch 202/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7292 - accuracy: 0.7225 - val_loss: 0.6559 - val_accuracy: 0.6138\n",
      "Epoch 203/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7166 - accuracy: 0.7275 - val_loss: 0.5395 - val_accuracy: 0.7635\n",
      "Epoch 204/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7489 - accuracy: 0.7183 - val_loss: 0.5745 - val_accuracy: 0.7777\n",
      "Epoch 205/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7459 - accuracy: 0.7192 - val_loss: 0.5308 - val_accuracy: 0.7698\n",
      "Epoch 206/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7449 - accuracy: 0.7223 - val_loss: 0.7160 - val_accuracy: 0.7819\n",
      "Epoch 207/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7687 - accuracy: 0.7204 - val_loss: 0.5595 - val_accuracy: 0.7546\n",
      "Epoch 208/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7131 - accuracy: 0.7252 - val_loss: 0.9586 - val_accuracy: 0.7825\n",
      "Epoch 209/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7119 - accuracy: 0.7268 - val_loss: 0.5326 - val_accuracy: 0.7736\n",
      "Epoch 210/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7289 - accuracy: 0.7214 - val_loss: 0.6057 - val_accuracy: 0.7268\n",
      "Epoch 211/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7144 - accuracy: 0.7261 - val_loss: 0.6718 - val_accuracy: 0.7817\n",
      "Epoch 212/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7367 - accuracy: 0.7198 - val_loss: 0.7601 - val_accuracy: 0.3938\n",
      "Epoch 213/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7363 - accuracy: 0.7227 - val_loss: 0.7392 - val_accuracy: 0.7823\n",
      "Epoch 214/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7429 - accuracy: 0.7208 - val_loss: 0.5322 - val_accuracy: 0.7690\n",
      "Epoch 215/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6855 - accuracy: 0.7274 - val_loss: 0.5651 - val_accuracy: 0.7563\n",
      "Epoch 216/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7129 - accuracy: 0.7241 - val_loss: 0.6179 - val_accuracy: 0.7802\n",
      "Epoch 217/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7508 - accuracy: 0.7194 - val_loss: 0.6118 - val_accuracy: 0.7232\n",
      "Epoch 218/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7405 - accuracy: 0.7192 - val_loss: 0.9397 - val_accuracy: 0.7829\n",
      "Epoch 219/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7272 - accuracy: 0.7238 - val_loss: 0.5300 - val_accuracy: 0.7705\n",
      "Epoch 220/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7087 - accuracy: 0.7287 - val_loss: 0.6312 - val_accuracy: 0.7808\n",
      "Epoch 221/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7356 - accuracy: 0.7248 - val_loss: 0.5314 - val_accuracy: 0.7751\n",
      "Epoch 222/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6865 - accuracy: 0.7297 - val_loss: 0.6249 - val_accuracy: 0.7806\n",
      "Epoch 223/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7316 - accuracy: 0.7177 - val_loss: 0.5651 - val_accuracy: 0.7789\n",
      "Epoch 224/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7157 - accuracy: 0.7236 - val_loss: 0.8847 - val_accuracy: 0.7825\n",
      "Epoch 225/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7174 - accuracy: 0.7229 - val_loss: 0.5882 - val_accuracy: 0.7445\n",
      "Epoch 226/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7031 - accuracy: 0.7267 - val_loss: 0.9773 - val_accuracy: 0.7827\n",
      "Epoch 227/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7298 - accuracy: 0.7223 - val_loss: 1.0774 - val_accuracy: 0.7827\n",
      "Epoch 228/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7069 - accuracy: 0.7237 - val_loss: 0.5682 - val_accuracy: 0.7789\n",
      "Epoch 229/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6973 - accuracy: 0.7279 - val_loss: 0.5706 - val_accuracy: 0.7789\n",
      "Epoch 230/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7454 - accuracy: 0.7231 - val_loss: 0.5298 - val_accuracy: 0.7758\n",
      "Epoch 231/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6933 - accuracy: 0.7330 - val_loss: 0.6837 - val_accuracy: 0.7825\n",
      "Epoch 232/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6738 - accuracy: 0.7341 - val_loss: 0.5337 - val_accuracy: 0.7762\n",
      "Epoch 233/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7180 - accuracy: 0.7249 - val_loss: 0.5281 - val_accuracy: 0.7734\n",
      "Epoch 234/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6970 - accuracy: 0.7258 - val_loss: 0.5309 - val_accuracy: 0.7762\n",
      "Epoch 235/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7250 - accuracy: 0.7227 - val_loss: 0.5652 - val_accuracy: 0.7589\n",
      "Epoch 236/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6902 - accuracy: 0.7322 - val_loss: 0.5421 - val_accuracy: 0.7787\n",
      "Epoch 237/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7272 - accuracy: 0.7255 - val_loss: 0.5636 - val_accuracy: 0.7595\n",
      "Epoch 238/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7159 - accuracy: 0.7241 - val_loss: 0.9500 - val_accuracy: 0.7827\n",
      "Epoch 239/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6970 - accuracy: 0.7316 - val_loss: 0.5581 - val_accuracy: 0.7791\n",
      "Epoch 240/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7192 - accuracy: 0.7261 - val_loss: 0.6025 - val_accuracy: 0.7815\n",
      "Epoch 241/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7002 - accuracy: 0.7294 - val_loss: 0.5536 - val_accuracy: 0.7793\n",
      "Epoch 242/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7025 - accuracy: 0.7269 - val_loss: 0.6069 - val_accuracy: 0.7815\n",
      "Epoch 243/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7176 - accuracy: 0.7240 - val_loss: 0.7694 - val_accuracy: 0.7827\n",
      "Epoch 244/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7272 - accuracy: 0.7250 - val_loss: 0.6355 - val_accuracy: 0.7823\n",
      "Epoch 245/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7007 - accuracy: 0.7287 - val_loss: 0.5450 - val_accuracy: 0.7648\n",
      "Epoch 246/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7025 - accuracy: 0.7284 - val_loss: 0.5341 - val_accuracy: 0.7777\n",
      "Epoch 247/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6928 - accuracy: 0.7297 - val_loss: 0.5732 - val_accuracy: 0.7802\n",
      "Epoch 248/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7210 - accuracy: 0.7296 - val_loss: 0.6115 - val_accuracy: 0.7314\n",
      "Epoch 249/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7076 - accuracy: 0.7288 - val_loss: 0.5952 - val_accuracy: 0.7810\n",
      "Epoch 250/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7061 - accuracy: 0.7258 - val_loss: 0.7866 - val_accuracy: 0.7829\n",
      "Epoch 251/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7087 - accuracy: 0.7283 - val_loss: 0.7902 - val_accuracy: 0.7829\n",
      "Epoch 252/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7045 - accuracy: 0.7299 - val_loss: 0.6219 - val_accuracy: 0.7823\n",
      "Epoch 253/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7123 - accuracy: 0.7260 - val_loss: 0.5607 - val_accuracy: 0.7804\n",
      "Epoch 254/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6913 - accuracy: 0.7269 - val_loss: 0.5385 - val_accuracy: 0.7791\n",
      "Epoch 255/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7156 - accuracy: 0.7242 - val_loss: 0.5504 - val_accuracy: 0.7798\n",
      "Epoch 256/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7000 - accuracy: 0.7267 - val_loss: 0.5335 - val_accuracy: 0.7791\n",
      "Epoch 257/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7097 - accuracy: 0.7286 - val_loss: 0.6255 - val_accuracy: 0.7825\n",
      "Epoch 258/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6760 - accuracy: 0.7321 - val_loss: 0.6400 - val_accuracy: 0.6769\n",
      "Epoch 259/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6907 - accuracy: 0.7278 - val_loss: 0.5599 - val_accuracy: 0.7804\n",
      "Epoch 260/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6947 - accuracy: 0.7245 - val_loss: 0.8646 - val_accuracy: 0.7829\n",
      "Epoch 261/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7108 - accuracy: 0.7257 - val_loss: 0.5270 - val_accuracy: 0.7741\n",
      "Epoch 262/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7018 - accuracy: 0.7292 - val_loss: 0.5485 - val_accuracy: 0.7665\n",
      "Epoch 263/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6662 - accuracy: 0.7340 - val_loss: 0.5824 - val_accuracy: 0.7817\n",
      "Epoch 264/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6729 - accuracy: 0.7328 - val_loss: 0.5423 - val_accuracy: 0.7701\n",
      "Epoch 265/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6784 - accuracy: 0.7338 - val_loss: 0.6460 - val_accuracy: 0.7825\n",
      "Epoch 266/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6858 - accuracy: 0.7298 - val_loss: 0.5265 - val_accuracy: 0.7785\n",
      "Epoch 267/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7074 - accuracy: 0.7287 - val_loss: 0.5690 - val_accuracy: 0.7593\n",
      "Epoch 268/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6952 - accuracy: 0.7331 - val_loss: 0.5845 - val_accuracy: 0.7570\n",
      "Epoch 269/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6855 - accuracy: 0.7317 - val_loss: 1.3931 - val_accuracy: 0.2540\n",
      "Epoch 270/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6856 - accuracy: 0.7338 - val_loss: 0.5528 - val_accuracy: 0.7673\n",
      "Epoch 271/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7156 - accuracy: 0.7264 - val_loss: 0.5342 - val_accuracy: 0.7802\n",
      "Epoch 272/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6978 - accuracy: 0.7292 - val_loss: 0.6265 - val_accuracy: 0.7823\n",
      "Epoch 273/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6956 - accuracy: 0.7306 - val_loss: 0.5702 - val_accuracy: 0.7815\n",
      "Epoch 274/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6694 - accuracy: 0.7313 - val_loss: 0.7874 - val_accuracy: 0.7829\n",
      "Epoch 275/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6724 - accuracy: 0.7300 - val_loss: 0.5450 - val_accuracy: 0.7812\n",
      "Epoch 276/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6993 - accuracy: 0.7300 - val_loss: 0.6165 - val_accuracy: 0.7823\n",
      "Epoch 277/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.7335 - val_loss: 0.5824 - val_accuracy: 0.7582\n",
      "Epoch 278/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6925 - accuracy: 0.7321 - val_loss: 0.5347 - val_accuracy: 0.7732\n",
      "Epoch 279/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6712 - accuracy: 0.7300 - val_loss: 0.6188 - val_accuracy: 0.7333\n",
      "Epoch 280/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6840 - accuracy: 0.7345 - val_loss: 0.5901 - val_accuracy: 0.7823\n",
      "Epoch 281/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6816 - accuracy: 0.7313 - val_loss: 0.6174 - val_accuracy: 0.7825\n",
      "Epoch 282/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6742 - accuracy: 0.7323 - val_loss: 0.7822 - val_accuracy: 0.3731\n",
      "Epoch 283/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6691 - accuracy: 0.7357 - val_loss: 0.5569 - val_accuracy: 0.7821\n",
      "Epoch 284/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7076 - accuracy: 0.7313 - val_loss: 0.5415 - val_accuracy: 0.7728\n",
      "Epoch 285/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6803 - accuracy: 0.7339 - val_loss: 0.7558 - val_accuracy: 0.4035\n",
      "Epoch 286/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6814 - accuracy: 0.7362 - val_loss: 0.6756 - val_accuracy: 0.7829\n",
      "Epoch 287/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6945 - accuracy: 0.7324 - val_loss: 0.8164 - val_accuracy: 0.7829\n",
      "Epoch 288/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6634 - accuracy: 0.7365 - val_loss: 0.5451 - val_accuracy: 0.7821\n",
      "Epoch 289/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6817 - accuracy: 0.7359 - val_loss: 0.6861 - val_accuracy: 0.7829\n",
      "Epoch 290/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6560 - accuracy: 0.7350 - val_loss: 0.5574 - val_accuracy: 0.7675\n",
      "Epoch 291/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6920 - accuracy: 0.7357 - val_loss: 0.6601 - val_accuracy: 0.7829\n",
      "Epoch 292/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7334 - val_loss: 1.1070 - val_accuracy: 0.2770\n",
      "Epoch 293/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6746 - accuracy: 0.7303 - val_loss: 0.6197 - val_accuracy: 0.7386\n",
      "Epoch 294/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6817 - accuracy: 0.7300 - val_loss: 0.5956 - val_accuracy: 0.7821\n",
      "Epoch 295/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6773 - accuracy: 0.7368 - val_loss: 0.5241 - val_accuracy: 0.7808\n",
      "Epoch 296/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6578 - accuracy: 0.7352 - val_loss: 0.5349 - val_accuracy: 0.7770\n",
      "Epoch 297/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.7343 - val_loss: 0.5784 - val_accuracy: 0.7823\n",
      "Epoch 298/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6775 - accuracy: 0.7335 - val_loss: 0.5966 - val_accuracy: 0.7570\n",
      "Epoch 299/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6858 - accuracy: 0.7315 - val_loss: 0.5257 - val_accuracy: 0.7793\n",
      "Epoch 300/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6602 - accuracy: 0.7378 - val_loss: 0.5234 - val_accuracy: 0.7808\n",
      "Epoch 301/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6888 - accuracy: 0.7310 - val_loss: 0.6567 - val_accuracy: 0.7829\n",
      "Epoch 302/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.7400 - val_loss: 0.9466 - val_accuracy: 0.7827\n",
      "Epoch 303/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6822 - accuracy: 0.7335 - val_loss: 0.5236 - val_accuracy: 0.7812\n",
      "Epoch 304/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.6611 - accuracy: 0.7371 - val_loss: 0.5832 - val_accuracy: 0.7825\n",
      "Epoch 305/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6734 - accuracy: 0.7397 - val_loss: 0.6037 - val_accuracy: 0.7827\n",
      "Epoch 306/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6819 - accuracy: 0.7314 - val_loss: 0.5249 - val_accuracy: 0.7802\n",
      "Epoch 307/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6629 - accuracy: 0.7389 - val_loss: 0.5948 - val_accuracy: 0.7589\n",
      "Epoch 308/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6987 - accuracy: 0.7267 - val_loss: 0.5437 - val_accuracy: 0.7768\n",
      "Epoch 309/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6580 - accuracy: 0.7378 - val_loss: 0.5637 - val_accuracy: 0.7819\n",
      "Epoch 310/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6931 - accuracy: 0.7328 - val_loss: 0.6850 - val_accuracy: 0.5281\n",
      "Epoch 311/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6631 - accuracy: 0.7401 - val_loss: 0.7947 - val_accuracy: 0.7829\n",
      "Epoch 312/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6699 - accuracy: 0.7365 - val_loss: 0.5409 - val_accuracy: 0.7823\n",
      "Epoch 313/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6385 - accuracy: 0.7394 - val_loss: 0.5244 - val_accuracy: 0.7808\n",
      "Epoch 314/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6603 - accuracy: 0.7395 - val_loss: 0.6920 - val_accuracy: 0.7829\n",
      "Epoch 315/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6848 - accuracy: 0.7338 - val_loss: 0.5524 - val_accuracy: 0.7821\n",
      "Epoch 316/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.7318 - val_loss: 0.6261 - val_accuracy: 0.7829\n",
      "Epoch 317/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6604 - accuracy: 0.7435 - val_loss: 0.5578 - val_accuracy: 0.7749\n",
      "Epoch 318/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6696 - accuracy: 0.7382 - val_loss: 0.5669 - val_accuracy: 0.7819\n",
      "Epoch 319/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6458 - accuracy: 0.7421 - val_loss: 0.5231 - val_accuracy: 0.7819\n",
      "Epoch 320/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6726 - accuracy: 0.7361 - val_loss: 0.5293 - val_accuracy: 0.7823\n",
      "Epoch 321/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6608 - accuracy: 0.7407 - val_loss: 0.5233 - val_accuracy: 0.7819\n",
      "Epoch 322/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6580 - accuracy: 0.7401 - val_loss: 0.5722 - val_accuracy: 0.7825\n",
      "Epoch 323/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.7369 - val_loss: 1.1653 - val_accuracy: 0.2734\n",
      "Epoch 324/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6698 - accuracy: 0.7327 - val_loss: 0.5718 - val_accuracy: 0.7728\n",
      "Epoch 325/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6574 - accuracy: 0.7349 - val_loss: 0.5321 - val_accuracy: 0.7800\n",
      "Epoch 326/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6638 - accuracy: 0.7382 - val_loss: 0.6887 - val_accuracy: 0.7829\n",
      "Epoch 327/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6556 - accuracy: 0.7365 - val_loss: 0.6388 - val_accuracy: 0.6938\n",
      "Epoch 328/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6451 - accuracy: 0.7414 - val_loss: 0.5710 - val_accuracy: 0.7825\n",
      "Epoch 329/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6812 - accuracy: 0.7337 - val_loss: 0.5383 - val_accuracy: 0.7825\n",
      "Epoch 330/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6669 - accuracy: 0.7369 - val_loss: 0.5317 - val_accuracy: 0.7825\n",
      "Epoch 331/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6471 - accuracy: 0.7396 - val_loss: 0.5267 - val_accuracy: 0.7806\n",
      "Epoch 332/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6611 - accuracy: 0.7353 - val_loss: 0.5963 - val_accuracy: 0.7827\n",
      "Epoch 333/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6656 - accuracy: 0.7364 - val_loss: 0.5221 - val_accuracy: 0.7819\n",
      "Epoch 334/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6280 - accuracy: 0.7447 - val_loss: 0.5218 - val_accuracy: 0.7819\n",
      "Epoch 335/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6679 - accuracy: 0.7353 - val_loss: 0.5314 - val_accuracy: 0.7825\n",
      "Epoch 336/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6493 - accuracy: 0.7400 - val_loss: 0.5261 - val_accuracy: 0.7810\n",
      "Epoch 337/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6719 - accuracy: 0.7345 - val_loss: 0.5636 - val_accuracy: 0.7819\n",
      "Epoch 338/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6515 - accuracy: 0.7429 - val_loss: 0.5459 - val_accuracy: 0.7821\n",
      "Epoch 339/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6663 - accuracy: 0.7343 - val_loss: 0.5437 - val_accuracy: 0.7823\n",
      "Epoch 340/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6571 - accuracy: 0.7375 - val_loss: 0.5687 - val_accuracy: 0.7760\n",
      "Epoch 341/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6665 - accuracy: 0.7344 - val_loss: 0.5216 - val_accuracy: 0.7823\n",
      "Epoch 342/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6578 - accuracy: 0.7366 - val_loss: 0.5481 - val_accuracy: 0.7781\n",
      "Epoch 343/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6611 - accuracy: 0.7402 - val_loss: 0.5214 - val_accuracy: 0.7823\n",
      "Epoch 344/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6504 - accuracy: 0.7388 - val_loss: 0.5335 - val_accuracy: 0.7825\n",
      "Epoch 345/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6500 - accuracy: 0.7416 - val_loss: 0.6197 - val_accuracy: 0.7829\n",
      "Epoch 346/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6464 - accuracy: 0.7419 - val_loss: 0.7396 - val_accuracy: 0.4352\n",
      "Epoch 347/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6685 - accuracy: 0.7354 - val_loss: 0.5212 - val_accuracy: 0.7823\n",
      "Epoch 348/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6419 - accuracy: 0.7466 - val_loss: 0.7549 - val_accuracy: 0.7829\n",
      "Epoch 349/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6668 - accuracy: 0.7444 - val_loss: 0.5312 - val_accuracy: 0.7825\n",
      "Epoch 350/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6581 - accuracy: 0.7365 - val_loss: 0.5212 - val_accuracy: 0.7823\n",
      "Epoch 351/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6641 - accuracy: 0.7405 - val_loss: 0.5241 - val_accuracy: 0.7825\n",
      "Epoch 352/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6697 - accuracy: 0.7378 - val_loss: 0.6170 - val_accuracy: 0.7829\n",
      "Epoch 353/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6548 - accuracy: 0.7419 - val_loss: 0.5277 - val_accuracy: 0.7819\n",
      "Epoch 354/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6631 - accuracy: 0.7370 - val_loss: 1.1343 - val_accuracy: 0.2791\n",
      "Epoch 355/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6575 - accuracy: 0.7376 - val_loss: 0.6834 - val_accuracy: 0.7829\n",
      "Epoch 356/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6372 - accuracy: 0.7451 - val_loss: 0.5578 - val_accuracy: 0.7819\n",
      "Epoch 357/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6549 - accuracy: 0.7419 - val_loss: 0.5365 - val_accuracy: 0.7808\n",
      "Epoch 358/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6558 - accuracy: 0.7411 - val_loss: 1.5436 - val_accuracy: 0.2521\n",
      "Epoch 359/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.7412 - val_loss: 0.5215 - val_accuracy: 0.7823\n",
      "Epoch 360/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6240 - accuracy: 0.7466 - val_loss: 0.6362 - val_accuracy: 0.7829\n",
      "Epoch 361/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6541 - accuracy: 0.7407 - val_loss: 0.7163 - val_accuracy: 0.4723\n",
      "Epoch 362/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6592 - accuracy: 0.7394 - val_loss: 0.5208 - val_accuracy: 0.7821\n",
      "Epoch 363/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.7454 - val_loss: 0.5239 - val_accuracy: 0.7825\n",
      "Epoch 364/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6364 - accuracy: 0.7457 - val_loss: 0.5502 - val_accuracy: 0.7819\n",
      "Epoch 365/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6608 - accuracy: 0.7383 - val_loss: 0.6105 - val_accuracy: 0.7829\n",
      "Epoch 366/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6472 - accuracy: 0.7433 - val_loss: 0.6636 - val_accuracy: 0.6039\n",
      "Epoch 367/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6468 - accuracy: 0.7364 - val_loss: 0.5240 - val_accuracy: 0.7823\n",
      "Epoch 368/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6530 - accuracy: 0.7460 - val_loss: 0.5291 - val_accuracy: 0.7825\n",
      "Epoch 369/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6605 - accuracy: 0.7389 - val_loss: 0.7616 - val_accuracy: 0.7829\n",
      "Epoch 370/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6292 - accuracy: 0.7452 - val_loss: 1.3050 - val_accuracy: 0.7827\n",
      "Epoch 371/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6389 - accuracy: 0.7445 - val_loss: 0.5223 - val_accuracy: 0.7825\n",
      "Epoch 372/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6437 - accuracy: 0.7400 - val_loss: 0.5394 - val_accuracy: 0.7821\n",
      "Epoch 373/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6565 - accuracy: 0.7391 - val_loss: 0.5837 - val_accuracy: 0.7747\n",
      "Epoch 374/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6498 - accuracy: 0.7410 - val_loss: 0.8886 - val_accuracy: 0.7827\n",
      "Epoch 375/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6267 - accuracy: 0.7452 - val_loss: 0.5845 - val_accuracy: 0.7827\n",
      "Epoch 376/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6452 - accuracy: 0.7450 - val_loss: 1.1411 - val_accuracy: 0.7827\n",
      "Epoch 377/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6504 - accuracy: 0.7428 - val_loss: 0.6377 - val_accuracy: 0.7829\n",
      "Epoch 378/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6481 - accuracy: 0.7418 - val_loss: 0.5268 - val_accuracy: 0.7825\n",
      "Epoch 379/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6420 - accuracy: 0.7446 - val_loss: 1.1006 - val_accuracy: 0.7827\n",
      "Epoch 380/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6370 - accuracy: 0.7461 - val_loss: 0.6547 - val_accuracy: 0.7829\n",
      "Epoch 381/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6446 - accuracy: 0.7420 - val_loss: 0.9371 - val_accuracy: 0.7827\n",
      "Epoch 382/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6491 - accuracy: 0.7398 - val_loss: 0.6624 - val_accuracy: 0.7829\n",
      "Epoch 383/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6359 - accuracy: 0.7441 - val_loss: 0.5203 - val_accuracy: 0.7825\n",
      "Epoch 384/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6469 - accuracy: 0.7419 - val_loss: 0.6068 - val_accuracy: 0.7829\n",
      "Epoch 385/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6456 - accuracy: 0.7391 - val_loss: 0.6706 - val_accuracy: 0.5859\n",
      "Epoch 386/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6440 - accuracy: 0.7421 - val_loss: 0.7767 - val_accuracy: 0.3917\n",
      "Epoch 387/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.7447 - val_loss: 0.5204 - val_accuracy: 0.7825\n",
      "Epoch 388/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6371 - accuracy: 0.7457 - val_loss: 0.5257 - val_accuracy: 0.7823\n",
      "Epoch 389/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6481 - accuracy: 0.7429 - val_loss: 0.8901 - val_accuracy: 0.3309\n",
      "Epoch 390/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6279 - accuracy: 0.7482 - val_loss: 0.5628 - val_accuracy: 0.7825\n",
      "Epoch 391/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6331 - accuracy: 0.7455 - val_loss: 0.5381 - val_accuracy: 0.7810\n",
      "Epoch 392/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6307 - accuracy: 0.7461 - val_loss: 0.5213 - val_accuracy: 0.7821\n",
      "Epoch 393/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6205 - accuracy: 0.7475 - val_loss: 0.5238 - val_accuracy: 0.7825\n",
      "Epoch 394/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6589 - accuracy: 0.7422 - val_loss: 0.5439 - val_accuracy: 0.7819\n",
      "Epoch 395/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6387 - accuracy: 0.7408 - val_loss: 0.5573 - val_accuracy: 0.7825\n",
      "Epoch 396/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6368 - accuracy: 0.7433 - val_loss: 0.5258 - val_accuracy: 0.7823\n",
      "Epoch 397/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6409 - accuracy: 0.7433 - val_loss: 0.5326 - val_accuracy: 0.7821\n",
      "Epoch 398/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6369 - accuracy: 0.7444 - val_loss: 0.5912 - val_accuracy: 0.7827\n",
      "Epoch 399/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6451 - accuracy: 0.7431 - val_loss: 0.5743 - val_accuracy: 0.7779\n",
      "Epoch 400/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6312 - accuracy: 0.7470 - val_loss: 0.5483 - val_accuracy: 0.7819\n",
      "Epoch 401/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6308 - accuracy: 0.7497 - val_loss: 0.5874 - val_accuracy: 0.7762\n",
      "Epoch 402/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6280 - accuracy: 0.7432 - val_loss: 0.5688 - val_accuracy: 0.7825\n",
      "Epoch 403/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6575 - accuracy: 0.7382 - val_loss: 0.6183 - val_accuracy: 0.7625\n",
      "Epoch 404/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6410 - accuracy: 0.7411 - val_loss: 0.5196 - val_accuracy: 0.7823\n",
      "Epoch 405/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6307 - accuracy: 0.7461 - val_loss: 0.5913 - val_accuracy: 0.7827\n",
      "Epoch 406/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6464 - accuracy: 0.7368 - val_loss: 0.6030 - val_accuracy: 0.7829\n",
      "Epoch 407/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6332 - accuracy: 0.7461 - val_loss: 0.5905 - val_accuracy: 0.7762\n",
      "Epoch 408/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6193 - accuracy: 0.7504 - val_loss: 0.5396 - val_accuracy: 0.7819\n",
      "Epoch 409/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6275 - accuracy: 0.7474 - val_loss: 0.6174 - val_accuracy: 0.7829\n",
      "Epoch 410/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6259 - accuracy: 0.7485 - val_loss: 0.6661 - val_accuracy: 0.7829\n",
      "Epoch 411/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6333 - accuracy: 0.7462 - val_loss: 0.6755 - val_accuracy: 0.7829\n",
      "Epoch 412/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6230 - accuracy: 0.7490 - val_loss: 0.6863 - val_accuracy: 0.7829\n",
      "Epoch 413/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6426 - accuracy: 0.7410 - val_loss: 0.6270 - val_accuracy: 0.7456\n",
      "Epoch 414/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6340 - accuracy: 0.7460 - val_loss: 0.5499 - val_accuracy: 0.7823\n",
      "Epoch 415/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6260 - accuracy: 0.7460 - val_loss: 0.9035 - val_accuracy: 0.3317\n",
      "Epoch 416/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6296 - accuracy: 0.7445 - val_loss: 0.5753 - val_accuracy: 0.7783\n",
      "Epoch 417/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6500 - accuracy: 0.7447 - val_loss: 0.5381 - val_accuracy: 0.7821\n",
      "Epoch 418/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6316 - accuracy: 0.7479 - val_loss: 0.5317 - val_accuracy: 0.7821\n",
      "Epoch 419/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6287 - accuracy: 0.7446 - val_loss: 0.5218 - val_accuracy: 0.7821\n",
      "Epoch 420/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6411 - accuracy: 0.7460 - val_loss: 0.5197 - val_accuracy: 0.7823\n",
      "Epoch 421/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6239 - accuracy: 0.7473 - val_loss: 0.7435 - val_accuracy: 0.4426\n",
      "Epoch 422/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6327 - accuracy: 0.7466 - val_loss: 1.0975 - val_accuracy: 0.7827\n",
      "Epoch 423/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6493 - accuracy: 0.7440 - val_loss: 1.0507 - val_accuracy: 0.7827\n",
      "Epoch 424/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6447 - accuracy: 0.7425 - val_loss: 0.6737 - val_accuracy: 0.7829\n",
      "Epoch 425/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6362 - accuracy: 0.7471 - val_loss: 0.5764 - val_accuracy: 0.7827\n",
      "Epoch 426/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6312 - accuracy: 0.7461 - val_loss: 0.6173 - val_accuracy: 0.7829\n",
      "Epoch 427/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6364 - accuracy: 0.7475 - val_loss: 0.7266 - val_accuracy: 0.4660\n",
      "Epoch 428/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6315 - accuracy: 0.7427 - val_loss: 0.5323 - val_accuracy: 0.7817\n",
      "Epoch 429/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6373 - accuracy: 0.7440 - val_loss: 0.6525 - val_accuracy: 0.7829\n",
      "Epoch 430/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6243 - accuracy: 0.7502 - val_loss: 0.9636 - val_accuracy: 0.3171\n",
      "Epoch 431/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6241 - accuracy: 0.7490 - val_loss: 0.5207 - val_accuracy: 0.7825\n",
      "Epoch 432/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6201 - accuracy: 0.7492 - val_loss: 0.5413 - val_accuracy: 0.7819\n",
      "Epoch 433/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6239 - accuracy: 0.7488 - val_loss: 0.6539 - val_accuracy: 0.7829\n",
      "Epoch 434/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6254 - accuracy: 0.7498 - val_loss: 0.7197 - val_accuracy: 0.7827\n",
      "Epoch 435/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6312 - accuracy: 0.7477 - val_loss: 0.5215 - val_accuracy: 0.7823\n",
      "Epoch 436/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6302 - accuracy: 0.7456 - val_loss: 0.5244 - val_accuracy: 0.7821\n",
      "Epoch 437/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6407 - accuracy: 0.7438 - val_loss: 0.5260 - val_accuracy: 0.7821\n",
      "Epoch 438/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6125 - accuracy: 0.7526 - val_loss: 0.5192 - val_accuracy: 0.7825\n",
      "Epoch 439/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6243 - accuracy: 0.7489 - val_loss: 0.5350 - val_accuracy: 0.7821\n",
      "Epoch 440/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6127 - accuracy: 0.7484 - val_loss: 0.5213 - val_accuracy: 0.7821\n",
      "Epoch 441/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6232 - accuracy: 0.7467 - val_loss: 0.6003 - val_accuracy: 0.7741\n",
      "Epoch 442/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6218 - accuracy: 0.7500 - val_loss: 0.5216 - val_accuracy: 0.7821\n",
      "Epoch 443/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.7440 - val_loss: 0.5368 - val_accuracy: 0.7815\n",
      "Epoch 444/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6264 - accuracy: 0.7458 - val_loss: 0.6530 - val_accuracy: 0.6582\n",
      "Epoch 445/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6365 - accuracy: 0.7460 - val_loss: 0.5217 - val_accuracy: 0.7821\n",
      "Epoch 446/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6367 - accuracy: 0.7504 - val_loss: 0.5547 - val_accuracy: 0.7825\n",
      "Epoch 447/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6250 - accuracy: 0.7505 - val_loss: 0.7685 - val_accuracy: 0.7827\n",
      "Epoch 448/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6333 - accuracy: 0.7466 - val_loss: 0.5377 - val_accuracy: 0.7819\n",
      "Epoch 449/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6210 - accuracy: 0.7443 - val_loss: 0.5357 - val_accuracy: 0.7821\n",
      "Epoch 450/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6308 - accuracy: 0.7435 - val_loss: 0.5218 - val_accuracy: 0.7821\n",
      "Epoch 451/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6285 - accuracy: 0.7498 - val_loss: 0.5268 - val_accuracy: 0.7823\n",
      "Epoch 452/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6296 - accuracy: 0.7485 - val_loss: 0.5575 - val_accuracy: 0.7825\n",
      "Epoch 453/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6386 - accuracy: 0.7485 - val_loss: 0.5197 - val_accuracy: 0.7823\n",
      "Epoch 454/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6328 - accuracy: 0.7457 - val_loss: 0.5436 - val_accuracy: 0.7823\n",
      "Epoch 455/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6344 - accuracy: 0.7455 - val_loss: 0.5275 - val_accuracy: 0.7821\n",
      "Epoch 456/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6304 - accuracy: 0.7436 - val_loss: 0.5191 - val_accuracy: 0.7825\n",
      "Epoch 457/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6169 - accuracy: 0.7465 - val_loss: 0.8146 - val_accuracy: 0.7827\n",
      "Epoch 458/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6300 - accuracy: 0.7469 - val_loss: 0.5510 - val_accuracy: 0.7825\n",
      "Epoch 459/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6348 - accuracy: 0.7439 - val_loss: 0.5187 - val_accuracy: 0.7825\n",
      "Epoch 460/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6318 - accuracy: 0.7476 - val_loss: 0.6119 - val_accuracy: 0.7829\n",
      "Epoch 461/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6441 - accuracy: 0.7436 - val_loss: 0.5719 - val_accuracy: 0.7827\n",
      "Epoch 462/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6193 - accuracy: 0.7508 - val_loss: 0.5381 - val_accuracy: 0.7823\n",
      "Epoch 463/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.7514 - val_loss: 0.8131 - val_accuracy: 0.7827\n",
      "Epoch 464/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6412 - accuracy: 0.7432 - val_loss: 0.5208 - val_accuracy: 0.7825\n",
      "Epoch 465/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6216 - accuracy: 0.7455 - val_loss: 0.6490 - val_accuracy: 0.7829\n",
      "Epoch 466/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6284 - accuracy: 0.7464 - val_loss: 0.5253 - val_accuracy: 0.7823\n",
      "Epoch 467/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6501 - accuracy: 0.7402 - val_loss: 0.6839 - val_accuracy: 0.7827\n",
      "Epoch 468/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6147 - accuracy: 0.7484 - val_loss: 0.5654 - val_accuracy: 0.7827\n",
      "Epoch 469/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6283 - accuracy: 0.7492 - val_loss: 0.7097 - val_accuracy: 0.4973\n",
      "Epoch 470/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6179 - accuracy: 0.7473 - val_loss: 0.5739 - val_accuracy: 0.7827\n",
      "Epoch 471/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6232 - accuracy: 0.7469 - val_loss: 0.6974 - val_accuracy: 0.7827\n",
      "Epoch 472/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6289 - accuracy: 0.7437 - val_loss: 0.5480 - val_accuracy: 0.7825\n",
      "Epoch 473/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6286 - accuracy: 0.7460 - val_loss: 0.5369 - val_accuracy: 0.7815\n",
      "Epoch 474/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6212 - accuracy: 0.7444 - val_loss: 0.7783 - val_accuracy: 0.7827\n",
      "Epoch 475/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6170 - accuracy: 0.7503 - val_loss: 0.5248 - val_accuracy: 0.7823\n",
      "Epoch 476/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6144 - accuracy: 0.7456 - val_loss: 0.6372 - val_accuracy: 0.7829\n",
      "Epoch 477/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6244 - accuracy: 0.7464 - val_loss: 0.5197 - val_accuracy: 0.7823\n",
      "Epoch 478/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6218 - accuracy: 0.7503 - val_loss: 0.5812 - val_accuracy: 0.7829\n",
      "Epoch 479/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6246 - accuracy: 0.7452 - val_loss: 0.7073 - val_accuracy: 0.5055\n",
      "Epoch 480/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6356 - accuracy: 0.7457 - val_loss: 0.5216 - val_accuracy: 0.7821\n",
      "Epoch 481/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6072 - accuracy: 0.7502 - val_loss: 2.0022 - val_accuracy: 0.2418\n",
      "Epoch 482/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6269 - accuracy: 0.7466 - val_loss: 0.8998 - val_accuracy: 0.7827\n",
      "Epoch 483/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6123 - accuracy: 0.7524 - val_loss: 0.5523 - val_accuracy: 0.7825\n",
      "Epoch 484/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6144 - accuracy: 0.7498 - val_loss: 0.5766 - val_accuracy: 0.7829\n",
      "Epoch 485/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6138 - accuracy: 0.7507 - val_loss: 0.5226 - val_accuracy: 0.7821\n",
      "Epoch 486/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6212 - accuracy: 0.7473 - val_loss: 0.7450 - val_accuracy: 0.7827\n",
      "Epoch 487/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6223 - accuracy: 0.7491 - val_loss: 0.7109 - val_accuracy: 0.4994\n",
      "Epoch 488/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6201 - accuracy: 0.7488 - val_loss: 0.8501 - val_accuracy: 0.7827\n",
      "Epoch 489/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6081 - accuracy: 0.7520 - val_loss: 0.5989 - val_accuracy: 0.7829\n",
      "Epoch 490/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6203 - accuracy: 0.7497 - val_loss: 0.5212 - val_accuracy: 0.7821\n",
      "Epoch 491/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6350 - accuracy: 0.7436 - val_loss: 0.5204 - val_accuracy: 0.7823\n",
      "Epoch 492/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5984 - accuracy: 0.7541 - val_loss: 0.5902 - val_accuracy: 0.7829\n",
      "Epoch 493/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6147 - accuracy: 0.7484 - val_loss: 0.5240 - val_accuracy: 0.7821\n",
      "Epoch 494/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6246 - accuracy: 0.7463 - val_loss: 0.5179 - val_accuracy: 0.7823\n",
      "Epoch 495/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6098 - accuracy: 0.7562 - val_loss: 0.5480 - val_accuracy: 0.7808\n",
      "Epoch 496/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6211 - accuracy: 0.7464 - val_loss: 0.5311 - val_accuracy: 0.7821\n",
      "Epoch 497/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6262 - accuracy: 0.7526 - val_loss: 0.5588 - val_accuracy: 0.7827\n",
      "Epoch 498/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6278 - accuracy: 0.7430 - val_loss: 0.5194 - val_accuracy: 0.7821\n",
      "Epoch 499/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6011 - accuracy: 0.7551 - val_loss: 0.5834 - val_accuracy: 0.7829\n",
      "Epoch 500/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6325 - accuracy: 0.7465 - val_loss: 0.5183 - val_accuracy: 0.7823\n",
      "Epoch 501/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6252 - accuracy: 0.7485 - val_loss: 0.5225 - val_accuracy: 0.7821\n",
      "Epoch 502/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6328 - accuracy: 0.7438 - val_loss: 0.5203 - val_accuracy: 0.7823\n",
      "Epoch 503/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6261 - accuracy: 0.7495 - val_loss: 0.5238 - val_accuracy: 0.7823\n",
      "Epoch 504/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6199 - accuracy: 0.7463 - val_loss: 0.5194 - val_accuracy: 0.7821\n",
      "Epoch 505/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6269 - accuracy: 0.7455 - val_loss: 0.8286 - val_accuracy: 0.7827\n",
      "Epoch 506/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6159 - accuracy: 0.7486 - val_loss: 0.5352 - val_accuracy: 0.7823\n",
      "Epoch 507/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6188 - accuracy: 0.7508 - val_loss: 0.5818 - val_accuracy: 0.7829\n",
      "Epoch 508/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6100 - accuracy: 0.7521 - val_loss: 0.5606 - val_accuracy: 0.7798\n",
      "Epoch 509/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.7514 - val_loss: 0.5176 - val_accuracy: 0.7823\n",
      "Epoch 510/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6128 - accuracy: 0.7479 - val_loss: 0.5227 - val_accuracy: 0.7821\n",
      "Epoch 511/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6142 - accuracy: 0.7510 - val_loss: 0.7193 - val_accuracy: 0.4886\n",
      "Epoch 512/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6189 - accuracy: 0.7471 - val_loss: 1.0592 - val_accuracy: 0.3110\n",
      "Epoch 513/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6135 - accuracy: 0.7513 - val_loss: 0.5411 - val_accuracy: 0.7823\n",
      "Epoch 514/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6226 - accuracy: 0.7476 - val_loss: 0.5296 - val_accuracy: 0.7821\n",
      "Epoch 515/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6184 - accuracy: 0.7511 - val_loss: 0.7398 - val_accuracy: 0.7827\n",
      "Epoch 516/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6256 - accuracy: 0.7479 - val_loss: 0.5339 - val_accuracy: 0.7823\n",
      "Epoch 517/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6117 - accuracy: 0.7482 - val_loss: 1.4069 - val_accuracy: 0.2669\n",
      "Epoch 518/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6188 - accuracy: 0.7511 - val_loss: 0.5644 - val_accuracy: 0.7827\n",
      "Epoch 519/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6038 - accuracy: 0.7532 - val_loss: 0.5582 - val_accuracy: 0.7827\n",
      "Epoch 520/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6103 - accuracy: 0.7486 - val_loss: 0.5202 - val_accuracy: 0.7821\n",
      "Epoch 521/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6052 - accuracy: 0.7558 - val_loss: 0.6907 - val_accuracy: 0.7827\n",
      "Epoch 522/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6087 - accuracy: 0.7532 - val_loss: 0.5348 - val_accuracy: 0.7823\n",
      "Epoch 523/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6074 - accuracy: 0.7502 - val_loss: 0.5187 - val_accuracy: 0.7823\n",
      "Epoch 524/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6159 - accuracy: 0.7504 - val_loss: 0.5172 - val_accuracy: 0.7823\n",
      "Epoch 525/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6118 - accuracy: 0.7513 - val_loss: 0.5186 - val_accuracy: 0.7821\n",
      "Epoch 526/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6230 - accuracy: 0.7461 - val_loss: 0.5263 - val_accuracy: 0.7825\n",
      "Epoch 527/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.7524 - val_loss: 0.5320 - val_accuracy: 0.7823\n",
      "Epoch 528/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6271 - accuracy: 0.7442 - val_loss: 0.5495 - val_accuracy: 0.7827\n",
      "Epoch 529/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6119 - accuracy: 0.7490 - val_loss: 0.5776 - val_accuracy: 0.7783\n",
      "Epoch 530/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6310 - accuracy: 0.7459 - val_loss: 0.5557 - val_accuracy: 0.7800\n",
      "Epoch 531/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6126 - accuracy: 0.7490 - val_loss: 0.5426 - val_accuracy: 0.7825\n",
      "Epoch 532/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6099 - accuracy: 0.7552 - val_loss: 0.5205 - val_accuracy: 0.7821\n",
      "Epoch 533/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6088 - accuracy: 0.7505 - val_loss: 0.5174 - val_accuracy: 0.7823\n",
      "Epoch 534/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6286 - accuracy: 0.7499 - val_loss: 0.5172 - val_accuracy: 0.7823\n",
      "Epoch 535/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6144 - accuracy: 0.7529 - val_loss: 0.6716 - val_accuracy: 0.7827\n",
      "Epoch 536/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6231 - accuracy: 0.7488 - val_loss: 0.5426 - val_accuracy: 0.7810\n",
      "Epoch 537/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6123 - accuracy: 0.7508 - val_loss: 0.5248 - val_accuracy: 0.7821\n",
      "Epoch 538/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.7492 - val_loss: 0.5183 - val_accuracy: 0.7821\n",
      "Epoch 539/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6126 - accuracy: 0.7538 - val_loss: 0.6478 - val_accuracy: 0.7827\n",
      "Epoch 540/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6172 - accuracy: 0.7497 - val_loss: 0.5909 - val_accuracy: 0.7829\n",
      "Epoch 541/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6194 - accuracy: 0.7483 - val_loss: 0.6146 - val_accuracy: 0.7829\n",
      "Epoch 542/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6039 - accuracy: 0.7509 - val_loss: 0.5243 - val_accuracy: 0.7823\n",
      "Epoch 543/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6084 - accuracy: 0.7492 - val_loss: 0.5196 - val_accuracy: 0.7823\n",
      "Epoch 544/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6150 - accuracy: 0.7545 - val_loss: 0.5172 - val_accuracy: 0.7823\n",
      "Epoch 545/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6195 - accuracy: 0.7517 - val_loss: 0.5242 - val_accuracy: 0.7823\n",
      "Epoch 546/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6289 - accuracy: 0.7490 - val_loss: 0.6838 - val_accuracy: 0.7827\n",
      "Epoch 547/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6094 - accuracy: 0.7520 - val_loss: 0.5177 - val_accuracy: 0.7821\n",
      "Epoch 548/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6189 - accuracy: 0.7499 - val_loss: 0.5503 - val_accuracy: 0.7827\n",
      "Epoch 549/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6204 - accuracy: 0.7504 - val_loss: 0.5170 - val_accuracy: 0.7823\n",
      "Epoch 550/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6091 - accuracy: 0.7560 - val_loss: 0.5209 - val_accuracy: 0.7821\n",
      "Epoch 551/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6126 - accuracy: 0.7501 - val_loss: 0.5807 - val_accuracy: 0.7829\n",
      "Epoch 552/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6098 - accuracy: 0.7498 - val_loss: 0.5362 - val_accuracy: 0.7823\n",
      "Epoch 553/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6164 - accuracy: 0.7494 - val_loss: 1.1017 - val_accuracy: 0.7827\n",
      "Epoch 554/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6100 - accuracy: 0.7516 - val_loss: 0.5706 - val_accuracy: 0.7829\n",
      "Epoch 555/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6039 - accuracy: 0.7499 - val_loss: 0.5700 - val_accuracy: 0.7829\n",
      "Epoch 556/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6106 - accuracy: 0.7504 - val_loss: 0.6384 - val_accuracy: 0.7827\n",
      "Epoch 557/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6012 - accuracy: 0.7545 - val_loss: 0.6696 - val_accuracy: 0.7827\n",
      "Epoch 558/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5983 - accuracy: 0.7537 - val_loss: 0.5686 - val_accuracy: 0.7793\n",
      "Epoch 559/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6090 - accuracy: 0.7515 - val_loss: 0.7301 - val_accuracy: 0.4738\n",
      "Epoch 560/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6125 - accuracy: 0.7521 - val_loss: 0.5497 - val_accuracy: 0.7827\n",
      "Epoch 561/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6111 - accuracy: 0.7527 - val_loss: 0.5517 - val_accuracy: 0.7800\n",
      "Epoch 562/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6013 - accuracy: 0.7543 - val_loss: 0.5627 - val_accuracy: 0.7829\n",
      "Epoch 563/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6035 - accuracy: 0.7515 - val_loss: 0.5279 - val_accuracy: 0.7823\n",
      "Epoch 564/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5966 - accuracy: 0.7520 - val_loss: 0.5569 - val_accuracy: 0.7800\n",
      "Epoch 565/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6034 - accuracy: 0.7497 - val_loss: 0.5304 - val_accuracy: 0.7823\n",
      "Epoch 566/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5898 - accuracy: 0.7561 - val_loss: 0.5809 - val_accuracy: 0.7829\n",
      "Epoch 567/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6187 - accuracy: 0.7490 - val_loss: 0.5168 - val_accuracy: 0.7821\n",
      "Epoch 568/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6163 - accuracy: 0.7501 - val_loss: 0.5384 - val_accuracy: 0.7825\n",
      "Epoch 569/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6008 - accuracy: 0.7537 - val_loss: 0.5166 - val_accuracy: 0.7821\n",
      "Epoch 570/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6037 - accuracy: 0.7531 - val_loss: 0.5364 - val_accuracy: 0.7825\n",
      "Epoch 571/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6038 - accuracy: 0.7523 - val_loss: 0.5397 - val_accuracy: 0.7812\n",
      "Epoch 572/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5876 - accuracy: 0.7564 - val_loss: 0.5494 - val_accuracy: 0.7827\n",
      "Epoch 573/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6022 - accuracy: 0.7528 - val_loss: 0.5182 - val_accuracy: 0.7821\n",
      "Epoch 574/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6052 - accuracy: 0.7551 - val_loss: 0.5807 - val_accuracy: 0.7829\n",
      "Epoch 575/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6082 - accuracy: 0.7544 - val_loss: 0.5263 - val_accuracy: 0.7823\n",
      "Epoch 576/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6102 - accuracy: 0.7513 - val_loss: 0.5317 - val_accuracy: 0.7823\n",
      "Epoch 577/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5958 - accuracy: 0.7560 - val_loss: 0.5602 - val_accuracy: 0.7796\n",
      "Epoch 578/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6004 - accuracy: 0.7548 - val_loss: 0.8203 - val_accuracy: 0.7827\n",
      "Epoch 579/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6120 - accuracy: 0.7527 - val_loss: 0.5869 - val_accuracy: 0.7829\n",
      "Epoch 580/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6179 - accuracy: 0.7550 - val_loss: 0.5165 - val_accuracy: 0.7821\n",
      "Epoch 581/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.7473 - val_loss: 0.5185 - val_accuracy: 0.7821\n",
      "Epoch 582/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6032 - accuracy: 0.7571 - val_loss: 0.5682 - val_accuracy: 0.7789\n",
      "Epoch 583/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6054 - accuracy: 0.7518 - val_loss: 0.5501 - val_accuracy: 0.7827\n",
      "Epoch 584/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6005 - accuracy: 0.7532 - val_loss: 0.5700 - val_accuracy: 0.7829\n",
      "Epoch 585/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6074 - accuracy: 0.7541 - val_loss: 0.5164 - val_accuracy: 0.7821\n",
      "Epoch 586/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6052 - accuracy: 0.7516 - val_loss: 0.5654 - val_accuracy: 0.7829\n",
      "Epoch 587/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6041 - accuracy: 0.7526 - val_loss: 0.5179 - val_accuracy: 0.7821\n",
      "Epoch 588/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6156 - accuracy: 0.7505 - val_loss: 0.9234 - val_accuracy: 0.7827\n",
      "Epoch 589/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6003 - accuracy: 0.7562 - val_loss: 0.5886 - val_accuracy: 0.7829\n",
      "Epoch 590/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6092 - accuracy: 0.7544 - val_loss: 0.5166 - val_accuracy: 0.7819\n",
      "Epoch 591/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6022 - accuracy: 0.7539 - val_loss: 0.6231 - val_accuracy: 0.7827\n",
      "Epoch 592/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6059 - accuracy: 0.7507 - val_loss: 0.5443 - val_accuracy: 0.7827\n",
      "Epoch 593/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6082 - accuracy: 0.7524 - val_loss: 0.5973 - val_accuracy: 0.7829\n",
      "Epoch 594/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6081 - accuracy: 0.7506 - val_loss: 0.5232 - val_accuracy: 0.7823\n",
      "Epoch 595/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6155 - accuracy: 0.7485 - val_loss: 0.5490 - val_accuracy: 0.7827\n",
      "Epoch 596/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5947 - accuracy: 0.7555 - val_loss: 0.5170 - val_accuracy: 0.7821\n",
      "Epoch 597/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5986 - accuracy: 0.7546 - val_loss: 0.5170 - val_accuracy: 0.7821\n",
      "Epoch 598/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6013 - accuracy: 0.7541 - val_loss: 0.5515 - val_accuracy: 0.7827\n",
      "Epoch 599/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6083 - accuracy: 0.7516 - val_loss: 0.5161 - val_accuracy: 0.7819\n",
      "Epoch 600/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6060 - accuracy: 0.7533 - val_loss: 0.5724 - val_accuracy: 0.7829\n",
      "Epoch 601/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5916 - accuracy: 0.7549 - val_loss: 0.5594 - val_accuracy: 0.7829\n",
      "Epoch 602/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6129 - accuracy: 0.7532 - val_loss: 0.5186 - val_accuracy: 0.7821\n",
      "Epoch 603/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6023 - accuracy: 0.7553 - val_loss: 0.5283 - val_accuracy: 0.7823\n",
      "Epoch 604/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6097 - accuracy: 0.7519 - val_loss: 0.5176 - val_accuracy: 0.7821\n",
      "Epoch 605/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6020 - accuracy: 0.7534 - val_loss: 0.5185 - val_accuracy: 0.7821\n",
      "Epoch 606/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5975 - accuracy: 0.7530 - val_loss: 0.5339 - val_accuracy: 0.7825\n",
      "Epoch 607/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6055 - accuracy: 0.7507 - val_loss: 0.5206 - val_accuracy: 0.7825\n",
      "Epoch 608/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5934 - accuracy: 0.7561 - val_loss: 0.5850 - val_accuracy: 0.7829\n",
      "Epoch 609/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5978 - accuracy: 0.7554 - val_loss: 0.5209 - val_accuracy: 0.7823\n",
      "Epoch 610/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6031 - accuracy: 0.7526 - val_loss: 0.5437 - val_accuracy: 0.7827\n",
      "Epoch 611/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6124 - accuracy: 0.7496 - val_loss: 0.7515 - val_accuracy: 0.7827\n",
      "Epoch 612/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5993 - accuracy: 0.7540 - val_loss: 0.6316 - val_accuracy: 0.7827\n",
      "Epoch 613/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6031 - accuracy: 0.7556 - val_loss: 0.5750 - val_accuracy: 0.7829\n",
      "Epoch 614/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6081 - accuracy: 0.7523 - val_loss: 0.5253 - val_accuracy: 0.7823\n",
      "Epoch 615/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5929 - accuracy: 0.7579 - val_loss: 1.3053 - val_accuracy: 0.2846\n",
      "Epoch 616/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6095 - accuracy: 0.7499 - val_loss: 0.5653 - val_accuracy: 0.7829\n",
      "Epoch 617/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5949 - accuracy: 0.7551 - val_loss: 0.5172 - val_accuracy: 0.7819\n",
      "Epoch 618/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6134 - accuracy: 0.7501 - val_loss: 0.5844 - val_accuracy: 0.7829\n",
      "Epoch 619/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5984 - accuracy: 0.7570 - val_loss: 0.5182 - val_accuracy: 0.7821\n",
      "Epoch 620/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6184 - accuracy: 0.7472 - val_loss: 0.7135 - val_accuracy: 0.5076\n",
      "Epoch 621/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6241 - accuracy: 0.7484 - val_loss: 0.5525 - val_accuracy: 0.7804\n",
      "Epoch 622/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6027 - accuracy: 0.7512 - val_loss: 0.5253 - val_accuracy: 0.7823\n",
      "Epoch 623/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5988 - accuracy: 0.7565 - val_loss: 0.5435 - val_accuracy: 0.7827\n",
      "Epoch 624/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6034 - accuracy: 0.7536 - val_loss: 0.7835 - val_accuracy: 0.7827\n",
      "Epoch 625/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6063 - accuracy: 0.7490 - val_loss: 0.9621 - val_accuracy: 0.3370\n",
      "Epoch 626/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5967 - accuracy: 0.7518 - val_loss: 0.5300 - val_accuracy: 0.7823\n",
      "Epoch 627/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5999 - accuracy: 0.7561 - val_loss: 0.5162 - val_accuracy: 0.7819\n",
      "Epoch 628/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6075 - accuracy: 0.7545 - val_loss: 0.5284 - val_accuracy: 0.7825\n",
      "Epoch 629/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6148 - accuracy: 0.7507 - val_loss: 1.4820 - val_accuracy: 0.2701\n",
      "Epoch 630/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6148 - accuracy: 0.7514 - val_loss: 0.5329 - val_accuracy: 0.7825\n",
      "Epoch 631/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6051 - accuracy: 0.7504 - val_loss: 0.5511 - val_accuracy: 0.7829\n",
      "Epoch 632/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6028 - accuracy: 0.7510 - val_loss: 0.5331 - val_accuracy: 0.7825\n",
      "Epoch 633/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5868 - accuracy: 0.7584 - val_loss: 0.5439 - val_accuracy: 0.7827\n",
      "Epoch 634/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5874 - accuracy: 0.7569 - val_loss: 0.5192 - val_accuracy: 0.7825\n",
      "Epoch 635/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6045 - accuracy: 0.7522 - val_loss: 0.7011 - val_accuracy: 0.7827\n",
      "Epoch 636/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6012 - accuracy: 0.7552 - val_loss: 0.5231 - val_accuracy: 0.7825\n",
      "Epoch 637/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5998 - accuracy: 0.7526 - val_loss: 0.6732 - val_accuracy: 0.7827\n",
      "Epoch 638/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5963 - accuracy: 0.7534 - val_loss: 0.5859 - val_accuracy: 0.7827\n",
      "Epoch 639/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6015 - accuracy: 0.7547 - val_loss: 0.5246 - val_accuracy: 0.7823\n",
      "Epoch 640/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6020 - accuracy: 0.7524 - val_loss: 0.5880 - val_accuracy: 0.7777\n",
      "Epoch 641/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6002 - accuracy: 0.7550 - val_loss: 0.6318 - val_accuracy: 0.7827\n",
      "Epoch 642/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5990 - accuracy: 0.7578 - val_loss: 0.5575 - val_accuracy: 0.7800\n",
      "Epoch 643/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6033 - accuracy: 0.7489 - val_loss: 0.6327 - val_accuracy: 0.7827\n",
      "Epoch 644/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6067 - accuracy: 0.7549 - val_loss: 0.5961 - val_accuracy: 0.7827\n",
      "Epoch 645/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5976 - accuracy: 0.7526 - val_loss: 0.5477 - val_accuracy: 0.7829\n",
      "Epoch 646/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6158 - accuracy: 0.7481 - val_loss: 0.5474 - val_accuracy: 0.7829\n",
      "Epoch 647/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6033 - accuracy: 0.7549 - val_loss: 0.5256 - val_accuracy: 0.7823\n",
      "Epoch 648/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5943 - accuracy: 0.7546 - val_loss: 0.5684 - val_accuracy: 0.7789\n",
      "Epoch 649/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5978 - accuracy: 0.7578 - val_loss: 0.5238 - val_accuracy: 0.7823\n",
      "Epoch 650/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6108 - accuracy: 0.7521 - val_loss: 0.5275 - val_accuracy: 0.7823\n",
      "Epoch 651/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.7559 - val_loss: 0.5231 - val_accuracy: 0.7823\n",
      "Epoch 652/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5974 - accuracy: 0.7556 - val_loss: 0.5294 - val_accuracy: 0.7823\n",
      "Epoch 653/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5993 - accuracy: 0.7575 - val_loss: 0.5526 - val_accuracy: 0.7829\n",
      "Epoch 654/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6190 - accuracy: 0.7477 - val_loss: 0.6102 - val_accuracy: 0.7827\n",
      "Epoch 655/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5958 - accuracy: 0.7540 - val_loss: 0.5641 - val_accuracy: 0.7796\n",
      "Epoch 656/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5881 - accuracy: 0.7561 - val_loss: 0.5159 - val_accuracy: 0.7819\n",
      "Epoch 657/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.7552 - val_loss: 0.5208 - val_accuracy: 0.7821\n",
      "Epoch 658/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6039 - accuracy: 0.7517 - val_loss: 0.5162 - val_accuracy: 0.7821\n",
      "Epoch 659/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5984 - accuracy: 0.7533 - val_loss: 0.5161 - val_accuracy: 0.7821\n",
      "Epoch 660/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5990 - accuracy: 0.7536 - val_loss: 0.5184 - val_accuracy: 0.7819\n",
      "Epoch 661/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6022 - accuracy: 0.7530 - val_loss: 0.5483 - val_accuracy: 0.7810\n",
      "Epoch 662/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6008 - accuracy: 0.7523 - val_loss: 0.5175 - val_accuracy: 0.7825\n",
      "Epoch 663/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6004 - accuracy: 0.7583 - val_loss: 0.6403 - val_accuracy: 0.6869\n",
      "Epoch 664/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5861 - accuracy: 0.7574 - val_loss: 0.5532 - val_accuracy: 0.7829\n",
      "Epoch 665/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6085 - accuracy: 0.7487 - val_loss: 0.5334 - val_accuracy: 0.7823\n",
      "Epoch 666/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6007 - accuracy: 0.7545 - val_loss: 0.5462 - val_accuracy: 0.7829\n",
      "Epoch 667/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5884 - accuracy: 0.7565 - val_loss: 0.5675 - val_accuracy: 0.7829\n",
      "Epoch 668/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6029 - accuracy: 0.7523 - val_loss: 0.7567 - val_accuracy: 0.7827\n",
      "Epoch 669/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5907 - accuracy: 0.7549 - val_loss: 0.5246 - val_accuracy: 0.7823\n",
      "Epoch 670/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5994 - accuracy: 0.7543 - val_loss: 0.5185 - val_accuracy: 0.7819\n",
      "Epoch 671/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5965 - accuracy: 0.7571 - val_loss: 0.5386 - val_accuracy: 0.7821\n",
      "Epoch 672/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6059 - accuracy: 0.7517 - val_loss: 0.5342 - val_accuracy: 0.7827\n",
      "Epoch 673/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5953 - accuracy: 0.7555 - val_loss: 0.7256 - val_accuracy: 0.7827\n",
      "Epoch 674/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6004 - accuracy: 0.7540 - val_loss: 0.5674 - val_accuracy: 0.7827\n",
      "Epoch 675/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5984 - accuracy: 0.7537 - val_loss: 0.6011 - val_accuracy: 0.7724\n",
      "Epoch 676/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5987 - accuracy: 0.7520 - val_loss: 0.5553 - val_accuracy: 0.7829\n",
      "Epoch 677/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5923 - accuracy: 0.7573 - val_loss: 0.5167 - val_accuracy: 0.7819\n",
      "Epoch 678/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5925 - accuracy: 0.7587 - val_loss: 0.5453 - val_accuracy: 0.7817\n",
      "Epoch 679/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6056 - accuracy: 0.7536 - val_loss: 0.5158 - val_accuracy: 0.7821\n",
      "Epoch 680/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5975 - accuracy: 0.7562 - val_loss: 0.6338 - val_accuracy: 0.7827\n",
      "Epoch 681/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5926 - accuracy: 0.7537 - val_loss: 0.5249 - val_accuracy: 0.7823\n",
      "Epoch 682/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6032 - accuracy: 0.7494 - val_loss: 0.5208 - val_accuracy: 0.7823\n",
      "Epoch 683/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5939 - accuracy: 0.7575 - val_loss: 0.5270 - val_accuracy: 0.7827\n",
      "Epoch 684/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5907 - accuracy: 0.7610 - val_loss: 0.5517 - val_accuracy: 0.7829\n",
      "Epoch 685/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5930 - accuracy: 0.7543 - val_loss: 0.6081 - val_accuracy: 0.7827\n",
      "Epoch 686/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5949 - accuracy: 0.7565 - val_loss: 0.5540 - val_accuracy: 0.7829\n",
      "Epoch 687/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6049 - accuracy: 0.7511 - val_loss: 0.5224 - val_accuracy: 0.7823\n",
      "Epoch 688/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6094 - accuracy: 0.7502 - val_loss: 0.5424 - val_accuracy: 0.7829\n",
      "Epoch 689/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5973 - accuracy: 0.7528 - val_loss: 0.5407 - val_accuracy: 0.7829\n",
      "Epoch 690/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5902 - accuracy: 0.7578 - val_loss: 0.7758 - val_accuracy: 0.4320\n",
      "Epoch 691/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6105 - accuracy: 0.7498 - val_loss: 0.5835 - val_accuracy: 0.7785\n",
      "Epoch 692/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6105 - accuracy: 0.7491 - val_loss: 0.5889 - val_accuracy: 0.7827\n",
      "Epoch 693/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5922 - accuracy: 0.7539 - val_loss: 0.5176 - val_accuracy: 0.7819\n",
      "Epoch 694/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5948 - accuracy: 0.7533 - val_loss: 0.5714 - val_accuracy: 0.7791\n",
      "Epoch 695/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5998 - accuracy: 0.7558 - val_loss: 0.5787 - val_accuracy: 0.7787\n",
      "Epoch 696/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5959 - accuracy: 0.7515 - val_loss: 0.6693 - val_accuracy: 0.7827\n",
      "Epoch 697/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5977 - accuracy: 0.7515 - val_loss: 0.5722 - val_accuracy: 0.7827\n",
      "Epoch 698/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5900 - accuracy: 0.7553 - val_loss: 0.5399 - val_accuracy: 0.7829\n",
      "Epoch 699/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5890 - accuracy: 0.7540 - val_loss: 0.5173 - val_accuracy: 0.7819\n",
      "Epoch 700/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5895 - accuracy: 0.7561 - val_loss: 0.5323 - val_accuracy: 0.7823\n",
      "Epoch 701/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5910 - accuracy: 0.7556 - val_loss: 0.9725 - val_accuracy: 0.7827\n",
      "Epoch 702/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5901 - accuracy: 0.7555 - val_loss: 0.6225 - val_accuracy: 0.7827\n",
      "Epoch 703/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5798 - accuracy: 0.7609 - val_loss: 0.5155 - val_accuracy: 0.7819\n",
      "Epoch 704/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5999 - accuracy: 0.7531 - val_loss: 0.7644 - val_accuracy: 0.4481\n",
      "Epoch 705/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5837 - accuracy: 0.7567 - val_loss: 0.6653 - val_accuracy: 0.7827\n",
      "Epoch 706/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5971 - accuracy: 0.7542 - val_loss: 0.5798 - val_accuracy: 0.7827\n",
      "Epoch 707/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5870 - accuracy: 0.7571 - val_loss: 0.5426 - val_accuracy: 0.7829\n",
      "Epoch 708/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5985 - accuracy: 0.7549 - val_loss: 0.6759 - val_accuracy: 0.5897\n",
      "Epoch 709/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5914 - accuracy: 0.7544 - val_loss: 0.6946 - val_accuracy: 0.5572\n",
      "Epoch 710/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5888 - accuracy: 0.7574 - val_loss: 0.8464 - val_accuracy: 0.7827\n",
      "Epoch 711/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.7540 - val_loss: 0.5297 - val_accuracy: 0.7827\n",
      "Epoch 712/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6036 - accuracy: 0.7530 - val_loss: 0.5464 - val_accuracy: 0.7829\n",
      "Epoch 713/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5859 - accuracy: 0.7575 - val_loss: 0.5230 - val_accuracy: 0.7825\n",
      "Epoch 714/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6002 - accuracy: 0.7555 - val_loss: 0.5381 - val_accuracy: 0.7829\n",
      "Epoch 715/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5954 - accuracy: 0.7537 - val_loss: 0.5378 - val_accuracy: 0.7829\n",
      "Epoch 716/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6003 - accuracy: 0.7512 - val_loss: 0.5308 - val_accuracy: 0.7827\n",
      "Epoch 717/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5880 - accuracy: 0.7555 - val_loss: 0.5149 - val_accuracy: 0.7821\n",
      "Epoch 718/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5854 - accuracy: 0.7588 - val_loss: 0.5444 - val_accuracy: 0.7829\n",
      "Epoch 719/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5850 - accuracy: 0.7570 - val_loss: 0.5560 - val_accuracy: 0.7815\n",
      "Epoch 720/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5864 - accuracy: 0.7563 - val_loss: 0.5221 - val_accuracy: 0.7819\n",
      "Epoch 721/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5893 - accuracy: 0.7577 - val_loss: 0.5239 - val_accuracy: 0.7827\n",
      "Epoch 722/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5872 - accuracy: 0.7573 - val_loss: 0.5367 - val_accuracy: 0.7821\n",
      "Epoch 723/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5888 - accuracy: 0.7578 - val_loss: 0.5238 - val_accuracy: 0.7819\n",
      "Epoch 724/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5964 - accuracy: 0.7538 - val_loss: 0.5347 - val_accuracy: 0.7829\n",
      "Epoch 725/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5894 - accuracy: 0.7544 - val_loss: 0.5164 - val_accuracy: 0.7825\n",
      "Epoch 726/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5921 - accuracy: 0.7584 - val_loss: 0.5149 - val_accuracy: 0.7821\n",
      "Epoch 727/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5940 - accuracy: 0.7576 - val_loss: 0.5770 - val_accuracy: 0.7827\n",
      "Epoch 728/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5787 - accuracy: 0.7590 - val_loss: 0.5623 - val_accuracy: 0.7827\n",
      "Epoch 729/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5967 - accuracy: 0.7574 - val_loss: 0.5149 - val_accuracy: 0.7821\n",
      "Epoch 730/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5879 - accuracy: 0.7548 - val_loss: 0.8721 - val_accuracy: 0.3765\n",
      "Epoch 731/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5873 - accuracy: 0.7583 - val_loss: 0.5502 - val_accuracy: 0.7817\n",
      "Epoch 732/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5889 - accuracy: 0.7551 - val_loss: 0.6909 - val_accuracy: 0.5631\n",
      "Epoch 733/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5934 - accuracy: 0.7568 - val_loss: 0.5188 - val_accuracy: 0.7819\n",
      "Epoch 734/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5825 - accuracy: 0.7580 - val_loss: 0.5264 - val_accuracy: 0.7827\n",
      "Epoch 735/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5829 - accuracy: 0.7599 - val_loss: 0.5458 - val_accuracy: 0.7827\n",
      "Epoch 736/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5899 - accuracy: 0.7563 - val_loss: 0.5808 - val_accuracy: 0.7827\n",
      "Epoch 737/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5869 - accuracy: 0.7567 - val_loss: 0.5206 - val_accuracy: 0.7825\n",
      "Epoch 738/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5874 - accuracy: 0.7580 - val_loss: 0.5662 - val_accuracy: 0.7827\n",
      "Epoch 739/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5909 - accuracy: 0.7556 - val_loss: 0.5418 - val_accuracy: 0.7819\n",
      "Epoch 740/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5962 - accuracy: 0.7526 - val_loss: 0.5344 - val_accuracy: 0.7821\n",
      "Epoch 741/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6004 - accuracy: 0.7514 - val_loss: 0.5148 - val_accuracy: 0.7821\n",
      "Epoch 742/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6024 - accuracy: 0.7546 - val_loss: 0.8946 - val_accuracy: 0.3685\n",
      "Epoch 743/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5986 - accuracy: 0.7548 - val_loss: 0.5365 - val_accuracy: 0.7829\n",
      "Epoch 744/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5938 - accuracy: 0.7535 - val_loss: 0.6060 - val_accuracy: 0.7827\n",
      "Epoch 745/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5859 - accuracy: 0.7563 - val_loss: 0.5525 - val_accuracy: 0.7827\n",
      "Epoch 746/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5906 - accuracy: 0.7569 - val_loss: 0.5160 - val_accuracy: 0.7825\n",
      "Epoch 747/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5871 - accuracy: 0.7566 - val_loss: 0.5335 - val_accuracy: 0.7829\n",
      "Epoch 748/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5818 - accuracy: 0.7574 - val_loss: 0.5203 - val_accuracy: 0.7825\n",
      "Epoch 749/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5909 - accuracy: 0.7577 - val_loss: 0.5417 - val_accuracy: 0.7827\n",
      "Epoch 750/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5906 - accuracy: 0.7572 - val_loss: 0.9191 - val_accuracy: 0.3617\n",
      "Epoch 751/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5861 - accuracy: 0.7580 - val_loss: 0.6055 - val_accuracy: 0.7827\n",
      "Epoch 752/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5849 - accuracy: 0.7565 - val_loss: 0.5555 - val_accuracy: 0.7827\n",
      "Epoch 753/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6034 - accuracy: 0.7489 - val_loss: 0.6898 - val_accuracy: 0.7827\n",
      "Epoch 754/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5885 - accuracy: 0.7569 - val_loss: 0.5146 - val_accuracy: 0.7825\n",
      "Epoch 755/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5874 - accuracy: 0.7539 - val_loss: 0.5758 - val_accuracy: 0.7791\n",
      "Epoch 756/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5927 - accuracy: 0.7561 - val_loss: 0.5272 - val_accuracy: 0.7827\n",
      "Epoch 757/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5924 - accuracy: 0.7555 - val_loss: 0.5152 - val_accuracy: 0.7825\n",
      "Epoch 758/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5856 - accuracy: 0.7575 - val_loss: 0.5276 - val_accuracy: 0.7819\n",
      "Epoch 759/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5825 - accuracy: 0.7592 - val_loss: 0.5577 - val_accuracy: 0.7827\n",
      "Epoch 760/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5786 - accuracy: 0.7597 - val_loss: 0.6369 - val_accuracy: 0.6894\n",
      "Epoch 761/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5762 - accuracy: 0.7617 - val_loss: 0.5494 - val_accuracy: 0.7827\n",
      "Epoch 762/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5884 - accuracy: 0.7549 - val_loss: 0.5158 - val_accuracy: 0.7825\n",
      "Epoch 763/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5880 - accuracy: 0.7573 - val_loss: 0.6582 - val_accuracy: 0.6315\n",
      "Epoch 764/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5815 - accuracy: 0.7585 - val_loss: 0.5416 - val_accuracy: 0.7827\n",
      "Epoch 765/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6003 - accuracy: 0.7545 - val_loss: 0.5677 - val_accuracy: 0.7827\n",
      "Epoch 766/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5891 - accuracy: 0.7560 - val_loss: 0.5179 - val_accuracy: 0.7819\n",
      "Epoch 767/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5740 - accuracy: 0.7617 - val_loss: 0.5416 - val_accuracy: 0.7827\n",
      "Epoch 768/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5917 - accuracy: 0.7543 - val_loss: 0.5760 - val_accuracy: 0.7796\n",
      "Epoch 769/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5948 - accuracy: 0.7550 - val_loss: 0.6482 - val_accuracy: 0.6613\n",
      "Epoch 770/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5874 - accuracy: 0.7576 - val_loss: 0.5256 - val_accuracy: 0.7827\n",
      "Epoch 771/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5857 - accuracy: 0.7569 - val_loss: 0.5146 - val_accuracy: 0.7825\n",
      "Epoch 772/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5935 - accuracy: 0.7544 - val_loss: 0.8150 - val_accuracy: 0.7827\n",
      "Epoch 773/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5880 - accuracy: 0.7570 - val_loss: 0.5621 - val_accuracy: 0.7827\n",
      "Epoch 774/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5856 - accuracy: 0.7568 - val_loss: 0.5560 - val_accuracy: 0.7827\n",
      "Epoch 775/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5818 - accuracy: 0.7591 - val_loss: 0.5155 - val_accuracy: 0.7825\n",
      "Epoch 776/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5925 - accuracy: 0.7572 - val_loss: 0.5331 - val_accuracy: 0.7829\n",
      "Epoch 777/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5866 - accuracy: 0.7581 - val_loss: 0.5417 - val_accuracy: 0.7827\n",
      "Epoch 778/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5899 - accuracy: 0.7570 - val_loss: 0.5699 - val_accuracy: 0.7796\n",
      "Epoch 779/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5864 - accuracy: 0.7576 - val_loss: 0.5402 - val_accuracy: 0.7827\n",
      "Epoch 780/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6007 - accuracy: 0.7511 - val_loss: 0.5145 - val_accuracy: 0.7825\n",
      "Epoch 781/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5804 - accuracy: 0.7617 - val_loss: 0.5475 - val_accuracy: 0.7827\n",
      "Epoch 782/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5867 - accuracy: 0.7572 - val_loss: 0.5148 - val_accuracy: 0.7825\n",
      "Epoch 783/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5976 - accuracy: 0.7552 - val_loss: 0.5315 - val_accuracy: 0.7829\n",
      "Epoch 784/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5831 - accuracy: 0.7583 - val_loss: 0.5355 - val_accuracy: 0.7821\n",
      "Epoch 785/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5885 - accuracy: 0.7590 - val_loss: 0.5473 - val_accuracy: 0.7827\n",
      "Epoch 786/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5937 - accuracy: 0.7561 - val_loss: 0.5145 - val_accuracy: 0.7825\n",
      "Epoch 787/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5848 - accuracy: 0.7593 - val_loss: 0.5237 - val_accuracy: 0.7829\n",
      "Epoch 788/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5858 - accuracy: 0.7537 - val_loss: 0.6812 - val_accuracy: 0.7827\n",
      "Epoch 789/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5913 - accuracy: 0.7555 - val_loss: 0.5748 - val_accuracy: 0.7827\n",
      "Epoch 790/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5744 - accuracy: 0.7599 - val_loss: 0.5170 - val_accuracy: 0.7819\n",
      "Epoch 791/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5841 - accuracy: 0.7571 - val_loss: 0.5311 - val_accuracy: 0.7827\n",
      "Epoch 792/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5853 - accuracy: 0.7591 - val_loss: 0.6567 - val_accuracy: 0.7827\n",
      "Epoch 793/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5817 - accuracy: 0.7563 - val_loss: 0.5880 - val_accuracy: 0.7827\n",
      "Epoch 794/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5822 - accuracy: 0.7565 - val_loss: 0.5227 - val_accuracy: 0.7819\n",
      "Epoch 795/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5872 - accuracy: 0.7544 - val_loss: 0.5152 - val_accuracy: 0.7823\n",
      "Epoch 796/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5860 - accuracy: 0.7569 - val_loss: 0.5311 - val_accuracy: 0.7827\n",
      "Epoch 797/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5788 - accuracy: 0.7584 - val_loss: 0.5334 - val_accuracy: 0.7827\n",
      "Epoch 798/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5804 - accuracy: 0.7570 - val_loss: 0.5233 - val_accuracy: 0.7829\n",
      "Epoch 799/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5830 - accuracy: 0.7607 - val_loss: 0.5147 - val_accuracy: 0.7825\n",
      "Epoch 800/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5761 - accuracy: 0.7628 - val_loss: 0.5325 - val_accuracy: 0.7817\n",
      "Epoch 801/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5791 - accuracy: 0.7584 - val_loss: 0.5169 - val_accuracy: 0.7819\n",
      "Epoch 802/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5820 - accuracy: 0.7573 - val_loss: 0.5364 - val_accuracy: 0.7827\n",
      "Epoch 803/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5795 - accuracy: 0.7624 - val_loss: 0.6039 - val_accuracy: 0.7631\n",
      "Epoch 804/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5826 - accuracy: 0.7584 - val_loss: 0.5363 - val_accuracy: 0.7827\n",
      "Epoch 805/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6006 - accuracy: 0.7535 - val_loss: 0.6074 - val_accuracy: 0.7576\n",
      "Epoch 806/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5840 - accuracy: 0.7608 - val_loss: 0.5154 - val_accuracy: 0.7823\n",
      "Epoch 807/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5855 - accuracy: 0.7557 - val_loss: 0.5494 - val_accuracy: 0.7827\n",
      "Epoch 808/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5911 - accuracy: 0.7537 - val_loss: 0.5151 - val_accuracy: 0.7825\n",
      "Epoch 809/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5929 - accuracy: 0.7563 - val_loss: 0.5142 - val_accuracy: 0.7825\n",
      "Epoch 810/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5927 - accuracy: 0.7558 - val_loss: 0.5333 - val_accuracy: 0.7817\n",
      "Epoch 811/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5918 - accuracy: 0.7545 - val_loss: 0.6723 - val_accuracy: 0.7827\n",
      "Epoch 812/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5791 - accuracy: 0.7586 - val_loss: 0.5624 - val_accuracy: 0.7827\n",
      "Epoch 813/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5826 - accuracy: 0.7588 - val_loss: 0.5261 - val_accuracy: 0.7829\n",
      "Epoch 814/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5901 - accuracy: 0.7527 - val_loss: 0.5295 - val_accuracy: 0.7827\n",
      "Epoch 815/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5846 - accuracy: 0.7572 - val_loss: 0.5150 - val_accuracy: 0.7825\n",
      "Epoch 816/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5941 - accuracy: 0.7578 - val_loss: 0.5248 - val_accuracy: 0.7829\n",
      "Epoch 817/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5822 - accuracy: 0.7576 - val_loss: 0.5502 - val_accuracy: 0.7827\n",
      "Epoch 818/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5743 - accuracy: 0.7618 - val_loss: 0.5141 - val_accuracy: 0.7825\n",
      "Epoch 819/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5857 - accuracy: 0.7627 - val_loss: 0.6170 - val_accuracy: 0.7827\n",
      "Epoch 820/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5762 - accuracy: 0.7607 - val_loss: 0.7506 - val_accuracy: 0.7827\n",
      "Epoch 821/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5891 - accuracy: 0.7551 - val_loss: 0.5331 - val_accuracy: 0.7817\n",
      "Epoch 822/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5711 - accuracy: 0.7629 - val_loss: 0.5193 - val_accuracy: 0.7819\n",
      "Epoch 823/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5828 - accuracy: 0.7575 - val_loss: 0.6119 - val_accuracy: 0.7498\n",
      "Epoch 824/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5782 - accuracy: 0.7594 - val_loss: 0.6526 - val_accuracy: 0.7827\n",
      "Epoch 825/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5865 - accuracy: 0.7583 - val_loss: 0.5150 - val_accuracy: 0.7825\n",
      "Epoch 826/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5789 - accuracy: 0.7611 - val_loss: 0.5143 - val_accuracy: 0.7825\n",
      "Epoch 827/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5777 - accuracy: 0.7597 - val_loss: 0.5152 - val_accuracy: 0.7825\n",
      "Epoch 828/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5892 - accuracy: 0.7571 - val_loss: 0.5378 - val_accuracy: 0.7817\n",
      "Epoch 829/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5831 - accuracy: 0.7580 - val_loss: 0.6235 - val_accuracy: 0.7827\n",
      "Epoch 830/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5806 - accuracy: 0.7583 - val_loss: 0.5543 - val_accuracy: 0.7827\n",
      "Epoch 831/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5753 - accuracy: 0.7583 - val_loss: 0.6753 - val_accuracy: 0.7827\n",
      "Epoch 832/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5779 - accuracy: 0.7580 - val_loss: 0.5332 - val_accuracy: 0.7819\n",
      "Epoch 833/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5928 - accuracy: 0.7535 - val_loss: 0.5603 - val_accuracy: 0.7815\n",
      "Epoch 834/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5882 - accuracy: 0.7592 - val_loss: 0.5838 - val_accuracy: 0.7793\n",
      "Epoch 835/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5975 - accuracy: 0.7559 - val_loss: 0.5670 - val_accuracy: 0.7827\n",
      "Epoch 836/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5882 - accuracy: 0.7568 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 837/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5850 - accuracy: 0.7573 - val_loss: 0.5175 - val_accuracy: 0.7827\n",
      "Epoch 838/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5948 - accuracy: 0.7544 - val_loss: 0.5393 - val_accuracy: 0.7827\n",
      "Epoch 839/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5834 - accuracy: 0.7586 - val_loss: 0.5544 - val_accuracy: 0.7827\n",
      "Epoch 840/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5792 - accuracy: 0.7579 - val_loss: 0.5853 - val_accuracy: 0.7827\n",
      "Epoch 841/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5744 - accuracy: 0.7640 - val_loss: 0.5149 - val_accuracy: 0.7825\n",
      "Epoch 842/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5793 - accuracy: 0.7610 - val_loss: 0.5917 - val_accuracy: 0.7777\n",
      "Epoch 843/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5842 - accuracy: 0.7590 - val_loss: 0.5436 - val_accuracy: 0.7827\n",
      "Epoch 844/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5832 - accuracy: 0.7601 - val_loss: 0.5811 - val_accuracy: 0.7827\n",
      "Epoch 845/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5801 - accuracy: 0.7602 - val_loss: 0.5398 - val_accuracy: 0.7817\n",
      "Epoch 846/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5825 - accuracy: 0.7582 - val_loss: 0.5732 - val_accuracy: 0.7827\n",
      "Epoch 847/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5871 - accuracy: 0.7553 - val_loss: 0.5159 - val_accuracy: 0.7827\n",
      "Epoch 848/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5813 - accuracy: 0.7579 - val_loss: 0.5183 - val_accuracy: 0.7829\n",
      "Epoch 849/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5747 - accuracy: 0.7586 - val_loss: 0.5952 - val_accuracy: 0.7747\n",
      "Epoch 850/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5784 - accuracy: 0.7594 - val_loss: 0.5337 - val_accuracy: 0.7817\n",
      "Epoch 851/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5907 - accuracy: 0.7561 - val_loss: 0.5414 - val_accuracy: 0.7827\n",
      "Epoch 852/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5905 - accuracy: 0.7546 - val_loss: 0.5194 - val_accuracy: 0.7829\n",
      "Epoch 853/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5846 - accuracy: 0.7608 - val_loss: 0.5206 - val_accuracy: 0.7819\n",
      "Epoch 854/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5859 - accuracy: 0.7560 - val_loss: 1.0881 - val_accuracy: 0.3243\n",
      "Epoch 855/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5812 - accuracy: 0.7565 - val_loss: 0.7219 - val_accuracy: 0.5099\n",
      "Epoch 856/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5765 - accuracy: 0.7619 - val_loss: 0.5138 - val_accuracy: 0.7825\n",
      "Epoch 857/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5810 - accuracy: 0.7618 - val_loss: 0.5722 - val_accuracy: 0.7827\n",
      "Epoch 858/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5823 - accuracy: 0.7588 - val_loss: 0.5141 - val_accuracy: 0.7825\n",
      "Epoch 859/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5805 - accuracy: 0.7593 - val_loss: 0.5139 - val_accuracy: 0.7825\n",
      "Epoch 860/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5800 - accuracy: 0.7605 - val_loss: 0.5143 - val_accuracy: 0.7825\n",
      "Epoch 861/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5825 - accuracy: 0.7589 - val_loss: 0.5137 - val_accuracy: 0.7825\n",
      "Epoch 862/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5762 - accuracy: 0.7608 - val_loss: 0.5149 - val_accuracy: 0.7825\n",
      "Epoch 863/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5877 - accuracy: 0.7586 - val_loss: 0.5732 - val_accuracy: 0.7798\n",
      "Epoch 864/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5895 - accuracy: 0.7597 - val_loss: 0.5263 - val_accuracy: 0.7827\n",
      "Epoch 865/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5808 - accuracy: 0.7580 - val_loss: 0.9295 - val_accuracy: 0.3619\n",
      "Epoch 866/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5796 - accuracy: 0.7578 - val_loss: 0.5533 - val_accuracy: 0.7827\n",
      "Epoch 867/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5747 - accuracy: 0.7598 - val_loss: 0.5477 - val_accuracy: 0.7827\n",
      "Epoch 868/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5803 - accuracy: 0.7562 - val_loss: 0.5139 - val_accuracy: 0.7825\n",
      "Epoch 869/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5815 - accuracy: 0.7586 - val_loss: 0.5286 - val_accuracy: 0.7819\n",
      "Epoch 870/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5835 - accuracy: 0.7588 - val_loss: 0.5657 - val_accuracy: 0.7827\n",
      "Epoch 871/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5788 - accuracy: 0.7588 - val_loss: 0.5297 - val_accuracy: 0.7819\n",
      "Epoch 872/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5745 - accuracy: 0.7611 - val_loss: 0.5213 - val_accuracy: 0.7827\n",
      "Epoch 873/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5719 - accuracy: 0.7626 - val_loss: 0.5595 - val_accuracy: 0.7827\n",
      "Epoch 874/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5837 - accuracy: 0.7563 - val_loss: 0.5326 - val_accuracy: 0.7817\n",
      "Epoch 875/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5748 - accuracy: 0.7582 - val_loss: 0.5352 - val_accuracy: 0.7827\n",
      "Epoch 876/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5808 - accuracy: 0.7599 - val_loss: 0.5199 - val_accuracy: 0.7827\n",
      "Epoch 877/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5848 - accuracy: 0.7571 - val_loss: 0.5145 - val_accuracy: 0.7825\n",
      "Epoch 878/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5781 - accuracy: 0.7588 - val_loss: 0.7847 - val_accuracy: 0.7827\n",
      "Epoch 879/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5817 - accuracy: 0.7567 - val_loss: 0.5361 - val_accuracy: 0.7827\n",
      "Epoch 880/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5739 - accuracy: 0.7607 - val_loss: 0.5191 - val_accuracy: 0.7827\n",
      "Epoch 881/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5750 - accuracy: 0.7610 - val_loss: 0.5154 - val_accuracy: 0.7825\n",
      "Epoch 882/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5774 - accuracy: 0.7587 - val_loss: 0.5139 - val_accuracy: 0.7825\n",
      "Epoch 883/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5769 - accuracy: 0.7592 - val_loss: 0.5136 - val_accuracy: 0.7827\n",
      "Epoch 884/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5791 - accuracy: 0.7583 - val_loss: 0.5469 - val_accuracy: 0.7827\n",
      "Epoch 885/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5879 - accuracy: 0.7546 - val_loss: 0.5779 - val_accuracy: 0.7827\n",
      "Epoch 886/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5825 - accuracy: 0.7608 - val_loss: 0.5235 - val_accuracy: 0.7819\n",
      "Epoch 887/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5829 - accuracy: 0.7600 - val_loss: 0.8873 - val_accuracy: 0.3782\n",
      "Epoch 888/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5815 - accuracy: 0.7573 - val_loss: 0.5139 - val_accuracy: 0.7825\n",
      "Epoch 889/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5723 - accuracy: 0.7629 - val_loss: 0.5185 - val_accuracy: 0.7823\n",
      "Epoch 890/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5866 - accuracy: 0.7589 - val_loss: 0.5435 - val_accuracy: 0.7827\n",
      "Epoch 891/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5768 - accuracy: 0.7587 - val_loss: 0.6984 - val_accuracy: 0.5494\n",
      "Epoch 892/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5739 - accuracy: 0.7600 - val_loss: 0.5181 - val_accuracy: 0.7823\n",
      "Epoch 893/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5760 - accuracy: 0.7586 - val_loss: 0.5255 - val_accuracy: 0.7827\n",
      "Epoch 894/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5782 - accuracy: 0.7593 - val_loss: 0.5262 - val_accuracy: 0.7819\n",
      "Epoch 895/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5824 - accuracy: 0.7582 - val_loss: 0.6372 - val_accuracy: 0.7827\n",
      "Epoch 896/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5834 - accuracy: 0.7596 - val_loss: 0.6774 - val_accuracy: 0.5885\n",
      "Epoch 897/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5778 - accuracy: 0.7594 - val_loss: 0.5360 - val_accuracy: 0.7827\n",
      "Epoch 898/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5815 - accuracy: 0.7608 - val_loss: 0.5355 - val_accuracy: 0.7827\n",
      "Epoch 899/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5749 - accuracy: 0.7598 - val_loss: 0.5215 - val_accuracy: 0.7827\n",
      "Epoch 900/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5821 - accuracy: 0.7566 - val_loss: 0.5136 - val_accuracy: 0.7827\n",
      "Epoch 901/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5933 - accuracy: 0.7539 - val_loss: 0.5306 - val_accuracy: 0.7819\n",
      "Epoch 902/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5879 - accuracy: 0.7575 - val_loss: 0.5296 - val_accuracy: 0.7827\n",
      "Epoch 903/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5751 - accuracy: 0.7588 - val_loss: 0.6440 - val_accuracy: 0.7827\n",
      "Epoch 904/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5849 - accuracy: 0.7577 - val_loss: 0.5259 - val_accuracy: 0.7819\n",
      "Epoch 905/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5783 - accuracy: 0.7593 - val_loss: 0.5146 - val_accuracy: 0.7829\n",
      "Epoch 906/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5792 - accuracy: 0.7587 - val_loss: 0.5135 - val_accuracy: 0.7827\n",
      "Epoch 907/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5752 - accuracy: 0.7605 - val_loss: 0.5213 - val_accuracy: 0.7827\n",
      "Epoch 908/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5701 - accuracy: 0.7631 - val_loss: 0.8354 - val_accuracy: 0.3980\n",
      "Epoch 909/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5771 - accuracy: 0.7564 - val_loss: 0.7077 - val_accuracy: 0.7827\n",
      "Epoch 910/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5836 - accuracy: 0.7579 - val_loss: 0.6634 - val_accuracy: 0.7827\n",
      "Epoch 911/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5794 - accuracy: 0.7575 - val_loss: 0.8301 - val_accuracy: 0.4035\n",
      "Epoch 912/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5712 - accuracy: 0.7633 - val_loss: 0.5796 - val_accuracy: 0.7827\n",
      "Epoch 913/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5872 - accuracy: 0.7588 - val_loss: 0.5305 - val_accuracy: 0.7819\n",
      "Epoch 914/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5746 - accuracy: 0.7614 - val_loss: 0.5407 - val_accuracy: 0.7827\n",
      "Epoch 915/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5794 - accuracy: 0.7592 - val_loss: 0.5343 - val_accuracy: 0.7819\n",
      "Epoch 916/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5743 - accuracy: 0.7594 - val_loss: 0.5231 - val_accuracy: 0.7827\n",
      "Epoch 917/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5770 - accuracy: 0.7601 - val_loss: 0.5157 - val_accuracy: 0.7827\n",
      "Epoch 918/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5798 - accuracy: 0.7593 - val_loss: 0.5137 - val_accuracy: 0.7827\n",
      "Epoch 919/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5861 - accuracy: 0.7560 - val_loss: 0.7132 - val_accuracy: 0.7827\n",
      "Epoch 920/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5722 - accuracy: 0.7587 - val_loss: 0.5427 - val_accuracy: 0.7827\n",
      "Epoch 921/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5694 - accuracy: 0.7637 - val_loss: 0.5252 - val_accuracy: 0.7819\n",
      "Epoch 922/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5757 - accuracy: 0.7594 - val_loss: 0.5681 - val_accuracy: 0.7827\n",
      "Epoch 923/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5769 - accuracy: 0.7590 - val_loss: 0.5438 - val_accuracy: 0.7817\n",
      "Epoch 924/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5803 - accuracy: 0.7626 - val_loss: 0.5639 - val_accuracy: 0.7827\n",
      "Epoch 925/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5806 - accuracy: 0.7588 - val_loss: 0.5440 - val_accuracy: 0.7817\n",
      "Epoch 926/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5735 - accuracy: 0.7607 - val_loss: 0.5147 - val_accuracy: 0.7825\n",
      "Epoch 927/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5862 - accuracy: 0.7581 - val_loss: 0.5763 - val_accuracy: 0.7827\n",
      "Epoch 928/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5758 - accuracy: 0.7604 - val_loss: 0.5134 - val_accuracy: 0.7825\n",
      "Epoch 929/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5769 - accuracy: 0.7607 - val_loss: 0.5582 - val_accuracy: 0.7827\n",
      "Epoch 930/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5798 - accuracy: 0.7582 - val_loss: 0.5269 - val_accuracy: 0.7819\n",
      "Epoch 931/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5798 - accuracy: 0.7626 - val_loss: 0.5319 - val_accuracy: 0.7827\n",
      "Epoch 932/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5808 - accuracy: 0.7600 - val_loss: 0.5224 - val_accuracy: 0.7823\n",
      "Epoch 933/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5725 - accuracy: 0.7601 - val_loss: 0.5274 - val_accuracy: 0.7827\n",
      "Epoch 934/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5726 - accuracy: 0.7616 - val_loss: 0.6123 - val_accuracy: 0.7451\n",
      "Epoch 935/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5809 - accuracy: 0.7590 - val_loss: 0.5476 - val_accuracy: 0.7827\n",
      "Epoch 936/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5756 - accuracy: 0.7631 - val_loss: 0.8862 - val_accuracy: 0.3792\n",
      "Epoch 937/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5844 - accuracy: 0.7564 - val_loss: 0.5796 - val_accuracy: 0.7827\n",
      "Epoch 938/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5692 - accuracy: 0.7613 - val_loss: 0.5753 - val_accuracy: 0.7802\n",
      "Epoch 939/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5689 - accuracy: 0.7618 - val_loss: 0.5137 - val_accuracy: 0.7827\n",
      "Epoch 940/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5729 - accuracy: 0.7596 - val_loss: 0.5564 - val_accuracy: 0.7827\n",
      "Epoch 941/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5748 - accuracy: 0.7620 - val_loss: 0.6276 - val_accuracy: 0.7827\n",
      "Epoch 942/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5780 - accuracy: 0.7612 - val_loss: 0.5170 - val_accuracy: 0.7823\n",
      "Epoch 943/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5831 - accuracy: 0.7620 - val_loss: 0.5260 - val_accuracy: 0.7827\n",
      "Epoch 944/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5717 - accuracy: 0.7619 - val_loss: 0.5412 - val_accuracy: 0.7817\n",
      "Epoch 945/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5802 - accuracy: 0.7599 - val_loss: 0.5337 - val_accuracy: 0.7827\n",
      "Epoch 946/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5774 - accuracy: 0.7595 - val_loss: 0.5493 - val_accuracy: 0.7827\n",
      "Epoch 947/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5733 - accuracy: 0.7605 - val_loss: 0.5178 - val_accuracy: 0.7827\n",
      "Epoch 948/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5639 - accuracy: 0.7646 - val_loss: 0.5391 - val_accuracy: 0.7827\n",
      "Epoch 949/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5823 - accuracy: 0.7595 - val_loss: 0.5470 - val_accuracy: 0.7827\n",
      "Epoch 950/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5749 - accuracy: 0.7606 - val_loss: 0.5139 - val_accuracy: 0.7827\n",
      "Epoch 951/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5675 - accuracy: 0.7662 - val_loss: 0.5387 - val_accuracy: 0.7827\n",
      "Epoch 952/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5844 - accuracy: 0.7541 - val_loss: 0.5384 - val_accuracy: 0.7827\n",
      "Epoch 953/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5799 - accuracy: 0.7592 - val_loss: 0.6166 - val_accuracy: 0.7407\n",
      "Epoch 954/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5730 - accuracy: 0.7621 - val_loss: 0.5670 - val_accuracy: 0.7827\n",
      "Epoch 955/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5879 - accuracy: 0.7585 - val_loss: 0.5257 - val_accuracy: 0.7823\n",
      "Epoch 956/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5856 - accuracy: 0.7559 - val_loss: 0.5134 - val_accuracy: 0.7823\n",
      "Epoch 957/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5868 - accuracy: 0.7577 - val_loss: 0.5273 - val_accuracy: 0.7827\n",
      "Epoch 958/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5665 - accuracy: 0.7646 - val_loss: 0.5134 - val_accuracy: 0.7825\n",
      "Epoch 959/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5736 - accuracy: 0.7599 - val_loss: 0.5469 - val_accuracy: 0.7827\n",
      "Epoch 960/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5704 - accuracy: 0.7571 - val_loss: 0.6712 - val_accuracy: 0.5952\n",
      "Epoch 961/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5731 - accuracy: 0.7615 - val_loss: 0.6103 - val_accuracy: 0.7827\n",
      "Epoch 962/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5764 - accuracy: 0.7594 - val_loss: 0.5202 - val_accuracy: 0.7823\n",
      "Epoch 963/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5801 - accuracy: 0.7608 - val_loss: 1.5354 - val_accuracy: 0.2760\n",
      "Epoch 964/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5724 - accuracy: 0.7584 - val_loss: 0.5138 - val_accuracy: 0.7827\n",
      "Epoch 965/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5710 - accuracy: 0.7613 - val_loss: 0.5144 - val_accuracy: 0.7827\n",
      "Epoch 966/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5746 - accuracy: 0.7613 - val_loss: 0.5775 - val_accuracy: 0.7800\n",
      "Epoch 967/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5820 - accuracy: 0.7565 - val_loss: 0.5274 - val_accuracy: 0.7827\n",
      "Epoch 968/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5655 - accuracy: 0.7608 - val_loss: 0.5529 - val_accuracy: 0.7827\n",
      "Epoch 969/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5712 - accuracy: 0.7629 - val_loss: 0.7337 - val_accuracy: 0.4901\n",
      "Epoch 970/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5683 - accuracy: 0.7637 - val_loss: 0.5142 - val_accuracy: 0.7827\n",
      "Epoch 971/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5747 - accuracy: 0.7611 - val_loss: 0.7693 - val_accuracy: 0.4508\n",
      "Epoch 972/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5834 - accuracy: 0.7590 - val_loss: 0.5591 - val_accuracy: 0.7827\n",
      "Epoch 973/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5897 - accuracy: 0.7549 - val_loss: 0.5633 - val_accuracy: 0.7815\n",
      "Epoch 974/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5805 - accuracy: 0.7562 - val_loss: 0.5748 - val_accuracy: 0.7810\n",
      "Epoch 975/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5736 - accuracy: 0.7636 - val_loss: 0.5182 - val_accuracy: 0.7823\n",
      "Epoch 976/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5699 - accuracy: 0.7638 - val_loss: 0.5987 - val_accuracy: 0.7827\n",
      "Epoch 977/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5699 - accuracy: 0.7606 - val_loss: 0.5296 - val_accuracy: 0.7819\n",
      "Epoch 978/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5659 - accuracy: 0.7622 - val_loss: 0.6018 - val_accuracy: 0.7663\n",
      "Epoch 979/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5678 - accuracy: 0.7628 - val_loss: 0.5151 - val_accuracy: 0.7827\n",
      "Epoch 980/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5806 - accuracy: 0.7602 - val_loss: 0.5243 - val_accuracy: 0.7827\n",
      "Epoch 981/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5781 - accuracy: 0.7597 - val_loss: 0.5173 - val_accuracy: 0.7827\n",
      "Epoch 982/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5746 - accuracy: 0.7598 - val_loss: 0.8142 - val_accuracy: 0.7827\n",
      "Epoch 983/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5773 - accuracy: 0.7603 - val_loss: 0.6450 - val_accuracy: 0.6632\n",
      "Epoch 984/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5779 - accuracy: 0.7599 - val_loss: 0.5599 - val_accuracy: 0.7827\n",
      "Epoch 985/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5791 - accuracy: 0.7610 - val_loss: 0.5166 - val_accuracy: 0.7827\n",
      "Epoch 986/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5736 - accuracy: 0.7606 - val_loss: 0.5379 - val_accuracy: 0.7827\n",
      "Epoch 987/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5741 - accuracy: 0.7611 - val_loss: 0.5135 - val_accuracy: 0.7827\n",
      "Epoch 988/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5689 - accuracy: 0.7618 - val_loss: 0.5136 - val_accuracy: 0.7827\n",
      "Epoch 989/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5733 - accuracy: 0.7622 - val_loss: 0.7426 - val_accuracy: 0.7827\n",
      "Epoch 990/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5797 - accuracy: 0.7575 - val_loss: 0.5435 - val_accuracy: 0.7817\n",
      "Epoch 991/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5810 - accuracy: 0.7599 - val_loss: 0.6530 - val_accuracy: 0.6394\n",
      "Epoch 992/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5767 - accuracy: 0.7616 - val_loss: 0.5260 - val_accuracy: 0.7827\n",
      "Epoch 993/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5772 - accuracy: 0.7605 - val_loss: 0.5745 - val_accuracy: 0.7812\n",
      "Epoch 994/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5801 - accuracy: 0.7594 - val_loss: 0.5168 - val_accuracy: 0.7827\n",
      "Epoch 995/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5708 - accuracy: 0.7631 - val_loss: 0.5206 - val_accuracy: 0.7823\n",
      "Epoch 996/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5767 - accuracy: 0.7613 - val_loss: 0.5681 - val_accuracy: 0.7817\n",
      "Epoch 997/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5663 - accuracy: 0.7625 - val_loss: 0.5140 - val_accuracy: 0.7827\n",
      "Epoch 998/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5731 - accuracy: 0.7614 - val_loss: 0.5158 - val_accuracy: 0.7827\n",
      "Epoch 999/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5739 - accuracy: 0.7612 - val_loss: 0.5813 - val_accuracy: 0.7804\n",
      "Epoch 1000/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5762 - accuracy: 0.7586 - val_loss: 0.5420 - val_accuracy: 0.7817\n"
     ]
    }
   ],
   "source": [
    "historyADA = model.fit(X_train, y_train, epochs=1000,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = historyADA.history['loss'] \n",
    "train_accuracy = historyADA.history['accuracy']  \n",
    "valid_loss = historyADA.history['val_loss'] \n",
    "valid_accuracy = historyADA.history['val_accuracy']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Training Loss: 60.874595642089844, Training Accuracy: 0.6995354890823364\n",
      "  Validation Loss: 6.940351963043213, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 2:\n",
      "  Training Loss: 7.656778812408447, Training Accuracy: 0.6964738368988037\n",
      "  Validation Loss: 1.6676033735275269, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 3:\n",
      "  Training Loss: 6.051638126373291, Training Accuracy: 0.6985325217247009\n",
      "  Validation Loss: 7.552335262298584, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 4:\n",
      "  Training Loss: 5.081567287445068, Training Accuracy: 0.697212815284729\n",
      "  Validation Loss: 5.316567420959473, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 5:\n",
      "  Training Loss: 4.294105052947998, Training Accuracy: 0.701224684715271\n",
      "  Validation Loss: 7.331454753875732, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 6:\n",
      "  Training Loss: 3.994002103805542, Training Accuracy: 0.7064505815505981\n",
      "  Validation Loss: 1.1122087240219116, Validation Accuracy: 0.6674408912658691\n",
      "Epoch 7:\n",
      "  Training Loss: 3.746077299118042, Training Accuracy: 0.7022275924682617\n",
      "  Validation Loss: 1.06474769115448, Validation Accuracy: 0.6602618098258972\n",
      "Epoch 8:\n",
      "  Training Loss: 3.4318668842315674, Training Accuracy: 0.6989020109176636\n",
      "  Validation Loss: 1.1885509490966797, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 9:\n",
      "  Training Loss: 3.175820827484131, Training Accuracy: 0.7019636631011963\n",
      "  Validation Loss: 1.9360291957855225, Validation Accuracy: 0.4522804021835327\n",
      "Epoch 10:\n",
      "  Training Loss: 2.997101306915283, Training Accuracy: 0.6989020109176636\n",
      "  Validation Loss: 2.2610716819763184, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 11:\n",
      "  Training Loss: 2.9331741333007812, Training Accuracy: 0.6989020109176636\n",
      "  Validation Loss: 3.6378791332244873, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 12:\n",
      "  Training Loss: 2.6546382904052734, Training Accuracy: 0.7025971412658691\n",
      "  Validation Loss: 1.1435914039611816, Validation Accuracy: 0.5206925868988037\n",
      "Epoch 13:\n",
      "  Training Loss: 2.562268018722534, Training Accuracy: 0.7006967663764954\n",
      "  Validation Loss: 0.6916362643241882, Validation Accuracy: 0.7692145109176636\n",
      "Epoch 14:\n",
      "  Training Loss: 2.4263455867767334, Training Accuracy: 0.7028610706329346\n",
      "  Validation Loss: 0.7925503253936768, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 15:\n",
      "  Training Loss: 2.4178500175476074, Training Accuracy: 0.6975823640823364\n",
      "  Validation Loss: 1.4189151525497437, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 16:\n",
      "  Training Loss: 2.3316285610198975, Training Accuracy: 0.6981630325317383\n",
      "  Validation Loss: 0.6302043795585632, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 17:\n",
      "  Training Loss: 2.190547227859497, Training Accuracy: 0.6987964510917664\n",
      "  Validation Loss: 0.607848584651947, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 18:\n",
      "  Training Loss: 2.026292562484741, Training Accuracy: 0.7023859620094299\n",
      "  Validation Loss: 2.2668850421905518, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 19:\n",
      "  Training Loss: 2.131474733352661, Training Accuracy: 0.6924092173576355\n",
      "  Validation Loss: 0.6096210479736328, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 20:\n",
      "  Training Loss: 1.9915612936019897, Training Accuracy: 0.6962626576423645\n",
      "  Validation Loss: 0.5721402764320374, Validation Accuracy: 0.7801942825317383\n",
      "Epoch 21:\n",
      "  Training Loss: 1.9856188297271729, Training Accuracy: 0.6961042881011963\n",
      "  Validation Loss: 1.6586291790008545, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 22:\n",
      "  Training Loss: 1.8460503816604614, Training Accuracy: 0.6962099075317383\n",
      "  Validation Loss: 0.5675846934318542, Validation Accuracy: 0.7751266956329346\n",
      "Epoch 23:\n",
      "  Training Loss: 1.8287830352783203, Training Accuracy: 0.6926731467247009\n",
      "  Validation Loss: 1.317911148071289, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 24:\n",
      "  Training Loss: 1.7580456733703613, Training Accuracy: 0.6928842663764954\n",
      "  Validation Loss: 1.7257013320922852, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 25:\n",
      "  Training Loss: 1.7498677968978882, Training Accuracy: 0.6905616521835327\n",
      "  Validation Loss: 0.9285635948181152, Validation Accuracy: 0.7810388803482056\n",
      "Epoch 26:\n",
      "  Training Loss: 1.6458759307861328, Training Accuracy: 0.6904560923576355\n",
      "  Validation Loss: 0.653009295463562, Validation Accuracy: 0.6222550868988037\n",
      "Epoch 27:\n",
      "  Training Loss: 1.694825530052185, Training Accuracy: 0.6897170543670654\n",
      "  Validation Loss: 3.1174135208129883, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 28:\n",
      "  Training Loss: 1.5911517143249512, Training Accuracy: 0.6938872337341309\n",
      "  Validation Loss: 1.8477898836135864, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 29:\n",
      "  Training Loss: 1.5067248344421387, Training Accuracy: 0.6902977228164673\n",
      "  Validation Loss: 0.6170712113380432, Validation Accuracy: 0.7696368098258972\n",
      "Epoch 30:\n",
      "  Training Loss: 1.4547966718673706, Training Accuracy: 0.6805849075317383\n",
      "  Validation Loss: 2.1928040981292725, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 31:\n",
      "  Training Loss: 1.5092335939407349, Training Accuracy: 0.6833826303482056\n",
      "  Validation Loss: 0.9043960571289062, Validation Accuracy: 0.7759712934494019\n",
      "Epoch 32:\n",
      "  Training Loss: 1.5076643228530884, Training Accuracy: 0.685599684715271\n",
      "  Validation Loss: 1.2715060710906982, Validation Accuracy: 0.7806165814399719\n",
      "Epoch 33:\n",
      "  Training Loss: 1.4204665422439575, Training Accuracy: 0.6860747337341309\n",
      "  Validation Loss: 0.5563026070594788, Validation Accuracy: 0.7451435923576355\n",
      "Epoch 34:\n",
      "  Training Loss: 1.4064944982528687, Training Accuracy: 0.6831186413764954\n",
      "  Validation Loss: 2.0727803707122803, Validation Accuracy: 0.22423987090587616\n",
      "Epoch 35:\n",
      "  Training Loss: 1.3674236536026, Training Accuracy: 0.6861275434494019\n",
      "  Validation Loss: 1.2870975732803345, Validation Accuracy: 0.7804054021835327\n",
      "Epoch 36:\n",
      "  Training Loss: 1.4046151638031006, Training Accuracy: 0.6802153587341309\n",
      "  Validation Loss: 0.8596963882446289, Validation Accuracy: 0.7744932174682617\n",
      "Epoch 37:\n",
      "  Training Loss: 1.4755115509033203, Training Accuracy: 0.6775760054588318\n",
      "  Validation Loss: 0.5847633481025696, Validation Accuracy: 0.6967905163764954\n",
      "Epoch 38:\n",
      "  Training Loss: 1.3833695650100708, Training Accuracy: 0.6858636140823364\n",
      "  Validation Loss: 1.4576833248138428, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 39:\n",
      "  Training Loss: 1.3270703554153442, Training Accuracy: 0.6801097989082336\n",
      "  Validation Loss: 0.7636774778366089, Validation Accuracy: 0.7721706032752991\n",
      "Epoch 40:\n",
      "  Training Loss: 1.2911617755889893, Training Accuracy: 0.6799514293670654\n",
      "  Validation Loss: 0.5869314074516296, Validation Accuracy: 0.7527449131011963\n",
      "Epoch 41:\n",
      "  Training Loss: 1.351120948791504, Training Accuracy: 0.6789484620094299\n",
      "  Validation Loss: 2.605604648590088, Validation Accuracy: 0.21748310327529907\n",
      "Epoch 42:\n",
      "  Training Loss: 1.225755214691162, Training Accuracy: 0.6840160489082336\n",
      "  Validation Loss: 1.387629747390747, Validation Accuracy: 0.22909627854824066\n",
      "Epoch 43:\n",
      "  Training Loss: 1.2904887199401855, Training Accuracy: 0.6803737282752991\n",
      "  Validation Loss: 0.6393177509307861, Validation Accuracy: 0.7639358043670654\n",
      "Epoch 44:\n",
      "  Training Loss: 1.3187549114227295, Training Accuracy: 0.6750950217247009\n",
      "  Validation Loss: 1.3414461612701416, Validation Accuracy: 0.22994087636470795\n",
      "Epoch 45:\n",
      "  Training Loss: 1.2403149604797363, Training Accuracy: 0.6817461848258972\n",
      "  Validation Loss: 0.5686275959014893, Validation Accuracy: 0.7394425868988037\n",
      "Epoch 46:\n",
      "  Training Loss: 1.2611515522003174, Training Accuracy: 0.6818517446517944\n",
      "  Validation Loss: 0.7014296054840088, Validation Accuracy: 0.771537184715271\n",
      "Epoch 47:\n",
      "  Training Loss: 1.2004104852676392, Training Accuracy: 0.6822212934494019\n",
      "  Validation Loss: 0.5874854922294617, Validation Accuracy: 0.685599684715271\n",
      "Epoch 48:\n",
      "  Training Loss: 1.219281792640686, Training Accuracy: 0.681587815284729\n",
      "  Validation Loss: 0.5684300661087036, Validation Accuracy: 0.7155827879905701\n",
      "Epoch 49:\n",
      "  Training Loss: 1.145155906677246, Training Accuracy: 0.6813238859176636\n",
      "  Validation Loss: 0.8142110705375671, Validation Accuracy: 0.7721706032752991\n",
      "Epoch 50:\n",
      "  Training Loss: 1.1528934240341187, Training Accuracy: 0.6810072064399719\n",
      "  Validation Loss: 1.230542540550232, Validation Accuracy: 0.7780827879905701\n",
      "Epoch 51:\n",
      "  Training Loss: 1.1599467992782593, Training Accuracy: 0.6813238859176636\n",
      "  Validation Loss: 0.6360669732093811, Validation Accuracy: 0.6245777010917664\n",
      "Epoch 52:\n",
      "  Training Loss: 1.1637804508209229, Training Accuracy: 0.6765202879905701\n",
      "  Validation Loss: 1.5004023313522339, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 53:\n",
      "  Training Loss: 1.1343685388565063, Training Accuracy: 0.6768369674682617\n",
      "  Validation Loss: 2.277869462966919, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 54:\n",
      "  Training Loss: 1.1341931819915771, Training Accuracy: 0.6810599565505981\n",
      "  Validation Loss: 1.2805732488632202, Validation Accuracy: 0.7789273858070374\n",
      "Epoch 55:\n",
      "  Training Loss: 1.080254077911377, Training Accuracy: 0.6841744184494019\n",
      "  Validation Loss: 0.5616409182548523, Validation Accuracy: 0.7341638803482056\n",
      "Epoch 56:\n",
      "  Training Loss: 1.0799387693405151, Training Accuracy: 0.6790540814399719\n",
      "  Validation Loss: 0.7151525020599365, Validation Accuracy: 0.771537184715271\n",
      "Epoch 57:\n",
      "  Training Loss: 1.1476560831069946, Training Accuracy: 0.6833826303482056\n",
      "  Validation Loss: 0.7635058760643005, Validation Accuracy: 0.771537184715271\n",
      "Epoch 58:\n",
      "  Training Loss: 1.1270370483398438, Training Accuracy: 0.6848606467247009\n",
      "  Validation Loss: 0.6438184380531311, Validation Accuracy: 0.7658361196517944\n",
      "Epoch 59:\n",
      "  Training Loss: 1.1454826593399048, Training Accuracy: 0.677734375\n",
      "  Validation Loss: 1.4462553262710571, Validation Accuracy: 0.22402872145175934\n",
      "Epoch 60:\n",
      "  Training Loss: 1.1579912900924683, Training Accuracy: 0.6802153587341309\n",
      "  Validation Loss: 1.4571667909622192, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 61:\n",
      "  Training Loss: 1.137825608253479, Training Accuracy: 0.6822212934494019\n",
      "  Validation Loss: 0.6118661761283875, Validation Accuracy: 0.7595016956329346\n",
      "Epoch 62:\n",
      "  Training Loss: 1.074034571647644, Training Accuracy: 0.6824852228164673\n",
      "  Validation Loss: 0.8295972347259521, Validation Accuracy: 0.7738597989082336\n",
      "Epoch 63:\n",
      "  Training Loss: 1.1032464504241943, Training Accuracy: 0.6801626086235046\n",
      "  Validation Loss: 0.6296301484107971, Validation Accuracy: 0.7643581032752991\n",
      "Epoch 64:\n",
      "  Training Loss: 1.049484133720398, Training Accuracy: 0.6783150434494019\n",
      "  Validation Loss: 0.9136400818824768, Validation Accuracy: 0.7763935923576355\n",
      "Epoch 65:\n",
      "  Training Loss: 1.0356626510620117, Training Accuracy: 0.6828547120094299\n",
      "  Validation Loss: 0.6640336513519287, Validation Accuracy: 0.7704814076423645\n",
      "Epoch 66:\n",
      "  Training Loss: 1.0577794313430786, Training Accuracy: 0.6857579946517944\n",
      "  Validation Loss: 0.9195085763931274, Validation Accuracy: 0.26604729890823364\n",
      "Epoch 67:\n",
      "  Training Loss: 1.0432870388031006, Training Accuracy: 0.6806904673576355\n",
      "  Validation Loss: 0.5604234933853149, Validation Accuracy: 0.7250844836235046\n",
      "Epoch 68:\n",
      "  Training Loss: 1.048156499862671, Training Accuracy: 0.6876055598258972\n",
      "  Validation Loss: 0.7275667190551758, Validation Accuracy: 0.771537184715271\n",
      "Epoch 69:\n",
      "  Training Loss: 0.999043345451355, Training Accuracy: 0.6878167390823364\n",
      "  Validation Loss: 0.89959716796875, Validation Accuracy: 0.7766047120094299\n",
      "Epoch 70:\n",
      "  Training Loss: 0.931025505065918, Training Accuracy: 0.6930426359176636\n",
      "  Validation Loss: 0.7701153755187988, Validation Accuracy: 0.7725929021835327\n",
      "Epoch 71:\n",
      "  Training Loss: 1.0499792098999023, Training Accuracy: 0.6842799782752991\n",
      "  Validation Loss: 1.6253130435943604, Validation Accuracy: 0.78125\n",
      "Epoch 72:\n",
      "  Training Loss: 1.043905258178711, Training Accuracy: 0.6886085271835327\n",
      "  Validation Loss: 0.608092188835144, Validation Accuracy: 0.7645692825317383\n",
      "Epoch 73:\n",
      "  Training Loss: 0.9736160039901733, Training Accuracy: 0.6882390379905701\n",
      "  Validation Loss: 0.7926326990127563, Validation Accuracy: 0.36486485600471497\n",
      "Epoch 74:\n",
      "  Training Loss: 1.0030605792999268, Training Accuracy: 0.6897698640823364\n",
      "  Validation Loss: 1.4120233058929443, Validation Accuracy: 0.22614020109176636\n",
      "Epoch 75:\n",
      "  Training Loss: 1.0242325067520142, Training Accuracy: 0.6864442825317383\n",
      "  Validation Loss: 0.557673990726471, Validation Accuracy: 0.7461993098258972\n",
      "Epoch 76:\n",
      "  Training Loss: 0.974838137626648, Training Accuracy: 0.6916701793670654\n",
      "  Validation Loss: 0.6097471714019775, Validation Accuracy: 0.7662584185600281\n",
      "Epoch 77:\n",
      "  Training Loss: 1.0012454986572266, Training Accuracy: 0.6877639293670654\n",
      "  Validation Loss: 0.6832892894744873, Validation Accuracy: 0.5491976141929626\n",
      "Epoch 78:\n",
      "  Training Loss: 1.0447109937667847, Training Accuracy: 0.6800569891929626\n",
      "  Validation Loss: 0.5904920697212219, Validation Accuracy: 0.7611908912658691\n",
      "Epoch 79:\n",
      "  Training Loss: 1.002244234085083, Training Accuracy: 0.6882917881011963\n",
      "  Validation Loss: 0.5719241499900818, Validation Accuracy: 0.7578125\n",
      "Epoch 80:\n",
      "  Training Loss: 0.9928439259529114, Training Accuracy: 0.6908255815505981\n",
      "  Validation Loss: 0.5607707500457764, Validation Accuracy: 0.7261402010917664\n",
      "Epoch 81:\n",
      "  Training Loss: 0.970404863357544, Training Accuracy: 0.6918285489082336\n",
      "  Validation Loss: 0.568454921245575, Validation Accuracy: 0.7571790814399719\n",
      "Epoch 82:\n",
      "  Training Loss: 0.991477370262146, Training Accuracy: 0.6882917881011963\n",
      "  Validation Loss: 0.7467638254165649, Validation Accuracy: 0.7730152010917664\n",
      "Epoch 83:\n",
      "  Training Loss: 0.9394211769104004, Training Accuracy: 0.6918813586235046\n",
      "  Validation Loss: 1.039229154586792, Validation Accuracy: 0.7780827879905701\n",
      "Epoch 84:\n",
      "  Training Loss: 0.9422652721405029, Training Accuracy: 0.6903505325317383\n",
      "  Validation Loss: 0.618246853351593, Validation Accuracy: 0.7696368098258972\n",
      "Epoch 85:\n",
      "  Training Loss: 1.021443247795105, Training Accuracy: 0.6870777010917664\n",
      "  Validation Loss: 1.4752849340438843, Validation Accuracy: 0.78125\n",
      "Epoch 86:\n",
      "  Training Loss: 0.9353662729263306, Training Accuracy: 0.6877639293670654\n",
      "  Validation Loss: 0.568450927734375, Validation Accuracy: 0.7595016956329346\n",
      "Epoch 87:\n",
      "  Training Loss: 0.9694288372993469, Training Accuracy: 0.6889780163764954\n",
      "  Validation Loss: 0.6243682503700256, Validation Accuracy: 0.7721706032752991\n",
      "Epoch 88:\n",
      "  Training Loss: 0.9680162668228149, Training Accuracy: 0.6920396685600281\n",
      "  Validation Loss: 0.7920171022415161, Validation Accuracy: 0.3566300570964813\n",
      "Epoch 89:\n",
      "  Training Loss: 0.9105295538902283, Training Accuracy: 0.6921452879905701\n",
      "  Validation Loss: 0.5561389923095703, Validation Accuracy: 0.7557010054588318\n",
      "Epoch 90:\n",
      "  Training Loss: 0.9228684902191162, Training Accuracy: 0.6932538151741028\n",
      "  Validation Loss: 0.6522763967514038, Validation Accuracy: 0.7717483043670654\n",
      "Epoch 91:\n",
      "  Training Loss: 0.9225171804428101, Training Accuracy: 0.697265625\n",
      "  Validation Loss: 0.5531142354011536, Validation Accuracy: 0.7548564076423645\n",
      "Epoch 92:\n",
      "  Training Loss: 0.9086980819702148, Training Accuracy: 0.689453125\n",
      "  Validation Loss: 0.7152250409126282, Validation Accuracy: 0.7734375\n",
      "Epoch 93:\n",
      "  Training Loss: 0.972856879234314, Training Accuracy: 0.691353440284729\n",
      "  Validation Loss: 0.5948530435562134, Validation Accuracy: 0.6984797120094299\n",
      "Epoch 94:\n",
      "  Training Loss: 0.9325454235076904, Training Accuracy: 0.6930954456329346\n",
      "  Validation Loss: 0.6073310375213623, Validation Accuracy: 0.6805320978164673\n",
      "Epoch 95:\n",
      "  Training Loss: 0.94120192527771, Training Accuracy: 0.6903505325317383\n",
      "  Validation Loss: 0.5569530129432678, Validation Accuracy: 0.7307854890823364\n",
      "Epoch 96:\n",
      "  Training Loss: 0.9429767727851868, Training Accuracy: 0.6929370760917664\n",
      "  Validation Loss: 0.8497254252433777, Validation Accuracy: 0.7766047120094299\n",
      "Epoch 97:\n",
      "  Training Loss: 0.892495334148407, Training Accuracy: 0.6988492608070374\n",
      "  Validation Loss: 0.9590796232223511, Validation Accuracy: 0.7782939076423645\n",
      "Epoch 98:\n",
      "  Training Loss: 0.8688826560974121, Training Accuracy: 0.7008023858070374\n",
      "  Validation Loss: 0.7676954865455627, Validation Accuracy: 0.7761824131011963\n",
      "Epoch 99:\n",
      "  Training Loss: 0.892396867275238, Training Accuracy: 0.6990076303482056\n",
      "  Validation Loss: 1.057328701019287, Validation Accuracy: 0.7804054021835327\n",
      "Epoch 100:\n",
      "  Training Loss: 0.98468416929245, Training Accuracy: 0.6955236196517944\n",
      "  Validation Loss: 0.5988225340843201, Validation Accuracy: 0.7700591087341309\n",
      "Epoch 101:\n",
      "  Training Loss: 0.9002882242202759, Training Accuracy: 0.6927787065505981\n",
      "  Validation Loss: 1.4461848735809326, Validation Accuracy: 0.2276182472705841\n",
      "Epoch 102:\n",
      "  Training Loss: 0.8728786110877991, Training Accuracy: 0.6980574131011963\n",
      "  Validation Loss: 0.6713019609451294, Validation Accuracy: 0.7732263803482056\n",
      "Epoch 103:\n",
      "  Training Loss: 0.9426708817481995, Training Accuracy: 0.6949430108070374\n",
      "  Validation Loss: 0.6441826820373535, Validation Accuracy: 0.771537184715271\n",
      "Epoch 104:\n",
      "  Training Loss: 0.8731775283813477, Training Accuracy: 0.697265625\n",
      "  Validation Loss: 0.7334637641906738, Validation Accuracy: 0.7749155163764954\n",
      "Epoch 105:\n",
      "  Training Loss: 0.8825047612190247, Training Accuracy: 0.6983741521835327\n",
      "  Validation Loss: 0.5918776988983154, Validation Accuracy: 0.7702702879905701\n",
      "Epoch 106:\n",
      "  Training Loss: 0.851935863494873, Training Accuracy: 0.6974767446517944\n",
      "  Validation Loss: 0.7158586978912354, Validation Accuracy: 0.4900760054588318\n",
      "Epoch 107:\n",
      "  Training Loss: 0.8800150752067566, Training Accuracy: 0.693412184715271\n",
      "  Validation Loss: 0.7465145587921143, Validation Accuracy: 0.7761824131011963\n",
      "Epoch 108:\n",
      "  Training Loss: 0.8981197476387024, Training Accuracy: 0.6963154673576355\n",
      "  Validation Loss: 0.5487472414970398, Validation Accuracy: 0.759712815284729\n",
      "Epoch 109:\n",
      "  Training Loss: 0.8870260119438171, Training Accuracy: 0.6995354890823364\n",
      "  Validation Loss: 0.6366976499557495, Validation Accuracy: 0.7725929021835327\n",
      "Epoch 110:\n",
      "  Training Loss: 0.868541419506073, Training Accuracy: 0.6995882391929626\n",
      "  Validation Loss: 0.6580355763435364, Validation Accuracy: 0.6043074131011963\n",
      "Epoch 111:\n",
      "  Training Loss: 0.8574360013008118, Training Accuracy: 0.6967377662658691\n",
      "  Validation Loss: 0.8626320958137512, Validation Accuracy: 0.7785050868988037\n",
      "Epoch 112:\n",
      "  Training Loss: 0.8372735977172852, Training Accuracy: 0.6986380815505981\n",
      "  Validation Loss: 0.5806108117103577, Validation Accuracy: 0.7709037065505981\n",
      "Epoch 113:\n",
      "  Training Loss: 0.8341048359870911, Training Accuracy: 0.7020164728164673\n",
      "  Validation Loss: 0.5576609373092651, Validation Accuracy: 0.765625\n",
      "Epoch 114:\n",
      "  Training Loss: 0.8684678077697754, Training Accuracy: 0.6989020109176636\n",
      "  Validation Loss: 0.9750220775604248, Validation Accuracy: 0.7804054021835327\n",
      "Epoch 115:\n",
      "  Training Loss: 0.82505863904953, Training Accuracy: 0.7037056684494019\n",
      "  Validation Loss: 0.6501362323760986, Validation Accuracy: 0.7730152010917664\n",
      "Epoch 116:\n",
      "  Training Loss: 0.85406094789505, Training Accuracy: 0.7014886140823364\n",
      "  Validation Loss: 0.551742672920227, Validation Accuracy: 0.763724684715271\n",
      "Epoch 117:\n",
      "  Training Loss: 0.8656824231147766, Training Accuracy: 0.7003272771835327\n",
      "  Validation Loss: 0.5467074513435364, Validation Accuracy: 0.7620354890823364\n",
      "Epoch 118:\n",
      "  Training Loss: 0.8666728138923645, Training Accuracy: 0.7027555108070374\n",
      "  Validation Loss: 0.6742845773696899, Validation Accuracy: 0.7738597989082336\n",
      "Epoch 119:\n",
      "  Training Loss: 0.8644949197769165, Training Accuracy: 0.7035472989082336\n",
      "  Validation Loss: 0.542820930480957, Validation Accuracy: 0.7502111196517944\n",
      "Epoch 120:\n",
      "  Training Loss: 0.8300755023956299, Training Accuracy: 0.6996938586235046\n",
      "  Validation Loss: 0.8376773595809937, Validation Accuracy: 0.7780827879905701\n",
      "Epoch 121:\n",
      "  Training Loss: 0.8566223978996277, Training Accuracy: 0.6976351141929626\n",
      "  Validation Loss: 0.9357022643089294, Validation Accuracy: 0.7804054021835327\n",
      "Epoch 122:\n",
      "  Training Loss: 0.8365761637687683, Training Accuracy: 0.7002744674682617\n",
      "  Validation Loss: 0.57025545835495, Validation Accuracy: 0.7316300868988037\n",
      "Epoch 123:\n",
      "  Training Loss: 0.8068127632141113, Training Accuracy: 0.7059755325317383\n",
      "  Validation Loss: 0.5538268089294434, Validation Accuracy: 0.7421875\n",
      "Epoch 124:\n",
      "  Training Loss: 0.8230245113372803, Training Accuracy: 0.703072190284729\n",
      "  Validation Loss: 0.7905384302139282, Validation Accuracy: 0.3534628450870514\n",
      "Epoch 125:\n",
      "  Training Loss: 0.8165136575698853, Training Accuracy: 0.7074535489082336\n",
      "  Validation Loss: 0.5464301705360413, Validation Accuracy: 0.7472550868988037\n",
      "Epoch 126:\n",
      "  Training Loss: 0.8325856924057007, Training Accuracy: 0.7039167881011963\n",
      "  Validation Loss: 0.6263468265533447, Validation Accuracy: 0.7736486196517944\n",
      "Epoch 127:\n",
      "  Training Loss: 0.8299394249916077, Training Accuracy: 0.7021748423576355\n",
      "  Validation Loss: 0.6154439449310303, Validation Accuracy: 0.6883445978164673\n",
      "Epoch 128:\n",
      "  Training Loss: 0.8563715815544128, Training Accuracy: 0.7009607553482056\n",
      "  Validation Loss: 0.7110681533813477, Validation Accuracy: 0.4942989945411682\n",
      "Epoch 129:\n",
      "  Training Loss: 0.8080189824104309, Training Accuracy: 0.7067673206329346\n",
      "  Validation Loss: 0.5414938926696777, Validation Accuracy: 0.7624577879905701\n",
      "Epoch 130:\n",
      "  Training Loss: 0.8236125707626343, Training Accuracy: 0.7052892446517944\n",
      "  Validation Loss: 0.5529202818870544, Validation Accuracy: 0.7683699131011963\n",
      "Epoch 131:\n",
      "  Training Loss: 0.8239476680755615, Training Accuracy: 0.7076646685600281\n",
      "  Validation Loss: 0.647691011428833, Validation Accuracy: 0.7742820978164673\n",
      "Epoch 132:\n",
      "  Training Loss: 0.815682053565979, Training Accuracy: 0.7025443315505981\n",
      "  Validation Loss: 0.8635544776916504, Validation Accuracy: 0.7804054021835327\n",
      "Epoch 133:\n",
      "  Training Loss: 0.8311026096343994, Training Accuracy: 0.7058699131011963\n",
      "  Validation Loss: 0.7160002589225769, Validation Accuracy: 0.7768158912658691\n",
      "Epoch 134:\n",
      "  Training Loss: 0.8031589984893799, Training Accuracy: 0.7095650434494019\n",
      "  Validation Loss: 0.5546086430549622, Validation Accuracy: 0.744087815284729\n",
      "Epoch 135:\n",
      "  Training Loss: 0.834934651851654, Training Accuracy: 0.7084037065505981\n",
      "  Validation Loss: 0.5747060179710388, Validation Accuracy: 0.771537184715271\n",
      "Epoch 136:\n",
      "  Training Loss: 0.8152570724487305, Training Accuracy: 0.7068728804588318\n",
      "  Validation Loss: 0.5421327352523804, Validation Accuracy: 0.7531672120094299\n",
      "Epoch 137:\n",
      "  Training Loss: 0.7625929117202759, Training Accuracy: 0.7101984620094299\n",
      "  Validation Loss: 0.6736301183700562, Validation Accuracy: 0.5694679021835327\n",
      "Epoch 138:\n",
      "  Training Loss: 0.8074637651443481, Training Accuracy: 0.7061338424682617\n",
      "  Validation Loss: 0.6144220232963562, Validation Accuracy: 0.7744932174682617\n",
      "Epoch 139:\n",
      "  Training Loss: 0.8300856947898865, Training Accuracy: 0.7079814076423645\n",
      "  Validation Loss: 1.0699368715286255, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 140:\n",
      "  Training Loss: 0.8251940011978149, Training Accuracy: 0.7003800868988037\n",
      "  Validation Loss: 0.7080417275428772, Validation Accuracy: 0.7774493098258972\n",
      "Epoch 141:\n",
      "  Training Loss: 0.8389090299606323, Training Accuracy: 0.7055532336235046\n",
      "  Validation Loss: 0.9565315246582031, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 142:\n",
      "  Training Loss: 0.7727746367454529, Training Accuracy: 0.7131017446517944\n",
      "  Validation Loss: 0.9628808498382568, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 143:\n",
      "  Training Loss: 0.8083053231239319, Training Accuracy: 0.7095122337341309\n",
      "  Validation Loss: 0.6020371317863464, Validation Accuracy: 0.7130489945411682\n",
      "Epoch 144:\n",
      "  Training Loss: 0.82181715965271, Training Accuracy: 0.7081925868988037\n",
      "  Validation Loss: 0.5378955602645874, Validation Accuracy: 0.7588682174682617\n",
      "Epoch 145:\n",
      "  Training Loss: 0.7827834486961365, Training Accuracy: 0.7079286575317383\n",
      "  Validation Loss: 1.1710898876190186, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 146:\n",
      "  Training Loss: 0.8236024975776672, Training Accuracy: 0.7055004239082336\n",
      "  Validation Loss: 0.602121889591217, Validation Accuracy: 0.7742820978164673\n",
      "Epoch 147:\n",
      "  Training Loss: 0.8023641705513, Training Accuracy: 0.7144214510917664\n",
      "  Validation Loss: 0.5765261054039001, Validation Accuracy: 0.7320523858070374\n",
      "Epoch 148:\n",
      "  Training Loss: 0.7863256335258484, Training Accuracy: 0.7093011140823364\n",
      "  Validation Loss: 0.5471204519271851, Validation Accuracy: 0.7510557174682617\n",
      "Epoch 149:\n",
      "  Training Loss: 0.8145859837532043, Training Accuracy: 0.7083509564399719\n",
      "  Validation Loss: 0.6046738028526306, Validation Accuracy: 0.7119932174682617\n",
      "Epoch 150:\n",
      "  Training Loss: 0.794875979423523, Training Accuracy: 0.7087204456329346\n",
      "  Validation Loss: 1.0509731769561768, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 151:\n",
      "  Training Loss: 0.8200492858886719, Training Accuracy: 0.7071896195411682\n",
      "  Validation Loss: 0.54571932554245, Validation Accuracy: 0.7700591087341309\n",
      "Epoch 152:\n",
      "  Training Loss: 0.8134610652923584, Training Accuracy: 0.7079814076423645\n",
      "  Validation Loss: 0.5812938809394836, Validation Accuracy: 0.7312077879905701\n",
      "Epoch 153:\n",
      "  Training Loss: 0.821006178855896, Training Accuracy: 0.7108319401741028\n",
      "  Validation Loss: 0.5378961563110352, Validation Accuracy: 0.7666807174682617\n",
      "Epoch 154:\n",
      "  Training Loss: 0.7890256643295288, Training Accuracy: 0.7079286575317383\n",
      "  Validation Loss: 0.8388722538948059, Validation Accuracy: 0.78125\n",
      "Epoch 155:\n",
      "  Training Loss: 0.779385507106781, Training Accuracy: 0.7132601141929626\n",
      "  Validation Loss: 0.6091825366020203, Validation Accuracy: 0.7742820978164673\n",
      "Epoch 156:\n",
      "  Training Loss: 0.8066730499267578, Training Accuracy: 0.7131017446517944\n",
      "  Validation Loss: 0.5460545420646667, Validation Accuracy: 0.7719594836235046\n",
      "Epoch 157:\n",
      "  Training Loss: 0.8057118058204651, Training Accuracy: 0.7094594836235046\n",
      "  Validation Loss: 0.5695584416389465, Validation Accuracy: 0.7728040814399719\n",
      "Epoch 158:\n",
      "  Training Loss: 0.7901694178581238, Training Accuracy: 0.7118349075317383\n",
      "  Validation Loss: 0.5418416857719421, Validation Accuracy: 0.7704814076423645\n",
      "Epoch 159:\n",
      "  Training Loss: 0.766954779624939, Training Accuracy: 0.7094066739082336\n",
      "  Validation Loss: 0.5920010209083557, Validation Accuracy: 0.7740709185600281\n",
      "Epoch 160:\n",
      "  Training Loss: 0.7773692607879639, Training Accuracy: 0.712943434715271\n",
      "  Validation Loss: 0.6265342831611633, Validation Accuracy: 0.7751266956329346\n",
      "Epoch 161:\n",
      "  Training Loss: 0.7831704020500183, Training Accuracy: 0.7072423696517944\n",
      "  Validation Loss: 0.8306741714477539, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 162:\n",
      "  Training Loss: 0.7861282825469971, Training Accuracy: 0.708984375\n",
      "  Validation Loss: 0.852207362651825, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 163:\n",
      "  Training Loss: 0.7495248317718506, Training Accuracy: 0.7119404673576355\n",
      "  Validation Loss: 0.7259047031402588, Validation Accuracy: 0.779349684715271\n",
      "Epoch 164:\n",
      "  Training Loss: 0.8072444200515747, Training Accuracy: 0.7053948640823364\n",
      "  Validation Loss: 0.5606523752212524, Validation Accuracy: 0.7451435923576355\n",
      "Epoch 165:\n",
      "  Training Loss: 0.794525682926178, Training Accuracy: 0.7139463424682617\n",
      "  Validation Loss: 0.658623218536377, Validation Accuracy: 0.7776604890823364\n",
      "Epoch 166:\n",
      "  Training Loss: 0.8134926557540894, Training Accuracy: 0.7078758478164673\n",
      "  Validation Loss: 0.5474227070808411, Validation Accuracy: 0.7523226141929626\n",
      "Epoch 167:\n",
      "  Training Loss: 0.7831281423568726, Training Accuracy: 0.7065033912658691\n",
      "  Validation Loss: 1.045346736907959, Validation Accuracy: 0.2664695978164673\n",
      "Epoch 168:\n",
      "  Training Loss: 0.7590494751930237, Training Accuracy: 0.7140519618988037\n",
      "  Validation Loss: 1.5755984783172607, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 169:\n",
      "  Training Loss: 0.785070538520813, Training Accuracy: 0.7140519618988037\n",
      "  Validation Loss: 0.9301463961601257, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 170:\n",
      "  Training Loss: 0.7826642990112305, Training Accuracy: 0.7103040814399719\n",
      "  Validation Loss: 0.5348573923110962, Validation Accuracy: 0.7696368098258972\n",
      "Epoch 171:\n",
      "  Training Loss: 0.7496727705001831, Training Accuracy: 0.7182221412658691\n",
      "  Validation Loss: 0.5420071482658386, Validation Accuracy: 0.7719594836235046\n",
      "Epoch 172:\n",
      "  Training Loss: 0.7542093396186829, Training Accuracy: 0.7135240435600281\n",
      "  Validation Loss: 0.5502656102180481, Validation Accuracy: 0.7728040814399719\n",
      "Epoch 173:\n",
      "  Training Loss: 0.7558168172836304, Training Accuracy: 0.7155299782752991\n",
      "  Validation Loss: 0.5493280291557312, Validation Accuracy: 0.7728040814399719\n",
      "Epoch 174:\n",
      "  Training Loss: 0.7708873748779297, Training Accuracy: 0.7124155163764954\n",
      "  Validation Loss: 0.7276945114135742, Validation Accuracy: 0.7804054021835327\n",
      "Epoch 175:\n",
      "  Training Loss: 0.7878056764602661, Training Accuracy: 0.7137352228164673\n",
      "  Validation Loss: 0.6804172396659851, Validation Accuracy: 0.7791385054588318\n",
      "Epoch 176:\n",
      "  Training Loss: 0.7642931342124939, Training Accuracy: 0.7164273858070374\n",
      "  Validation Loss: 0.5326191782951355, Validation Accuracy: 0.7673141956329346\n",
      "Epoch 177:\n",
      "  Training Loss: 0.763102114200592, Training Accuracy: 0.7151604890823364\n",
      "  Validation Loss: 0.6338819265365601, Validation Accuracy: 0.6803209185600281\n",
      "Epoch 178:\n",
      "  Training Loss: 0.7728660702705383, Training Accuracy: 0.7106735706329346\n",
      "  Validation Loss: 0.6373080015182495, Validation Accuracy: 0.6750422120094299\n",
      "Epoch 179:\n",
      "  Training Loss: 0.7557565569877625, Training Accuracy: 0.718697190284729\n",
      "  Validation Loss: 1.147819995880127, Validation Accuracy: 0.25612330436706543\n",
      "Epoch 180:\n",
      "  Training Loss: 0.8045748472213745, Training Accuracy: 0.7097234129905701\n",
      "  Validation Loss: 0.5438574552536011, Validation Accuracy: 0.7567567825317383\n",
      "Epoch 181:\n",
      "  Training Loss: 0.7555917501449585, Training Accuracy: 0.7118349075317383\n",
      "  Validation Loss: 0.7693860530853271, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 182:\n",
      "  Training Loss: 0.7617616653442383, Training Accuracy: 0.7120988368988037\n",
      "  Validation Loss: 1.0457895994186401, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 183:\n",
      "  Training Loss: 0.7620108723640442, Training Accuracy: 0.7155299782752991\n",
      "  Validation Loss: 0.53959721326828, Validation Accuracy: 0.7721706032752991\n",
      "Epoch 184:\n",
      "  Training Loss: 0.7449706792831421, Training Accuracy: 0.7197529673576355\n",
      "  Validation Loss: 0.5458899736404419, Validation Accuracy: 0.7734375\n",
      "Epoch 185:\n",
      "  Training Loss: 0.7805752158164978, Training Accuracy: 0.7091954946517944\n",
      "  Validation Loss: 0.6689833402633667, Validation Accuracy: 0.7791385054588318\n",
      "Epoch 186:\n",
      "  Training Loss: 0.773507833480835, Training Accuracy: 0.7204391956329346\n",
      "  Validation Loss: 0.9726454019546509, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 187:\n",
      "  Training Loss: 0.7561084032058716, Training Accuracy: 0.7156355381011963\n",
      "  Validation Loss: 0.5620880722999573, Validation Accuracy: 0.7478885054588318\n",
      "Epoch 188:\n",
      "  Training Loss: 0.7556679248809814, Training Accuracy: 0.7110430598258972\n",
      "  Validation Loss: 0.5608468651771545, Validation Accuracy: 0.7744932174682617\n",
      "Epoch 189:\n",
      "  Training Loss: 0.7505282163619995, Training Accuracy: 0.716796875\n",
      "  Validation Loss: 0.5326515436172485, Validation Accuracy: 0.7652027010917664\n",
      "Epoch 190:\n",
      "  Training Loss: 0.7446680068969727, Training Accuracy: 0.7144742608070374\n",
      "  Validation Loss: 0.6479973793029785, Validation Accuracy: 0.7789273858070374\n",
      "Epoch 191:\n",
      "  Training Loss: 0.7672054767608643, Training Accuracy: 0.7155299782752991\n",
      "  Validation Loss: 0.5313416123390198, Validation Accuracy: 0.7698479890823364\n",
      "Epoch 192:\n",
      "  Training Loss: 0.721190869808197, Training Accuracy: 0.7212309837341309\n",
      "  Validation Loss: 0.6768732070922852, Validation Accuracy: 0.5656672120094299\n",
      "Epoch 193:\n",
      "  Training Loss: 0.7073419094085693, Training Accuracy: 0.7262457609176636\n",
      "  Validation Loss: 0.573625385761261, Validation Accuracy: 0.7755489945411682\n",
      "Epoch 194:\n",
      "  Training Loss: 0.745003879070282, Training Accuracy: 0.7197529673576355\n",
      "  Validation Loss: 0.5695207715034485, Validation Accuracy: 0.7757601141929626\n",
      "Epoch 195:\n",
      "  Training Loss: 0.7119009494781494, Training Accuracy: 0.7219172120094299\n",
      "  Validation Loss: 1.6442471742630005, Validation Accuracy: 0.23416385054588318\n",
      "Epoch 196:\n",
      "  Training Loss: 0.7701042890548706, Training Accuracy: 0.7150548696517944\n",
      "  Validation Loss: 0.5307269096374512, Validation Accuracy: 0.7704814076423645\n",
      "Epoch 197:\n",
      "  Training Loss: 0.7535378932952881, Training Accuracy: 0.7158467173576355\n",
      "  Validation Loss: 0.5496416091918945, Validation Accuracy: 0.7742820978164673\n",
      "Epoch 198:\n",
      "  Training Loss: 0.7171446084976196, Training Accuracy: 0.7235536575317383\n",
      "  Validation Loss: 0.5337237119674683, Validation Accuracy: 0.7721706032752991\n",
      "Epoch 199:\n",
      "  Training Loss: 0.7349361181259155, Training Accuracy: 0.7212309837341309\n",
      "  Validation Loss: 0.5433714389801025, Validation Accuracy: 0.7740709185600281\n",
      "Epoch 200:\n",
      "  Training Loss: 0.7320212721824646, Training Accuracy: 0.7234480381011963\n",
      "  Validation Loss: 0.6633925437927246, Validation Accuracy: 0.5943834185600281\n",
      "Epoch 201:\n",
      "  Training Loss: 0.6896254420280457, Training Accuracy: 0.728515625\n",
      "  Validation Loss: 0.5835691690444946, Validation Accuracy: 0.7774493098258972\n",
      "Epoch 202:\n",
      "  Training Loss: 0.7291569113731384, Training Accuracy: 0.7224978804588318\n",
      "  Validation Loss: 0.6559231281280518, Validation Accuracy: 0.6138091087341309\n",
      "Epoch 203:\n",
      "  Training Loss: 0.7165868282318115, Training Accuracy: 0.7274599075317383\n",
      "  Validation Loss: 0.5395478010177612, Validation Accuracy: 0.7635135054588318\n",
      "Epoch 204:\n",
      "  Training Loss: 0.7489129304885864, Training Accuracy: 0.7182748913764954\n",
      "  Validation Loss: 0.5745111703872681, Validation Accuracy: 0.7776604890823364\n",
      "Epoch 205:\n",
      "  Training Loss: 0.7458792328834534, Training Accuracy: 0.7192251086235046\n",
      "  Validation Loss: 0.5308272242546082, Validation Accuracy: 0.7698479890823364\n",
      "Epoch 206:\n",
      "  Training Loss: 0.7449342608451843, Training Accuracy: 0.7222867608070374\n",
      "  Validation Loss: 0.7160344123840332, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 207:\n",
      "  Training Loss: 0.7687109708786011, Training Accuracy: 0.7203863859176636\n",
      "  Validation Loss: 0.5595254898071289, Validation Accuracy: 0.7546452879905701\n",
      "Epoch 208:\n",
      "  Training Loss: 0.7131338119506836, Training Accuracy: 0.7252427935600281\n",
      "  Validation Loss: 0.9585750699043274, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 209:\n",
      "  Training Loss: 0.7119395136833191, Training Accuracy: 0.7268264293670654\n",
      "  Validation Loss: 0.532625675201416, Validation Accuracy: 0.7736486196517944\n",
      "Epoch 210:\n",
      "  Training Loss: 0.7289419174194336, Training Accuracy: 0.7213893532752991\n",
      "  Validation Loss: 0.6057453751564026, Validation Accuracy: 0.7267736196517944\n",
      "Epoch 211:\n",
      "  Training Loss: 0.7144075632095337, Training Accuracy: 0.7260873913764954\n",
      "  Validation Loss: 0.6718272566795349, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 212:\n",
      "  Training Loss: 0.7366601824760437, Training Accuracy: 0.7198057174682617\n",
      "  Validation Loss: 0.7601052522659302, Validation Accuracy: 0.3937922418117523\n",
      "Epoch 213:\n",
      "  Training Loss: 0.7362874746322632, Training Accuracy: 0.72265625\n",
      "  Validation Loss: 0.7391611337661743, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 214:\n",
      "  Training Loss: 0.7429481744766235, Training Accuracy: 0.720755934715271\n",
      "  Validation Loss: 0.5321817398071289, Validation Accuracy: 0.7690033912658691\n",
      "Epoch 215:\n",
      "  Training Loss: 0.6855213046073914, Training Accuracy: 0.7273542881011963\n",
      "  Validation Loss: 0.5650789141654968, Validation Accuracy: 0.7563344836235046\n",
      "Epoch 216:\n",
      "  Training Loss: 0.712912917137146, Training Accuracy: 0.7241342663764954\n",
      "  Validation Loss: 0.6179271340370178, Validation Accuracy: 0.7801942825317383\n",
      "Epoch 217:\n",
      "  Training Loss: 0.7508026361465454, Training Accuracy: 0.7194362282752991\n",
      "  Validation Loss: 0.6118299961090088, Validation Accuracy: 0.7231841087341309\n",
      "Epoch 218:\n",
      "  Training Loss: 0.7405287623405457, Training Accuracy: 0.7191722989082336\n",
      "  Validation Loss: 0.93971848487854, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 219:\n",
      "  Training Loss: 0.7272493243217468, Training Accuracy: 0.7237647771835327\n",
      "  Validation Loss: 0.5300280451774597, Validation Accuracy: 0.7704814076423645\n",
      "Epoch 220:\n",
      "  Training Loss: 0.708731472492218, Training Accuracy: 0.7286739945411682\n",
      "  Validation Loss: 0.6312419772148132, Validation Accuracy: 0.7808277010917664\n",
      "Epoch 221:\n",
      "  Training Loss: 0.735564649105072, Training Accuracy: 0.7248204946517944\n",
      "  Validation Loss: 0.531410813331604, Validation Accuracy: 0.7751266956329346\n",
      "Epoch 222:\n",
      "  Training Loss: 0.686542272567749, Training Accuracy: 0.7297297120094299\n",
      "  Validation Loss: 0.6248875260353088, Validation Accuracy: 0.7806165814399719\n",
      "Epoch 223:\n",
      "  Training Loss: 0.7316112518310547, Training Accuracy: 0.7176942825317383\n",
      "  Validation Loss: 0.5651093125343323, Validation Accuracy: 0.7789273858070374\n",
      "Epoch 224:\n",
      "  Training Loss: 0.7156704068183899, Training Accuracy: 0.7235536575317383\n",
      "  Validation Loss: 0.8847065567970276, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 225:\n",
      "  Training Loss: 0.7173883318901062, Training Accuracy: 0.7229201793670654\n",
      "  Validation Loss: 0.5881931185722351, Validation Accuracy: 0.7445101141929626\n",
      "Epoch 226:\n",
      "  Training Loss: 0.7031366229057312, Training Accuracy: 0.7267208695411682\n",
      "  Validation Loss: 0.9773117303848267, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 227:\n",
      "  Training Loss: 0.729762852191925, Training Accuracy: 0.7223395109176636\n",
      "  Validation Loss: 1.0774425268173218, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 228:\n",
      "  Training Loss: 0.7068758010864258, Training Accuracy: 0.7237119674682617\n",
      "  Validation Loss: 0.5682357549667358, Validation Accuracy: 0.7789273858070374\n",
      "Epoch 229:\n",
      "  Training Loss: 0.6973311901092529, Training Accuracy: 0.7279349565505981\n",
      "  Validation Loss: 0.5705624222755432, Validation Accuracy: 0.7789273858070374\n",
      "Epoch 230:\n",
      "  Training Loss: 0.7453738451004028, Training Accuracy: 0.7231313586235046\n",
      "  Validation Loss: 0.5297835469245911, Validation Accuracy: 0.7757601141929626\n",
      "Epoch 231:\n",
      "  Training Loss: 0.6933395266532898, Training Accuracy: 0.7330025434494019\n",
      "  Validation Loss: 0.68366938829422, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 232:\n",
      "  Training Loss: 0.6738092303276062, Training Accuracy: 0.7341110706329346\n",
      "  Validation Loss: 0.5336657166481018, Validation Accuracy: 0.7761824131011963\n",
      "Epoch 233:\n",
      "  Training Loss: 0.7180401682853699, Training Accuracy: 0.7249261140823364\n",
      "  Validation Loss: 0.5281099677085876, Validation Accuracy: 0.7734375\n",
      "Epoch 234:\n",
      "  Training Loss: 0.6970375180244446, Training Accuracy: 0.7258234620094299\n",
      "  Validation Loss: 0.5308827757835388, Validation Accuracy: 0.7761824131011963\n",
      "Epoch 235:\n",
      "  Training Loss: 0.7249820232391357, Training Accuracy: 0.72265625\n",
      "  Validation Loss: 0.5651794075965881, Validation Accuracy: 0.7588682174682617\n",
      "Epoch 236:\n",
      "  Training Loss: 0.6901785135269165, Training Accuracy: 0.7322107553482056\n",
      "  Validation Loss: 0.5421180129051208, Validation Accuracy: 0.7787162065505981\n",
      "Epoch 237:\n",
      "  Training Loss: 0.7272002100944519, Training Accuracy: 0.7255067825317383\n",
      "  Validation Loss: 0.5636453628540039, Validation Accuracy: 0.7595016956329346\n",
      "Epoch 238:\n",
      "  Training Loss: 0.7159065008163452, Training Accuracy: 0.7241342663764954\n",
      "  Validation Loss: 0.9499750137329102, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 239:\n",
      "  Training Loss: 0.6970304250717163, Training Accuracy: 0.7315772771835327\n",
      "  Validation Loss: 0.5581116080284119, Validation Accuracy: 0.7791385054588318\n",
      "Epoch 240:\n",
      "  Training Loss: 0.7191552519798279, Training Accuracy: 0.7260873913764954\n",
      "  Validation Loss: 0.6024805903434753, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 241:\n",
      "  Training Loss: 0.700153112411499, Training Accuracy: 0.7294130325317383\n",
      "  Validation Loss: 0.5536103844642639, Validation Accuracy: 0.779349684715271\n",
      "Epoch 242:\n",
      "  Training Loss: 0.7024968862533569, Training Accuracy: 0.7269319891929626\n",
      "  Validation Loss: 0.6069457530975342, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 243:\n",
      "  Training Loss: 0.7176424264907837, Training Accuracy: 0.7240287065505981\n",
      "  Validation Loss: 0.7693950533866882, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 244:\n",
      "  Training Loss: 0.7271999716758728, Training Accuracy: 0.7249788641929626\n",
      "  Validation Loss: 0.6354565620422363, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 245:\n",
      "  Training Loss: 0.7007336616516113, Training Accuracy: 0.7287267446517944\n",
      "  Validation Loss: 0.5450081825256348, Validation Accuracy: 0.7647804021835327\n",
      "Epoch 246:\n",
      "  Training Loss: 0.7025265693664551, Training Accuracy: 0.7284100651741028\n",
      "  Validation Loss: 0.5341480374336243, Validation Accuracy: 0.7776604890823364\n",
      "Epoch 247:\n",
      "  Training Loss: 0.6928280591964722, Training Accuracy: 0.7296769618988037\n",
      "  Validation Loss: 0.5731716156005859, Validation Accuracy: 0.7801942825317383\n",
      "Epoch 248:\n",
      "  Training Loss: 0.7209547758102417, Training Accuracy: 0.7295713424682617\n",
      "  Validation Loss: 0.6115061044692993, Validation Accuracy: 0.7314189076423645\n",
      "Epoch 249:\n",
      "  Training Loss: 0.7075938582420349, Training Accuracy: 0.7287795543670654\n",
      "  Validation Loss: 0.5951873064041138, Validation Accuracy: 0.7810388803482056\n",
      "Epoch 250:\n",
      "  Training Loss: 0.7060620784759521, Training Accuracy: 0.7257707118988037\n",
      "  Validation Loss: 0.7866191267967224, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 251:\n",
      "  Training Loss: 0.7087470889091492, Training Accuracy: 0.7283045053482056\n",
      "  Validation Loss: 0.7902005910873413, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 252:\n",
      "  Training Loss: 0.704524576663971, Training Accuracy: 0.7299408912658691\n",
      "  Validation Loss: 0.6219257712364197, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 253:\n",
      "  Training Loss: 0.7123166918754578, Training Accuracy: 0.7260346412658691\n",
      "  Validation Loss: 0.5606546401977539, Validation Accuracy: 0.7804054021835327\n",
      "Epoch 254:\n",
      "  Training Loss: 0.6913475394248962, Training Accuracy: 0.7268792390823364\n",
      "  Validation Loss: 0.5384605526924133, Validation Accuracy: 0.7791385054588318\n",
      "Epoch 255:\n",
      "  Training Loss: 0.7156233787536621, Training Accuracy: 0.7241870760917664\n",
      "  Validation Loss: 0.5503826141357422, Validation Accuracy: 0.7797719836235046\n",
      "Epoch 256:\n",
      "  Training Loss: 0.6999914050102234, Training Accuracy: 0.7266680598258972\n",
      "  Validation Loss: 0.5334821939468384, Validation Accuracy: 0.7791385054588318\n",
      "Epoch 257:\n",
      "  Training Loss: 0.7097238898277283, Training Accuracy: 0.728568434715271\n",
      "  Validation Loss: 0.6254830360412598, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 258:\n",
      "  Training Loss: 0.676042377948761, Training Accuracy: 0.7321051359176636\n",
      "  Validation Loss: 0.6399935483932495, Validation Accuracy: 0.6769425868988037\n",
      "Epoch 259:\n",
      "  Training Loss: 0.690708339214325, Training Accuracy: 0.7278293967247009\n",
      "  Validation Loss: 0.5598611235618591, Validation Accuracy: 0.7804054021835327\n",
      "Epoch 260:\n",
      "  Training Loss: 0.6946889758110046, Training Accuracy: 0.7245038151741028\n",
      "  Validation Loss: 0.8646436333656311, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 261:\n",
      "  Training Loss: 0.710778534412384, Training Accuracy: 0.7257179021835327\n",
      "  Validation Loss: 0.5270091891288757, Validation Accuracy: 0.7740709185600281\n",
      "Epoch 262:\n",
      "  Training Loss: 0.701799750328064, Training Accuracy: 0.7292018532752991\n",
      "  Validation Loss: 0.5484778881072998, Validation Accuracy: 0.7664695978164673\n",
      "Epoch 263:\n",
      "  Training Loss: 0.6662082672119141, Training Accuracy: 0.7340055108070374\n",
      "  Validation Loss: 0.5824499130249023, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 264:\n",
      "  Training Loss: 0.6729278564453125, Training Accuracy: 0.7327913641929626\n",
      "  Validation Loss: 0.5423315167427063, Validation Accuracy: 0.7700591087341309\n",
      "Epoch 265:\n",
      "  Training Loss: 0.6783700585365295, Training Accuracy: 0.7337943315505981\n",
      "  Validation Loss: 0.6460311412811279, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 266:\n",
      "  Training Loss: 0.6858144998550415, Training Accuracy: 0.7298353314399719\n",
      "  Validation Loss: 0.5265055894851685, Validation Accuracy: 0.7785050868988037\n",
      "Epoch 267:\n",
      "  Training Loss: 0.7073826193809509, Training Accuracy: 0.7286739945411682\n",
      "  Validation Loss: 0.5689538717269897, Validation Accuracy: 0.7592905163764954\n",
      "Epoch 268:\n",
      "  Training Loss: 0.695229709148407, Training Accuracy: 0.7330552935600281\n",
      "  Validation Loss: 0.584516704082489, Validation Accuracy: 0.7569679021835327\n",
      "Epoch 269:\n",
      "  Training Loss: 0.6854826211929321, Training Accuracy: 0.7316828370094299\n",
      "  Validation Loss: 1.3931249380111694, Validation Accuracy: 0.2540118098258972\n",
      "Epoch 270:\n",
      "  Training Loss: 0.6856333017349243, Training Accuracy: 0.7338471412658691\n",
      "  Validation Loss: 0.552833616733551, Validation Accuracy: 0.7673141956329346\n",
      "Epoch 271:\n",
      "  Training Loss: 0.7155961990356445, Training Accuracy: 0.7264041304588318\n",
      "  Validation Loss: 0.534243643283844, Validation Accuracy: 0.7801942825317383\n",
      "Epoch 272:\n",
      "  Training Loss: 0.6977652907371521, Training Accuracy: 0.7292018532752991\n",
      "  Validation Loss: 0.6264790296554565, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 273:\n",
      "  Training Loss: 0.6955942511558533, Training Accuracy: 0.7305743098258972\n",
      "  Validation Loss: 0.5701805353164673, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 274:\n",
      "  Training Loss: 0.6693853735923767, Training Accuracy: 0.7312605381011963\n",
      "  Validation Loss: 0.7873505353927612, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 275:\n",
      "  Training Loss: 0.6724106073379517, Training Accuracy: 0.7299936413764954\n",
      "  Validation Loss: 0.5450217127799988, Validation Accuracy: 0.78125\n",
      "Epoch 276:\n",
      "  Training Loss: 0.6993284821510315, Training Accuracy: 0.7300464510917664\n",
      "  Validation Loss: 0.616476833820343, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 277:\n",
      "  Training Loss: 0.689107358455658, Training Accuracy: 0.7334775924682617\n",
      "  Validation Loss: 0.5823946595191956, Validation Accuracy: 0.7582347989082336\n",
      "Epoch 278:\n",
      "  Training Loss: 0.692521870136261, Training Accuracy: 0.7321051359176636\n",
      "  Validation Loss: 0.5346543192863464, Validation Accuracy: 0.7732263803482056\n",
      "Epoch 279:\n",
      "  Training Loss: 0.6712465286254883, Training Accuracy: 0.7300464510917664\n",
      "  Validation Loss: 0.6187789440155029, Validation Accuracy: 0.7333192825317383\n",
      "Epoch 280:\n",
      "  Training Loss: 0.684023916721344, Training Accuracy: 0.7344805598258972\n",
      "  Validation Loss: 0.5900982618331909, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 281:\n",
      "  Training Loss: 0.6816462874412537, Training Accuracy: 0.7313133478164673\n",
      "  Validation Loss: 0.6173548102378845, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 282:\n",
      "  Training Loss: 0.674237847328186, Training Accuracy: 0.7322635054588318\n",
      "  Validation Loss: 0.7821598052978516, Validation Accuracy: 0.3730996549129486\n",
      "Epoch 283:\n",
      "  Training Loss: 0.6690593957901001, Training Accuracy: 0.7356947064399719\n",
      "  Validation Loss: 0.5568581223487854, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 284:\n",
      "  Training Loss: 0.7075974345207214, Training Accuracy: 0.7313133478164673\n",
      "  Validation Loss: 0.541531503200531, Validation Accuracy: 0.7728040814399719\n",
      "Epoch 285:\n",
      "  Training Loss: 0.6803373098373413, Training Accuracy: 0.7338998913764954\n",
      "  Validation Loss: 0.7557671070098877, Validation Accuracy: 0.4035050570964813\n",
      "Epoch 286:\n",
      "  Training Loss: 0.6814042925834656, Training Accuracy: 0.7362225651741028\n",
      "  Validation Loss: 0.675581157207489, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 287:\n",
      "  Training Loss: 0.6945128440856934, Training Accuracy: 0.732421875\n",
      "  Validation Loss: 0.8163711428642273, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 288:\n",
      "  Training Loss: 0.663417637348175, Training Accuracy: 0.7365392446517944\n",
      "  Validation Loss: 0.5451231598854065, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 289:\n",
      "  Training Loss: 0.6816875338554382, Training Accuracy: 0.7358530163764954\n",
      "  Validation Loss: 0.6860990524291992, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 290:\n",
      "  Training Loss: 0.6559709310531616, Training Accuracy: 0.7349556684494019\n",
      "  Validation Loss: 0.5574274063110352, Validation Accuracy: 0.767525315284729\n",
      "Epoch 291:\n",
      "  Training Loss: 0.6919755339622498, Training Accuracy: 0.7357474565505981\n",
      "  Validation Loss: 0.6600742340087891, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 292:\n",
      "  Training Loss: 0.6734310388565063, Training Accuracy: 0.7333720326423645\n",
      "  Validation Loss: 1.107035517692566, Validation Accuracy: 0.27702704071998596\n",
      "Epoch 293:\n",
      "  Training Loss: 0.6746311187744141, Training Accuracy: 0.7303103804588318\n",
      "  Validation Loss: 0.6197338104248047, Validation Accuracy: 0.7385979890823364\n",
      "Epoch 294:\n",
      "  Training Loss: 0.6816872358322144, Training Accuracy: 0.7299936413764954\n",
      "  Validation Loss: 0.5956254601478577, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 295:\n",
      "  Training Loss: 0.6773210763931274, Training Accuracy: 0.7368032336235046\n",
      "  Validation Loss: 0.5240582823753357, Validation Accuracy: 0.7808277010917664\n",
      "Epoch 296:\n",
      "  Training Loss: 0.6577607989311218, Training Accuracy: 0.7351667881011963\n",
      "  Validation Loss: 0.534925103187561, Validation Accuracy: 0.7770270109176636\n",
      "Epoch 297:\n",
      "  Training Loss: 0.6645846962928772, Training Accuracy: 0.734322190284729\n",
      "  Validation Loss: 0.578375518321991, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 298:\n",
      "  Training Loss: 0.6774747967720032, Training Accuracy: 0.7334775924682617\n",
      "  Validation Loss: 0.5966378450393677, Validation Accuracy: 0.7569679021835327\n",
      "Epoch 299:\n",
      "  Training Loss: 0.6858440637588501, Training Accuracy: 0.7314717173576355\n",
      "  Validation Loss: 0.5257479548454285, Validation Accuracy: 0.779349684715271\n",
      "Epoch 300:\n",
      "  Training Loss: 0.6601800918579102, Training Accuracy: 0.7377533912658691\n",
      "  Validation Loss: 0.5233796238899231, Validation Accuracy: 0.7808277010917664\n",
      "Epoch 301:\n",
      "  Training Loss: 0.6887878179550171, Training Accuracy: 0.7309966087341309\n",
      "  Validation Loss: 0.6566851735115051, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 302:\n",
      "  Training Loss: 0.6530051231384277, Training Accuracy: 0.7400232553482056\n",
      "  Validation Loss: 0.9465550780296326, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 303:\n",
      "  Training Loss: 0.6821947693824768, Training Accuracy: 0.7334775924682617\n",
      "  Validation Loss: 0.5235725045204163, Validation Accuracy: 0.78125\n",
      "Epoch 304:\n",
      "  Training Loss: 0.6610517501831055, Training Accuracy: 0.7370671629905701\n",
      "  Validation Loss: 0.5832380056381226, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 305:\n",
      "  Training Loss: 0.6734198927879333, Training Accuracy: 0.7397065162658691\n",
      "  Validation Loss: 0.6036555767059326, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 306:\n",
      "  Training Loss: 0.6819204092025757, Training Accuracy: 0.7314189076423645\n",
      "  Validation Loss: 0.5248898267745972, Validation Accuracy: 0.7801942825317383\n",
      "Epoch 307:\n",
      "  Training Loss: 0.6629125475883484, Training Accuracy: 0.7389146685600281\n",
      "  Validation Loss: 0.5948259234428406, Validation Accuracy: 0.7588682174682617\n",
      "Epoch 308:\n",
      "  Training Loss: 0.6986718773841858, Training Accuracy: 0.7267208695411682\n",
      "  Validation Loss: 0.5436879396438599, Validation Accuracy: 0.7768158912658691\n",
      "Epoch 309:\n",
      "  Training Loss: 0.6579844355583191, Training Accuracy: 0.7377533912658691\n",
      "  Validation Loss: 0.5637373328208923, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 310:\n",
      "  Training Loss: 0.6930841207504272, Training Accuracy: 0.7328441739082336\n",
      "  Validation Loss: 0.6849655508995056, Validation Accuracy: 0.5280827879905701\n",
      "Epoch 311:\n",
      "  Training Loss: 0.6631001234054565, Training Accuracy: 0.7401288151741028\n",
      "  Validation Loss: 0.7946807146072388, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 312:\n",
      "  Training Loss: 0.6699008941650391, Training Accuracy: 0.7365392446517944\n",
      "  Validation Loss: 0.5408825874328613, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 313:\n",
      "  Training Loss: 0.6385206580162048, Training Accuracy: 0.7394425868988037\n",
      "  Validation Loss: 0.5243735313415527, Validation Accuracy: 0.7808277010917664\n",
      "Epoch 314:\n",
      "  Training Loss: 0.6602798104286194, Training Accuracy: 0.7395481467247009\n",
      "  Validation Loss: 0.6919848322868347, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 315:\n",
      "  Training Loss: 0.6847856640815735, Training Accuracy: 0.7337943315505981\n",
      "  Validation Loss: 0.5524448752403259, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 316:\n",
      "  Training Loss: 0.6623216867446899, Training Accuracy: 0.7317884564399719\n",
      "  Validation Loss: 0.6261468529701233, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 317:\n",
      "  Training Loss: 0.6603803634643555, Training Accuracy: 0.7434543967247009\n",
      "  Validation Loss: 0.5577975511550903, Validation Accuracy: 0.7749155163764954\n",
      "Epoch 318:\n",
      "  Training Loss: 0.6696419715881348, Training Accuracy: 0.7381756901741028\n",
      "  Validation Loss: 0.5669326186180115, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 319:\n",
      "  Training Loss: 0.6458327174186707, Training Accuracy: 0.742134690284729\n",
      "  Validation Loss: 0.523054838180542, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 320:\n",
      "  Training Loss: 0.6725558042526245, Training Accuracy: 0.7360641956329346\n",
      "  Validation Loss: 0.5292968153953552, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 321:\n",
      "  Training Loss: 0.6607980728149414, Training Accuracy: 0.7407094836235046\n",
      "  Validation Loss: 0.5232762098312378, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 322:\n",
      "  Training Loss: 0.6580269932746887, Training Accuracy: 0.7400760054588318\n",
      "  Validation Loss: 0.5721593499183655, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 323:\n",
      "  Training Loss: 0.6847367882728577, Training Accuracy: 0.7368559837341309\n",
      "  Validation Loss: 1.1653329133987427, Validation Accuracy: 0.2734375\n",
      "Epoch 324:\n",
      "  Training Loss: 0.6697667837142944, Training Accuracy: 0.7327386140823364\n",
      "  Validation Loss: 0.5718416571617126, Validation Accuracy: 0.7728040814399719\n",
      "Epoch 325:\n",
      "  Training Loss: 0.6573747396469116, Training Accuracy: 0.7349028587341309\n",
      "  Validation Loss: 0.532105028629303, Validation Accuracy: 0.7799831032752991\n",
      "Epoch 326:\n",
      "  Training Loss: 0.6637610793113708, Training Accuracy: 0.738228440284729\n",
      "  Validation Loss: 0.6887245774269104, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 327:\n",
      "  Training Loss: 0.6555821299552917, Training Accuracy: 0.7365392446517944\n",
      "  Validation Loss: 0.638783872127533, Validation Accuracy: 0.6938344836235046\n",
      "Epoch 328:\n",
      "  Training Loss: 0.6451436877250671, Training Accuracy: 0.7414484620094299\n",
      "  Validation Loss: 0.5709658861160278, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 329:\n",
      "  Training Loss: 0.6812397837638855, Training Accuracy: 0.7336887717247009\n",
      "  Validation Loss: 0.5383106470108032, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 330:\n",
      "  Training Loss: 0.6669443845748901, Training Accuracy: 0.7369087934494019\n",
      "  Validation Loss: 0.5317184329032898, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 331:\n",
      "  Training Loss: 0.6471166610717773, Training Accuracy: 0.7396009564399719\n",
      "  Validation Loss: 0.5267320275306702, Validation Accuracy: 0.7806165814399719\n",
      "Epoch 332:\n",
      "  Training Loss: 0.6610812544822693, Training Accuracy: 0.7353251576423645\n",
      "  Validation Loss: 0.5962903499603271, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 333:\n",
      "  Training Loss: 0.66562420129776, Training Accuracy: 0.7364336848258972\n",
      "  Validation Loss: 0.5220925211906433, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 334:\n",
      "  Training Loss: 0.6280145645141602, Training Accuracy: 0.7446684837341309\n",
      "  Validation Loss: 0.5217623114585876, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 335:\n",
      "  Training Loss: 0.6678817272186279, Training Accuracy: 0.7353251576423645\n",
      "  Validation Loss: 0.5313709378242493, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 336:\n",
      "  Training Loss: 0.6493030786514282, Training Accuracy: 0.7399704456329346\n",
      "  Validation Loss: 0.5260976552963257, Validation Accuracy: 0.7810388803482056\n",
      "Epoch 337:\n",
      "  Training Loss: 0.6718576550483704, Training Accuracy: 0.7345333695411682\n",
      "  Validation Loss: 0.5635678172111511, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 338:\n",
      "  Training Loss: 0.6515405178070068, Training Accuracy: 0.7428737282752991\n",
      "  Validation Loss: 0.5459460020065308, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 339:\n",
      "  Training Loss: 0.6662790179252625, Training Accuracy: 0.734322190284729\n",
      "  Validation Loss: 0.5437463521957397, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 340:\n",
      "  Training Loss: 0.6571027040481567, Training Accuracy: 0.7375422120094299\n",
      "  Validation Loss: 0.568675696849823, Validation Accuracy: 0.7759712934494019\n",
      "Epoch 341:\n",
      "  Training Loss: 0.6664966940879822, Training Accuracy: 0.734375\n",
      "  Validation Loss: 0.5215816497802734, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 342:\n",
      "  Training Loss: 0.6578065156936646, Training Accuracy: 0.7366448640823364\n",
      "  Validation Loss: 0.5480942130088806, Validation Accuracy: 0.7780827879905701\n",
      "Epoch 343:\n",
      "  Training Loss: 0.6611341238021851, Training Accuracy: 0.740181565284729\n",
      "  Validation Loss: 0.5214070081710815, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 344:\n",
      "  Training Loss: 0.6503656506538391, Training Accuracy: 0.7387563586235046\n",
      "  Validation Loss: 0.533486008644104, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 345:\n",
      "  Training Loss: 0.6499961018562317, Training Accuracy: 0.7416068315505981\n",
      "  Validation Loss: 0.6197284460067749, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 346:\n",
      "  Training Loss: 0.6463905572891235, Training Accuracy: 0.7418707609176636\n",
      "  Validation Loss: 0.7395502328872681, Validation Accuracy: 0.43517735600471497\n",
      "Epoch 347:\n",
      "  Training Loss: 0.6684992909431458, Training Accuracy: 0.7354307174682617\n",
      "  Validation Loss: 0.5212235450744629, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 348:\n",
      "  Training Loss: 0.6419317126274109, Training Accuracy: 0.7466216087341309\n",
      "  Validation Loss: 0.7549405694007874, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 349:\n",
      "  Training Loss: 0.666842520236969, Training Accuracy: 0.7443517446517944\n",
      "  Validation Loss: 0.5311555862426758, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 350:\n",
      "  Training Loss: 0.6580939888954163, Training Accuracy: 0.7364864945411682\n",
      "  Validation Loss: 0.5211947560310364, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 351:\n",
      "  Training Loss: 0.6640617251396179, Training Accuracy: 0.7404983043670654\n",
      "  Validation Loss: 0.5240559577941895, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 352:\n",
      "  Training Loss: 0.6697012782096863, Training Accuracy: 0.7378061413764954\n",
      "  Validation Loss: 0.6170381903648376, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 353:\n",
      "  Training Loss: 0.6548178195953369, Training Accuracy: 0.7418707609176636\n",
      "  Validation Loss: 0.5276885032653809, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 354:\n",
      "  Training Loss: 0.6631205081939697, Training Accuracy: 0.7370143532752991\n",
      "  Validation Loss: 1.1343234777450562, Validation Accuracy: 0.2791385054588318\n",
      "Epoch 355:\n",
      "  Training Loss: 0.6575044989585876, Training Accuracy: 0.7375950217247009\n",
      "  Validation Loss: 0.6834124326705933, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 356:\n",
      "  Training Loss: 0.6371798515319824, Training Accuracy: 0.7450907826423645\n",
      "  Validation Loss: 0.5577636957168579, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 357:\n",
      "  Training Loss: 0.6548995971679688, Training Accuracy: 0.7419235706329346\n",
      "  Validation Loss: 0.5364713072776794, Validation Accuracy: 0.7808277010917664\n",
      "Epoch 358:\n",
      "  Training Loss: 0.6558049917221069, Training Accuracy: 0.7410789728164673\n",
      "  Validation Loss: 1.5435981750488281, Validation Accuracy: 0.2521114945411682\n",
      "Epoch 359:\n",
      "  Training Loss: 0.6643320918083191, Training Accuracy: 0.7412373423576355\n",
      "  Validation Loss: 0.5215365886688232, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 360:\n",
      "  Training Loss: 0.6240314245223999, Training Accuracy: 0.7466216087341309\n",
      "  Validation Loss: 0.6361579298973083, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 361:\n",
      "  Training Loss: 0.6540893316268921, Training Accuracy: 0.7407094836235046\n",
      "  Validation Loss: 0.7162656784057617, Validation Accuracy: 0.47233954071998596\n",
      "Epoch 362:\n",
      "  Training Loss: 0.6592047214508057, Training Accuracy: 0.7393897771835327\n",
      "  Validation Loss: 0.520799458026886, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 363:\n",
      "  Training Loss: 0.6529663801193237, Training Accuracy: 0.7453547120094299\n",
      "  Validation Loss: 0.5239459276199341, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 364:\n",
      "  Training Loss: 0.6364057660102844, Training Accuracy: 0.7456714510917664\n",
      "  Validation Loss: 0.5501535534858704, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 365:\n",
      "  Training Loss: 0.6607837677001953, Training Accuracy: 0.738334059715271\n",
      "  Validation Loss: 0.6104961037635803, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 366:\n",
      "  Training Loss: 0.6472097039222717, Training Accuracy: 0.7432960271835327\n",
      "  Validation Loss: 0.6635608673095703, Validation Accuracy: 0.6038851141929626\n",
      "Epoch 367:\n",
      "  Training Loss: 0.6468154191970825, Training Accuracy: 0.736380934715271\n",
      "  Validation Loss: 0.52399080991745, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 368:\n",
      "  Training Loss: 0.652979850769043, Training Accuracy: 0.746040940284729\n",
      "  Validation Loss: 0.5291429162025452, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 369:\n",
      "  Training Loss: 0.6605387330055237, Training Accuracy: 0.7389146685600281\n",
      "  Validation Loss: 0.7616167068481445, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 370:\n",
      "  Training Loss: 0.6291800141334534, Training Accuracy: 0.7452491521835327\n",
      "  Validation Loss: 1.3050075769424438, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 371:\n",
      "  Training Loss: 0.6388853192329407, Training Accuracy: 0.7444573640823364\n",
      "  Validation Loss: 0.522331178188324, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 372:\n",
      "  Training Loss: 0.6436700820922852, Training Accuracy: 0.7400232553482056\n",
      "  Validation Loss: 0.5394033789634705, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 373:\n",
      "  Training Loss: 0.6564821004867554, Training Accuracy: 0.7391258478164673\n",
      "  Validation Loss: 0.5837228298187256, Validation Accuracy: 0.7747043967247009\n",
      "Epoch 374:\n",
      "  Training Loss: 0.6497795581817627, Training Accuracy: 0.7409734129905701\n",
      "  Validation Loss: 0.8885561227798462, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 375:\n",
      "  Training Loss: 0.6266580820083618, Training Accuracy: 0.7451963424682617\n",
      "  Validation Loss: 0.5845075845718384, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 376:\n",
      "  Training Loss: 0.6452179551124573, Training Accuracy: 0.7449852228164673\n",
      "  Validation Loss: 1.1411263942718506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 377:\n",
      "  Training Loss: 0.6503579020500183, Training Accuracy: 0.7428209185600281\n",
      "  Validation Loss: 0.6377365589141846, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 378:\n",
      "  Training Loss: 0.648099422454834, Training Accuracy: 0.7417652010917664\n",
      "  Validation Loss: 0.5268341898918152, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 379:\n",
      "  Training Loss: 0.6419835090637207, Training Accuracy: 0.7446157336235046\n",
      "  Validation Loss: 1.1005579233169556, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 380:\n",
      "  Training Loss: 0.6369736790657043, Training Accuracy: 0.746146559715271\n",
      "  Validation Loss: 0.6546642184257507, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 381:\n",
      "  Training Loss: 0.6446086168289185, Training Accuracy: 0.7420291304588318\n",
      "  Validation Loss: 0.9370759725570679, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 382:\n",
      "  Training Loss: 0.6491048336029053, Training Accuracy: 0.7398120760917664\n",
      "  Validation Loss: 0.6624342203140259, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 383:\n",
      "  Training Loss: 0.6358712315559387, Training Accuracy: 0.744140625\n",
      "  Validation Loss: 0.5202905535697937, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 384:\n",
      "  Training Loss: 0.6469435095787048, Training Accuracy: 0.7419235706329346\n",
      "  Validation Loss: 0.6067651510238647, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 385:\n",
      "  Training Loss: 0.6456068158149719, Training Accuracy: 0.7390730381011963\n",
      "  Validation Loss: 0.6706337332725525, Validation Accuracy: 0.5859375\n",
      "Epoch 386:\n",
      "  Training Loss: 0.6439733505249023, Training Accuracy: 0.742134690284729\n",
      "  Validation Loss: 0.7766912579536438, Validation Accuracy: 0.3916807472705841\n",
      "Epoch 387:\n",
      "  Training Loss: 0.6526584029197693, Training Accuracy: 0.7447212934494019\n",
      "  Validation Loss: 0.5204305648803711, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 388:\n",
      "  Training Loss: 0.6371296644210815, Training Accuracy: 0.7456714510917664\n",
      "  Validation Loss: 0.5257440805435181, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 389:\n",
      "  Training Loss: 0.64814293384552, Training Accuracy: 0.7429265379905701\n",
      "  Validation Loss: 0.8901407122612, Validation Accuracy: 0.3308699429035187\n",
      "Epoch 390:\n",
      "  Training Loss: 0.6279255151748657, Training Accuracy: 0.7481524348258972\n",
      "  Validation Loss: 0.56280118227005, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 391:\n",
      "  Training Loss: 0.633105456829071, Training Accuracy: 0.7455130815505981\n",
      "  Validation Loss: 0.5381166934967041, Validation Accuracy: 0.7810388803482056\n",
      "Epoch 392:\n",
      "  Training Loss: 0.6306726336479187, Training Accuracy: 0.74609375\n",
      "  Validation Loss: 0.5212893486022949, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 393:\n",
      "  Training Loss: 0.6204529404640198, Training Accuracy: 0.7474662065505981\n",
      "  Validation Loss: 0.5238450765609741, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 394:\n",
      "  Training Loss: 0.6588875651359558, Training Accuracy: 0.7421875\n",
      "  Validation Loss: 0.5439027547836304, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 395:\n",
      "  Training Loss: 0.6387166976928711, Training Accuracy: 0.7407622337341309\n",
      "  Validation Loss: 0.5573244094848633, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 396:\n",
      "  Training Loss: 0.6367835998535156, Training Accuracy: 0.7432960271835327\n",
      "  Validation Loss: 0.5258265733718872, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 397:\n",
      "  Training Loss: 0.6408557295799255, Training Accuracy: 0.7432960271835327\n",
      "  Validation Loss: 0.5325918197631836, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 398:\n",
      "  Training Loss: 0.6369194984436035, Training Accuracy: 0.7444045543670654\n",
      "  Validation Loss: 0.5912405252456665, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 399:\n",
      "  Training Loss: 0.6450745463371277, Training Accuracy: 0.7431376576423645\n",
      "  Validation Loss: 0.5742942690849304, Validation Accuracy: 0.7778716087341309\n",
      "Epoch 400:\n",
      "  Training Loss: 0.631209671497345, Training Accuracy: 0.7469911575317383\n",
      "  Validation Loss: 0.5482946634292603, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 401:\n",
      "  Training Loss: 0.6308158040046692, Training Accuracy: 0.7496832609176636\n",
      "  Validation Loss: 0.587364673614502, Validation Accuracy: 0.7761824131011963\n",
      "Epoch 402:\n",
      "  Training Loss: 0.6279552578926086, Training Accuracy: 0.7431904673576355\n",
      "  Validation Loss: 0.5687881708145142, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 403:\n",
      "  Training Loss: 0.6574918031692505, Training Accuracy: 0.738228440284729\n",
      "  Validation Loss: 0.6182592511177063, Validation Accuracy: 0.7624577879905701\n",
      "Epoch 404:\n",
      "  Training Loss: 0.6410319805145264, Training Accuracy: 0.7411317825317383\n",
      "  Validation Loss: 0.5196381211280823, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 405:\n",
      "  Training Loss: 0.6306570172309875, Training Accuracy: 0.746146559715271\n",
      "  Validation Loss: 0.5912550091743469, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 406:\n",
      "  Training Loss: 0.6463978290557861, Training Accuracy: 0.7368032336235046\n",
      "  Validation Loss: 0.6030469536781311, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 407:\n",
      "  Training Loss: 0.6331705451011658, Training Accuracy: 0.746146559715271\n",
      "  Validation Loss: 0.590467631816864, Validation Accuracy: 0.7761824131011963\n",
      "Epoch 408:\n",
      "  Training Loss: 0.6192753911018372, Training Accuracy: 0.7504222989082336\n",
      "  Validation Loss: 0.5396069288253784, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 409:\n",
      "  Training Loss: 0.6275297999382019, Training Accuracy: 0.7474134564399719\n",
      "  Validation Loss: 0.6174207925796509, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 410:\n",
      "  Training Loss: 0.6258934736251831, Training Accuracy: 0.7484691739082336\n",
      "  Validation Loss: 0.6660899519920349, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 411:\n",
      "  Training Loss: 0.6332628726959229, Training Accuracy: 0.7461993098258972\n",
      "  Validation Loss: 0.6754792332649231, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 412:\n",
      "  Training Loss: 0.6229616403579712, Training Accuracy: 0.7490498423576355\n",
      "  Validation Loss: 0.6863338351249695, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 413:\n",
      "  Training Loss: 0.6426231861114502, Training Accuracy: 0.7409734129905701\n",
      "  Validation Loss: 0.6270216703414917, Validation Accuracy: 0.7455658912658691\n",
      "Epoch 414:\n",
      "  Training Loss: 0.6339502334594727, Training Accuracy: 0.746040940284729\n",
      "  Validation Loss: 0.5498634576797485, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 415:\n",
      "  Training Loss: 0.6259853839874268, Training Accuracy: 0.746040940284729\n",
      "  Validation Loss: 0.9035066366195679, Validation Accuracy: 0.33171454071998596\n",
      "Epoch 416:\n",
      "  Training Loss: 0.629554271697998, Training Accuracy: 0.7445101141929626\n",
      "  Validation Loss: 0.5752596259117126, Validation Accuracy: 0.7782939076423645\n",
      "Epoch 417:\n",
      "  Training Loss: 0.6500440835952759, Training Accuracy: 0.7447212934494019\n",
      "  Validation Loss: 0.5380862951278687, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 418:\n",
      "  Training Loss: 0.6316213607788086, Training Accuracy: 0.7478885054588318\n",
      "  Validation Loss: 0.5317386388778687, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 419:\n",
      "  Training Loss: 0.6286564469337463, Training Accuracy: 0.7445629239082336\n",
      "  Validation Loss: 0.5218433737754822, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 420:\n",
      "  Training Loss: 0.6411070823669434, Training Accuracy: 0.746040940284729\n",
      "  Validation Loss: 0.5196661949157715, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 421:\n",
      "  Training Loss: 0.6238844990730286, Training Accuracy: 0.7473078370094299\n",
      "  Validation Loss: 0.743475079536438, Validation Accuracy: 0.4425675570964813\n",
      "Epoch 422:\n",
      "  Training Loss: 0.6326920986175537, Training Accuracy: 0.7466216087341309\n",
      "  Validation Loss: 1.097537636756897, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 423:\n",
      "  Training Loss: 0.6493106484413147, Training Accuracy: 0.7439822554588318\n",
      "  Validation Loss: 1.050704836845398, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 424:\n",
      "  Training Loss: 0.6446592211723328, Training Accuracy: 0.7424514293670654\n",
      "  Validation Loss: 0.6737047433853149, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 425:\n",
      "  Training Loss: 0.63624107837677, Training Accuracy: 0.7471494674682617\n",
      "  Validation Loss: 0.5763987898826599, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 426:\n",
      "  Training Loss: 0.6311609148979187, Training Accuracy: 0.746146559715271\n",
      "  Validation Loss: 0.6172811985015869, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 427:\n",
      "  Training Loss: 0.6364479660987854, Training Accuracy: 0.7475190162658691\n",
      "  Validation Loss: 0.7266062498092651, Validation Accuracy: 0.4660050570964813\n",
      "Epoch 428:\n",
      "  Training Loss: 0.6315372586250305, Training Accuracy: 0.7427153587341309\n",
      "  Validation Loss: 0.5322715044021606, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 429:\n",
      "  Training Loss: 0.6373305916786194, Training Accuracy: 0.7439822554588318\n",
      "  Validation Loss: 0.6524905562400818, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 430:\n",
      "  Training Loss: 0.6242876052856445, Training Accuracy: 0.7501583695411682\n",
      "  Validation Loss: 0.9636383056640625, Validation Accuracy: 0.3171452581882477\n",
      "Epoch 431:\n",
      "  Training Loss: 0.6241331100463867, Training Accuracy: 0.7490498423576355\n",
      "  Validation Loss: 0.5206682682037354, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 432:\n",
      "  Training Loss: 0.6201393008232117, Training Accuracy: 0.7492082118988037\n",
      "  Validation Loss: 0.5413442254066467, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 433:\n",
      "  Training Loss: 0.6239280104637146, Training Accuracy: 0.7487859129905701\n",
      "  Validation Loss: 0.6539071798324585, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 434:\n",
      "  Training Loss: 0.6253567934036255, Training Accuracy: 0.7497888803482056\n",
      "  Validation Loss: 0.7196739912033081, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 435:\n",
      "  Training Loss: 0.6311955451965332, Training Accuracy: 0.7477301359176636\n",
      "  Validation Loss: 0.5214813351631165, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 436:\n",
      "  Training Loss: 0.6301568746566772, Training Accuracy: 0.7455658912658691\n",
      "  Validation Loss: 0.5244016647338867, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 437:\n",
      "  Training Loss: 0.6406545639038086, Training Accuracy: 0.7437711358070374\n",
      "  Validation Loss: 0.5260316133499146, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 438:\n",
      "  Training Loss: 0.612464964389801, Training Accuracy: 0.7525865435600281\n",
      "  Validation Loss: 0.5192486047744751, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 439:\n",
      "  Training Loss: 0.6243138909339905, Training Accuracy: 0.7488914728164673\n",
      "  Validation Loss: 0.5349953770637512, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 440:\n",
      "  Training Loss: 0.6126734614372253, Training Accuracy: 0.7483636140823364\n",
      "  Validation Loss: 0.5213464498519897, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 441:\n",
      "  Training Loss: 0.6231545805931091, Training Accuracy: 0.7466744184494019\n",
      "  Validation Loss: 0.6003388166427612, Validation Accuracy: 0.7740709185600281\n",
      "Epoch 442:\n",
      "  Training Loss: 0.621849775314331, Training Accuracy: 0.75\n",
      "  Validation Loss: 0.5216189026832581, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 443:\n",
      "  Training Loss: 0.6346941590309143, Training Accuracy: 0.7440350651741028\n",
      "  Validation Loss: 0.5368349552154541, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 444:\n",
      "  Training Loss: 0.626424252986908, Training Accuracy: 0.7457770109176636\n",
      "  Validation Loss: 0.6529680490493774, Validation Accuracy: 0.658150315284729\n",
      "Epoch 445:\n",
      "  Training Loss: 0.6364995837211609, Training Accuracy: 0.7459881901741028\n",
      "  Validation Loss: 0.5217153429985046, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 446:\n",
      "  Training Loss: 0.6366806626319885, Training Accuracy: 0.7503694891929626\n",
      "  Validation Loss: 0.5546571612358093, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 447:\n",
      "  Training Loss: 0.6249537467956543, Training Accuracy: 0.7504751086235046\n",
      "  Validation Loss: 0.7685387134552002, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 448:\n",
      "  Training Loss: 0.6332696676254272, Training Accuracy: 0.7466216087341309\n",
      "  Validation Loss: 0.5377102494239807, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 449:\n",
      "  Training Loss: 0.6209785342216492, Training Accuracy: 0.7442989945411682\n",
      "  Validation Loss: 0.5357475280761719, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 450:\n",
      "  Training Loss: 0.6308467388153076, Training Accuracy: 0.7435072064399719\n",
      "  Validation Loss: 0.521783173084259, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 451:\n",
      "  Training Loss: 0.6285081505775452, Training Accuracy: 0.7498416304588318\n",
      "  Validation Loss: 0.5268254280090332, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 452:\n",
      "  Training Loss: 0.6295906901359558, Training Accuracy: 0.7485219836235046\n",
      "  Validation Loss: 0.5575153231620789, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 453:\n",
      "  Training Loss: 0.638572096824646, Training Accuracy: 0.7485219836235046\n",
      "  Validation Loss: 0.5197405815124512, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 454:\n",
      "  Training Loss: 0.6328094005584717, Training Accuracy: 0.7456714510917664\n",
      "  Validation Loss: 0.5435917377471924, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 455:\n",
      "  Training Loss: 0.634441077709198, Training Accuracy: 0.7454603314399719\n",
      "  Validation Loss: 0.527503490447998, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 456:\n",
      "  Training Loss: 0.6303724050521851, Training Accuracy: 0.7435599565505981\n",
      "  Validation Loss: 0.5190747976303101, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 457:\n",
      "  Training Loss: 0.6168729066848755, Training Accuracy: 0.7465160489082336\n",
      "  Validation Loss: 0.8145635724067688, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 458:\n",
      "  Training Loss: 0.6299903988838196, Training Accuracy: 0.7468855381011963\n",
      "  Validation Loss: 0.5509520769119263, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 459:\n",
      "  Training Loss: 0.6348369717597961, Training Accuracy: 0.7438766956329346\n",
      "  Validation Loss: 0.518739640712738, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 460:\n",
      "  Training Loss: 0.6317567229270935, Training Accuracy: 0.7475717663764954\n",
      "  Validation Loss: 0.6119211316108704, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 461:\n",
      "  Training Loss: 0.6440631151199341, Training Accuracy: 0.7436127662658691\n",
      "  Validation Loss: 0.5719236731529236, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 462:\n",
      "  Training Loss: 0.6192920207977295, Training Accuracy: 0.7507917881011963\n",
      "  Validation Loss: 0.5381165742874146, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 463:\n",
      "  Training Loss: 0.616534411907196, Training Accuracy: 0.7514252662658691\n",
      "  Validation Loss: 0.8131052255630493, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 464:\n",
      "  Training Loss: 0.6412418484687805, Training Accuracy: 0.7431904673576355\n",
      "  Validation Loss: 0.5208298563957214, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 465:\n",
      "  Training Loss: 0.6216430068016052, Training Accuracy: 0.7455130815505981\n",
      "  Validation Loss: 0.6489558815956116, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 466:\n",
      "  Training Loss: 0.6284081935882568, Training Accuracy: 0.7463576793670654\n",
      "  Validation Loss: 0.5253207087516785, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 467:\n",
      "  Training Loss: 0.6500608325004578, Training Accuracy: 0.740181565284729\n",
      "  Validation Loss: 0.6839017868041992, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 468:\n",
      "  Training Loss: 0.6146570444107056, Training Accuracy: 0.7483636140823364\n",
      "  Validation Loss: 0.5654329061508179, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 469:\n",
      "  Training Loss: 0.628273606300354, Training Accuracy: 0.7492082118988037\n",
      "  Validation Loss: 0.7097086310386658, Validation Accuracy: 0.4972550570964813\n",
      "Epoch 470:\n",
      "  Training Loss: 0.6178892254829407, Training Accuracy: 0.7472550868988037\n",
      "  Validation Loss: 0.5739129781723022, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 471:\n",
      "  Training Loss: 0.6232143044471741, Training Accuracy: 0.7468855381011963\n",
      "  Validation Loss: 0.6974485516548157, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 472:\n",
      "  Training Loss: 0.6288873553276062, Training Accuracy: 0.7437183260917664\n",
      "  Validation Loss: 0.5480247735977173, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 473:\n",
      "  Training Loss: 0.6285501718521118, Training Accuracy: 0.746040940284729\n",
      "  Validation Loss: 0.5368515849113464, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 474:\n",
      "  Training Loss: 0.6211576461791992, Training Accuracy: 0.7444045543670654\n",
      "  Validation Loss: 0.7782796621322632, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 475:\n",
      "  Training Loss: 0.616984486579895, Training Accuracy: 0.7502639293670654\n",
      "  Validation Loss: 0.5248407125473022, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 476:\n",
      "  Training Loss: 0.6144216656684875, Training Accuracy: 0.7456186413764954\n",
      "  Validation Loss: 0.637180507183075, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 477:\n",
      "  Training Loss: 0.6243953704833984, Training Accuracy: 0.7464104890823364\n",
      "  Validation Loss: 0.5196646451950073, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 478:\n",
      "  Training Loss: 0.6217679977416992, Training Accuracy: 0.7502639293670654\n",
      "  Validation Loss: 0.5811803936958313, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 479:\n",
      "  Training Loss: 0.6246294975280762, Training Accuracy: 0.7452491521835327\n",
      "  Validation Loss: 0.7073127627372742, Validation Accuracy: 0.5054898858070374\n",
      "Epoch 480:\n",
      "  Training Loss: 0.6356455087661743, Training Accuracy: 0.7457242608070374\n",
      "  Validation Loss: 0.521589994430542, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 481:\n",
      "  Training Loss: 0.6071809530258179, Training Accuracy: 0.7501583695411682\n",
      "  Validation Loss: 2.002157211303711, Validation Accuracy: 0.24176520109176636\n",
      "Epoch 482:\n",
      "  Training Loss: 0.6268998980522156, Training Accuracy: 0.7465688586235046\n",
      "  Validation Loss: 0.8997839689254761, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 483:\n",
      "  Training Loss: 0.6122977137565613, Training Accuracy: 0.7523754239082336\n",
      "  Validation Loss: 0.5523179769515991, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 484:\n",
      "  Training Loss: 0.6143668293952942, Training Accuracy: 0.7498416304588318\n",
      "  Validation Loss: 0.5765624046325684, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 485:\n",
      "  Training Loss: 0.6138147711753845, Training Accuracy: 0.7506862282752991\n",
      "  Validation Loss: 0.522581934928894, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 486:\n",
      "  Training Loss: 0.6211605072021484, Training Accuracy: 0.7473078370094299\n",
      "  Validation Loss: 0.7449896335601807, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 487:\n",
      "  Training Loss: 0.6222881078720093, Training Accuracy: 0.7491025924682617\n",
      "  Validation Loss: 0.7108685970306396, Validation Accuracy: 0.49936655163764954\n",
      "Epoch 488:\n",
      "  Training Loss: 0.6200689077377319, Training Accuracy: 0.7488386631011963\n",
      "  Validation Loss: 0.8500845432281494, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 489:\n",
      "  Training Loss: 0.6081318855285645, Training Accuracy: 0.751953125\n",
      "  Validation Loss: 0.5988903641700745, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 490:\n",
      "  Training Loss: 0.6203277707099915, Training Accuracy: 0.7497360706329346\n",
      "  Validation Loss: 0.5212011337280273, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 491:\n",
      "  Training Loss: 0.6349765062332153, Training Accuracy: 0.7435599565505981\n",
      "  Validation Loss: 0.5203880071640015, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 492:\n",
      "  Training Loss: 0.5983583331108093, Training Accuracy: 0.7540646195411682\n",
      "  Validation Loss: 0.5902088284492493, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 493:\n",
      "  Training Loss: 0.6146639585494995, Training Accuracy: 0.7484163641929626\n",
      "  Validation Loss: 0.5239953398704529, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 494:\n",
      "  Training Loss: 0.6246374845504761, Training Accuracy: 0.7463048696517944\n",
      "  Validation Loss: 0.5179397463798523, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 495:\n",
      "  Training Loss: 0.6098100543022156, Training Accuracy: 0.7562288641929626\n",
      "  Validation Loss: 0.5479791760444641, Validation Accuracy: 0.7808277010917664\n",
      "Epoch 496:\n",
      "  Training Loss: 0.6210570931434631, Training Accuracy: 0.7463576793670654\n",
      "  Validation Loss: 0.5310534238815308, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 497:\n",
      "  Training Loss: 0.6262074112892151, Training Accuracy: 0.7526393532752991\n",
      "  Validation Loss: 0.5588030219078064, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 498:\n",
      "  Training Loss: 0.6278466582298279, Training Accuracy: 0.7429792881011963\n",
      "  Validation Loss: 0.5193837881088257, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 499:\n",
      "  Training Loss: 0.6011301875114441, Training Accuracy: 0.7551203370094299\n",
      "  Validation Loss: 0.5833526849746704, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 500:\n",
      "  Training Loss: 0.632470965385437, Training Accuracy: 0.7465160489082336\n",
      "  Validation Loss: 0.5182667374610901, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 501:\n",
      "  Training Loss: 0.6251649856567383, Training Accuracy: 0.7485219836235046\n",
      "  Validation Loss: 0.5225347280502319, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 502:\n",
      "  Training Loss: 0.6328328847885132, Training Accuracy: 0.7438238859176636\n",
      "  Validation Loss: 0.520298957824707, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 503:\n",
      "  Training Loss: 0.6260660290718079, Training Accuracy: 0.7494721412658691\n",
      "  Validation Loss: 0.5237582921981812, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 504:\n",
      "  Training Loss: 0.6198542714118958, Training Accuracy: 0.7462521195411682\n",
      "  Validation Loss: 0.5193967819213867, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 505:\n",
      "  Training Loss: 0.626927375793457, Training Accuracy: 0.7455130815505981\n",
      "  Validation Loss: 0.8285729289054871, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 506:\n",
      "  Training Loss: 0.6159425973892212, Training Accuracy: 0.7485747337341309\n",
      "  Validation Loss: 0.5351880788803101, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 507:\n",
      "  Training Loss: 0.6187942028045654, Training Accuracy: 0.7507917881011963\n",
      "  Validation Loss: 0.5818331241607666, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 508:\n",
      "  Training Loss: 0.6099838614463806, Training Accuracy: 0.7521114945411682\n",
      "  Validation Loss: 0.5606256723403931, Validation Accuracy: 0.7797719836235046\n",
      "Epoch 509:\n",
      "  Training Loss: 0.6251404285430908, Training Accuracy: 0.7514252662658691\n",
      "  Validation Loss: 0.5175908803939819, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 510:\n",
      "  Training Loss: 0.6128309369087219, Training Accuracy: 0.7479413151741028\n",
      "  Validation Loss: 0.5226719975471497, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 511:\n",
      "  Training Loss: 0.6142366528511047, Training Accuracy: 0.7510029673576355\n",
      "  Validation Loss: 0.7193265557289124, Validation Accuracy: 0.48859795928001404\n",
      "Epoch 512:\n",
      "  Training Loss: 0.6188719272613525, Training Accuracy: 0.7471494674682617\n",
      "  Validation Loss: 1.0591533184051514, Validation Accuracy: 0.31102195382118225\n",
      "Epoch 513:\n",
      "  Training Loss: 0.6134687066078186, Training Accuracy: 0.7512668967247009\n",
      "  Validation Loss: 0.5410513281822205, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 514:\n",
      "  Training Loss: 0.622589111328125, Training Accuracy: 0.7475717663764954\n",
      "  Validation Loss: 0.5296480059623718, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 515:\n",
      "  Training Loss: 0.618374764919281, Training Accuracy: 0.7510557174682617\n",
      "  Validation Loss: 0.7397577166557312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 516:\n",
      "  Training Loss: 0.625606894493103, Training Accuracy: 0.7479413151741028\n",
      "  Validation Loss: 0.5339078903198242, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 517:\n",
      "  Training Loss: 0.6117253303527832, Training Accuracy: 0.7482052445411682\n",
      "  Validation Loss: 1.406873345375061, Validation Accuracy: 0.2668918967247009\n",
      "Epoch 518:\n",
      "  Training Loss: 0.6187626123428345, Training Accuracy: 0.7511085271835327\n",
      "  Validation Loss: 0.5643965005874634, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 519:\n",
      "  Training Loss: 0.6038105487823486, Training Accuracy: 0.7532200217247009\n",
      "  Validation Loss: 0.5582179427146912, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 520:\n",
      "  Training Loss: 0.6102908849716187, Training Accuracy: 0.7485747337341309\n",
      "  Validation Loss: 0.5201542973518372, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 521:\n",
      "  Training Loss: 0.605229377746582, Training Accuracy: 0.755806565284729\n",
      "  Validation Loss: 0.690706193447113, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 522:\n",
      "  Training Loss: 0.6087068319320679, Training Accuracy: 0.7531672120094299\n",
      "  Validation Loss: 0.5347818732261658, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 523:\n",
      "  Training Loss: 0.6073899865150452, Training Accuracy: 0.7502111196517944\n",
      "  Validation Loss: 0.5186938643455505, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 524:\n",
      "  Training Loss: 0.6158971786499023, Training Accuracy: 0.7503694891929626\n",
      "  Validation Loss: 0.5171610116958618, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 525:\n",
      "  Training Loss: 0.6117890477180481, Training Accuracy: 0.7512668967247009\n",
      "  Validation Loss: 0.5185899138450623, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 526:\n",
      "  Training Loss: 0.6230413317680359, Training Accuracy: 0.74609375\n",
      "  Validation Loss: 0.5262786149978638, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 527:\n",
      "  Training Loss: 0.6156671643257141, Training Accuracy: 0.7523754239082336\n",
      "  Validation Loss: 0.5320062637329102, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 528:\n",
      "  Training Loss: 0.6270841956138611, Training Accuracy: 0.7442461848258972\n",
      "  Validation Loss: 0.549511730670929, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 529:\n",
      "  Training Loss: 0.6119332313537598, Training Accuracy: 0.7489970326423645\n",
      "  Validation Loss: 0.5776292085647583, Validation Accuracy: 0.7782939076423645\n",
      "Epoch 530:\n",
      "  Training Loss: 0.6309924721717834, Training Accuracy: 0.7458826303482056\n",
      "  Validation Loss: 0.5557451844215393, Validation Accuracy: 0.7799831032752991\n",
      "Epoch 531:\n",
      "  Training Loss: 0.6125680804252625, Training Accuracy: 0.7490498423576355\n",
      "  Validation Loss: 0.5425931811332703, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 532:\n",
      "  Training Loss: 0.6099293231964111, Training Accuracy: 0.7551731467247009\n",
      "  Validation Loss: 0.5205456018447876, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 533:\n",
      "  Training Loss: 0.6088269352912903, Training Accuracy: 0.7505278587341309\n",
      "  Validation Loss: 0.5173740983009338, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 534:\n",
      "  Training Loss: 0.6285873055458069, Training Accuracy: 0.7498944401741028\n",
      "  Validation Loss: 0.5172138214111328, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 535:\n",
      "  Training Loss: 0.6143769025802612, Training Accuracy: 0.7528505325317383\n",
      "  Validation Loss: 0.6716437935829163, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 536:\n",
      "  Training Loss: 0.6230530142784119, Training Accuracy: 0.7487859129905701\n",
      "  Validation Loss: 0.5426329970359802, Validation Accuracy: 0.7810388803482056\n",
      "Epoch 537:\n",
      "  Training Loss: 0.612333357334137, Training Accuracy: 0.7508445978164673\n",
      "  Validation Loss: 0.524846613407135, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 538:\n",
      "  Training Loss: 0.6157293319702148, Training Accuracy: 0.7491554021835327\n",
      "  Validation Loss: 0.518277108669281, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 539:\n",
      "  Training Loss: 0.6125897169113159, Training Accuracy: 0.7538006901741028\n",
      "  Validation Loss: 0.6477842330932617, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 540:\n",
      "  Training Loss: 0.6171705722808838, Training Accuracy: 0.7497360706329346\n",
      "  Validation Loss: 0.5908880233764648, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 541:\n",
      "  Training Loss: 0.6194247603416443, Training Accuracy: 0.7482579946517944\n",
      "  Validation Loss: 0.6145671606063843, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 542:\n",
      "  Training Loss: 0.6038839221000671, Training Accuracy: 0.7508974075317383\n",
      "  Validation Loss: 0.5243081450462341, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 543:\n",
      "  Training Loss: 0.6084398031234741, Training Accuracy: 0.7492082118988037\n",
      "  Validation Loss: 0.5195764303207397, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 544:\n",
      "  Training Loss: 0.6149933934211731, Training Accuracy: 0.7545396685600281\n",
      "  Validation Loss: 0.5171743035316467, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 545:\n",
      "  Training Loss: 0.6195264458656311, Training Accuracy: 0.7517420053482056\n",
      "  Validation Loss: 0.5241518616676331, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 546:\n",
      "  Training Loss: 0.6288753151893616, Training Accuracy: 0.7490498423576355\n",
      "  Validation Loss: 0.6838235855102539, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 547:\n",
      "  Training Loss: 0.609404444694519, Training Accuracy: 0.751953125\n",
      "  Validation Loss: 0.5176721811294556, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 548:\n",
      "  Training Loss: 0.6189455986022949, Training Accuracy: 0.7498944401741028\n",
      "  Validation Loss: 0.5502501130104065, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 549:\n",
      "  Training Loss: 0.6203536987304688, Training Accuracy: 0.7504222989082336\n",
      "  Validation Loss: 0.5170256495475769, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 550:\n",
      "  Training Loss: 0.6091105937957764, Training Accuracy: 0.7559649348258972\n",
      "  Validation Loss: 0.5208919644355774, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 551:\n",
      "  Training Loss: 0.6126339435577393, Training Accuracy: 0.7501055598258972\n",
      "  Validation Loss: 0.5806854367256165, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 552:\n",
      "  Training Loss: 0.6097567081451416, Training Accuracy: 0.7497888803482056\n",
      "  Validation Loss: 0.53619784116745, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 553:\n",
      "  Training Loss: 0.616429328918457, Training Accuracy: 0.7494193315505981\n",
      "  Validation Loss: 1.101730465888977, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 554:\n",
      "  Training Loss: 0.6100419759750366, Training Accuracy: 0.7516363859176636\n",
      "  Validation Loss: 0.5706213116645813, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 555:\n",
      "  Training Loss: 0.603854238986969, Training Accuracy: 0.749947190284729\n",
      "  Validation Loss: 0.5700328350067139, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 556:\n",
      "  Training Loss: 0.6105661988258362, Training Accuracy: 0.7504222989082336\n",
      "  Validation Loss: 0.6383925676345825, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 557:\n",
      "  Training Loss: 0.6011852622032166, Training Accuracy: 0.7545396685600281\n",
      "  Validation Loss: 0.6695994734764099, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 558:\n",
      "  Training Loss: 0.5983084440231323, Training Accuracy: 0.7537478804588318\n",
      "  Validation Loss: 0.5685809254646301, Validation Accuracy: 0.779349684715271\n",
      "Epoch 559:\n",
      "  Training Loss: 0.6090120077133179, Training Accuracy: 0.7515308260917664\n",
      "  Validation Loss: 0.730076253414154, Validation Accuracy: 0.4738175570964813\n",
      "Epoch 560:\n",
      "  Training Loss: 0.6124776005744934, Training Accuracy: 0.7521114945411682\n",
      "  Validation Loss: 0.549681544303894, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 561:\n",
      "  Training Loss: 0.6110771894454956, Training Accuracy: 0.7527449131011963\n",
      "  Validation Loss: 0.5517436265945435, Validation Accuracy: 0.7799831032752991\n",
      "Epoch 562:\n",
      "  Training Loss: 0.6013280749320984, Training Accuracy: 0.7542757391929626\n",
      "  Validation Loss: 0.5627322196960449, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 563:\n",
      "  Training Loss: 0.6035038828849792, Training Accuracy: 0.7515308260917664\n",
      "  Validation Loss: 0.5278713703155518, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 564:\n",
      "  Training Loss: 0.5965957045555115, Training Accuracy: 0.752005934715271\n",
      "  Validation Loss: 0.5569034814834595, Validation Accuracy: 0.7799831032752991\n",
      "Epoch 565:\n",
      "  Training Loss: 0.6034464240074158, Training Accuracy: 0.7497360706329346\n",
      "  Validation Loss: 0.5303792357444763, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 566:\n",
      "  Training Loss: 0.5898163914680481, Training Accuracy: 0.7561233043670654\n",
      "  Validation Loss: 0.5809052586555481, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 567:\n",
      "  Training Loss: 0.6186685562133789, Training Accuracy: 0.7489970326423645\n",
      "  Validation Loss: 0.5168328285217285, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 568:\n",
      "  Training Loss: 0.6162678599357605, Training Accuracy: 0.7501055598258972\n",
      "  Validation Loss: 0.5384210348129272, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 569:\n",
      "  Training Loss: 0.6008489727973938, Training Accuracy: 0.7536951303482056\n",
      "  Validation Loss: 0.5166115164756775, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 570:\n",
      "  Training Loss: 0.603712260723114, Training Accuracy: 0.7531144618988037\n",
      "  Validation Loss: 0.5363704562187195, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 571:\n",
      "  Training Loss: 0.603844940662384, Training Accuracy: 0.7523226141929626\n",
      "  Validation Loss: 0.5397320985794067, Validation Accuracy: 0.78125\n",
      "Epoch 572:\n",
      "  Training Loss: 0.5876312255859375, Training Accuracy: 0.7563872337341309\n",
      "  Validation Loss: 0.5494030117988586, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 573:\n",
      "  Training Loss: 0.6021944284439087, Training Accuracy: 0.7527977228164673\n",
      "  Validation Loss: 0.5181859731674194, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 574:\n",
      "  Training Loss: 0.605172872543335, Training Accuracy: 0.7551203370094299\n",
      "  Validation Loss: 0.5807068347930908, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 575:\n",
      "  Training Loss: 0.608156144618988, Training Accuracy: 0.7544341087341309\n",
      "  Validation Loss: 0.5262961387634277, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 576:\n",
      "  Training Loss: 0.6101979613304138, Training Accuracy: 0.7513197064399719\n",
      "  Validation Loss: 0.531735897064209, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 577:\n",
      "  Training Loss: 0.5958390831947327, Training Accuracy: 0.7560177445411682\n",
      "  Validation Loss: 0.5601779818534851, Validation Accuracy: 0.7795608043670654\n",
      "Epoch 578:\n",
      "  Training Loss: 0.600370466709137, Training Accuracy: 0.7548036575317383\n",
      "  Validation Loss: 0.8203343152999878, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 579:\n",
      "  Training Loss: 0.6120387315750122, Training Accuracy: 0.7527449131011963\n",
      "  Validation Loss: 0.5869007706642151, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 580:\n",
      "  Training Loss: 0.6179227232933044, Training Accuracy: 0.7550147771835327\n",
      "  Validation Loss: 0.5165042281150818, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 581:\n",
      "  Training Loss: 0.6157070398330688, Training Accuracy: 0.7472550868988037\n",
      "  Validation Loss: 0.518452525138855, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 582:\n",
      "  Training Loss: 0.6031708121299744, Training Accuracy: 0.7571262717247009\n",
      "  Validation Loss: 0.5682153701782227, Validation Accuracy: 0.7789273858070374\n",
      "Epoch 583:\n",
      "  Training Loss: 0.6053659319877625, Training Accuracy: 0.7517947554588318\n",
      "  Validation Loss: 0.5501279830932617, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 584:\n",
      "  Training Loss: 0.6004844903945923, Training Accuracy: 0.7531672120094299\n",
      "  Validation Loss: 0.5699855089187622, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 585:\n",
      "  Training Loss: 0.607374370098114, Training Accuracy: 0.7541173696517944\n",
      "  Validation Loss: 0.5163618922233582, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 586:\n",
      "  Training Loss: 0.6051667928695679, Training Accuracy: 0.7515836358070374\n",
      "  Validation Loss: 0.5653802156448364, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 587:\n",
      "  Training Loss: 0.6041443347930908, Training Accuracy: 0.7526393532752991\n",
      "  Validation Loss: 0.5178617238998413, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 588:\n",
      "  Training Loss: 0.6156222224235535, Training Accuracy: 0.7504751086235046\n",
      "  Validation Loss: 0.9234387278556824, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 589:\n",
      "  Training Loss: 0.6003386974334717, Training Accuracy: 0.7561761140823364\n",
      "  Validation Loss: 0.5885576009750366, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 590:\n",
      "  Training Loss: 0.609201967716217, Training Accuracy: 0.7543813586235046\n",
      "  Validation Loss: 0.5166020393371582, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 591:\n",
      "  Training Loss: 0.6022161841392517, Training Accuracy: 0.753853440284729\n",
      "  Validation Loss: 0.6230700612068176, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 592:\n",
      "  Training Loss: 0.605938732624054, Training Accuracy: 0.7507390379905701\n",
      "  Validation Loss: 0.5443484783172607, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 593:\n",
      "  Training Loss: 0.6082029342651367, Training Accuracy: 0.7523754239082336\n",
      "  Validation Loss: 0.5972967147827148, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 594:\n",
      "  Training Loss: 0.6081296801567078, Training Accuracy: 0.7506334185600281\n",
      "  Validation Loss: 0.523155689239502, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 595:\n",
      "  Training Loss: 0.6155357956886292, Training Accuracy: 0.7484691739082336\n",
      "  Validation Loss: 0.5489974617958069, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 596:\n",
      "  Training Loss: 0.5947057604789734, Training Accuracy: 0.7555426359176636\n",
      "  Validation Loss: 0.5169548988342285, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 597:\n",
      "  Training Loss: 0.5985636115074158, Training Accuracy: 0.7545924782752991\n",
      "  Validation Loss: 0.5170242190361023, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 598:\n",
      "  Training Loss: 0.601300835609436, Training Accuracy: 0.7541173696517944\n",
      "  Validation Loss: 0.5515162348747253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 599:\n",
      "  Training Loss: 0.608311116695404, Training Accuracy: 0.7515836358070374\n",
      "  Validation Loss: 0.5161476731300354, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 600:\n",
      "  Training Loss: 0.6060054302215576, Training Accuracy: 0.7533255815505981\n",
      "  Validation Loss: 0.5723897814750671, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 601:\n",
      "  Training Loss: 0.5916177034378052, Training Accuracy: 0.7548564076423645\n",
      "  Validation Loss: 0.5593941807746887, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 602:\n",
      "  Training Loss: 0.6129159927368164, Training Accuracy: 0.7531672120094299\n",
      "  Validation Loss: 0.5185838937759399, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 603:\n",
      "  Training Loss: 0.6023227572441101, Training Accuracy: 0.7553315162658691\n",
      "  Validation Loss: 0.5282523036003113, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 604:\n",
      "  Training Loss: 0.6097145676612854, Training Accuracy: 0.751900315284729\n",
      "  Validation Loss: 0.5175924301147461, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 605:\n",
      "  Training Loss: 0.6020402908325195, Training Accuracy: 0.7534311413764954\n",
      "  Validation Loss: 0.5185182690620422, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 606:\n",
      "  Training Loss: 0.5974785685539246, Training Accuracy: 0.7530088424682617\n",
      "  Validation Loss: 0.5338724851608276, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 607:\n",
      "  Training Loss: 0.6055274605751038, Training Accuracy: 0.7507390379905701\n",
      "  Validation Loss: 0.5205928683280945, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 608:\n",
      "  Training Loss: 0.5933755040168762, Training Accuracy: 0.7561233043670654\n",
      "  Validation Loss: 0.5850370526313782, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 609:\n",
      "  Training Loss: 0.5978411436080933, Training Accuracy: 0.7553842663764954\n",
      "  Validation Loss: 0.5209252834320068, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 610:\n",
      "  Training Loss: 0.6031267046928406, Training Accuracy: 0.7525865435600281\n",
      "  Validation Loss: 0.5436668992042542, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 611:\n",
      "  Training Loss: 0.6123534440994263, Training Accuracy: 0.7495777010917664\n",
      "  Validation Loss: 0.7514886260032654, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 612:\n",
      "  Training Loss: 0.599324107170105, Training Accuracy: 0.7540118098258972\n",
      "  Validation Loss: 0.6315970420837402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 613:\n",
      "  Training Loss: 0.6030650734901428, Training Accuracy: 0.7556482553482056\n",
      "  Validation Loss: 0.574956476688385, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 614:\n",
      "  Training Loss: 0.6081157326698303, Training Accuracy: 0.7523226141929626\n",
      "  Validation Loss: 0.5252687931060791, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 615:\n",
      "  Training Loss: 0.5928795337677002, Training Accuracy: 0.7579180598258972\n",
      "  Validation Loss: 1.3053420782089233, Validation Accuracy: 0.28462839126586914\n",
      "Epoch 616:\n",
      "  Training Loss: 0.6094866394996643, Training Accuracy: 0.7498944401741028\n",
      "  Validation Loss: 0.565298855304718, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 617:\n",
      "  Training Loss: 0.59487384557724, Training Accuracy: 0.7550675868988037\n",
      "  Validation Loss: 0.5172072052955627, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 618:\n",
      "  Training Loss: 0.6133642792701721, Training Accuracy: 0.7501055598258972\n",
      "  Validation Loss: 0.5843835473060608, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 619:\n",
      "  Training Loss: 0.5983864665031433, Training Accuracy: 0.7570207118988037\n",
      "  Validation Loss: 0.5182142853736877, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 620:\n",
      "  Training Loss: 0.6184076070785522, Training Accuracy: 0.7472022771835327\n",
      "  Validation Loss: 0.7134561538696289, Validation Accuracy: 0.5076013803482056\n",
      "Epoch 621:\n",
      "  Training Loss: 0.6241065263748169, Training Accuracy: 0.7484163641929626\n",
      "  Validation Loss: 0.5525017976760864, Validation Accuracy: 0.7804054021835327\n",
      "Epoch 622:\n",
      "  Training Loss: 0.6027059555053711, Training Accuracy: 0.7512140870094299\n",
      "  Validation Loss: 0.5252847671508789, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 623:\n",
      "  Training Loss: 0.5988348126411438, Training Accuracy: 0.7564927935600281\n",
      "  Validation Loss: 0.5434805750846863, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 624:\n",
      "  Training Loss: 0.6034024953842163, Training Accuracy: 0.7535895109176636\n",
      "  Validation Loss: 0.783473551273346, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 625:\n",
      "  Training Loss: 0.6062537431716919, Training Accuracy: 0.7489970326423645\n",
      "  Validation Loss: 0.9620519280433655, Validation Accuracy: 0.3369932472705841\n",
      "Epoch 626:\n",
      "  Training Loss: 0.5967259407043457, Training Accuracy: 0.7518475651741028\n",
      "  Validation Loss: 0.5299807190895081, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 627:\n",
      "  Training Loss: 0.5999011397361755, Training Accuracy: 0.7561233043670654\n",
      "  Validation Loss: 0.5162056684494019, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 628:\n",
      "  Training Loss: 0.6075470447540283, Training Accuracy: 0.7544869184494019\n",
      "  Validation Loss: 0.5283942818641663, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 629:\n",
      "  Training Loss: 0.6148229837417603, Training Accuracy: 0.7506862282752991\n",
      "  Validation Loss: 1.4819873571395874, Validation Accuracy: 0.27005910873413086\n",
      "Epoch 630:\n",
      "  Training Loss: 0.6148025989532471, Training Accuracy: 0.7513724565505981\n",
      "  Validation Loss: 0.5329256653785706, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 631:\n",
      "  Training Loss: 0.6050551533699036, Training Accuracy: 0.7504222989082336\n",
      "  Validation Loss: 0.5510683059692383, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 632:\n",
      "  Training Loss: 0.6027883291244507, Training Accuracy: 0.7510029673576355\n",
      "  Validation Loss: 0.5330604910850525, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 633:\n",
      "  Training Loss: 0.5867915153503418, Training Accuracy: 0.7583931684494019\n",
      "  Validation Loss: 0.5439084768295288, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 634:\n",
      "  Training Loss: 0.5874210000038147, Training Accuracy: 0.7568623423576355\n",
      "  Validation Loss: 0.5191552042961121, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 635:\n",
      "  Training Loss: 0.604514479637146, Training Accuracy: 0.7522170543670654\n",
      "  Validation Loss: 0.7010796666145325, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 636:\n",
      "  Training Loss: 0.6012495756149292, Training Accuracy: 0.7552259564399719\n",
      "  Validation Loss: 0.5230684876441956, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 637:\n",
      "  Training Loss: 0.5997658967971802, Training Accuracy: 0.7526393532752991\n",
      "  Validation Loss: 0.6732062697410583, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 638:\n",
      "  Training Loss: 0.59632408618927, Training Accuracy: 0.7533783912658691\n",
      "  Validation Loss: 0.5858936309814453, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 639:\n",
      "  Training Loss: 0.6015239953994751, Training Accuracy: 0.7546980381011963\n",
      "  Validation Loss: 0.5246042013168335, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 640:\n",
      "  Training Loss: 0.6019584536552429, Training Accuracy: 0.7524282336235046\n",
      "  Validation Loss: 0.5879971981048584, Validation Accuracy: 0.7776604890823364\n",
      "Epoch 641:\n",
      "  Training Loss: 0.6002101302146912, Training Accuracy: 0.7549619674682617\n",
      "  Validation Loss: 0.6317949295043945, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 642:\n",
      "  Training Loss: 0.5989790558815002, Training Accuracy: 0.757759690284729\n",
      "  Validation Loss: 0.5575239062309265, Validation Accuracy: 0.7799831032752991\n",
      "Epoch 643:\n",
      "  Training Loss: 0.6032595634460449, Training Accuracy: 0.7489442825317383\n",
      "  Validation Loss: 0.632672131061554, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 644:\n",
      "  Training Loss: 0.6067342758178711, Training Accuracy: 0.7548564076423645\n",
      "  Validation Loss: 0.5961337685585022, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 645:\n",
      "  Training Loss: 0.5976251363754272, Training Accuracy: 0.7525865435600281\n",
      "  Validation Loss: 0.5477269887924194, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 646:\n",
      "  Training Loss: 0.6158119440078735, Training Accuracy: 0.748099684715271\n",
      "  Validation Loss: 0.5474221706390381, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 647:\n",
      "  Training Loss: 0.6032557487487793, Training Accuracy: 0.7548564076423645\n",
      "  Validation Loss: 0.5255742073059082, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 648:\n",
      "  Training Loss: 0.5943047404289246, Training Accuracy: 0.7546452879905701\n",
      "  Validation Loss: 0.5684399008750916, Validation Accuracy: 0.7789273858070374\n",
      "Epoch 649:\n",
      "  Training Loss: 0.5978190302848816, Training Accuracy: 0.7578125\n",
      "  Validation Loss: 0.5238312482833862, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 650:\n",
      "  Training Loss: 0.6108061671257019, Training Accuracy: 0.7521114945411682\n",
      "  Validation Loss: 0.5274685025215149, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 651:\n",
      "  Training Loss: 0.600012481212616, Training Accuracy: 0.755859375\n",
      "  Validation Loss: 0.523125171661377, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 652:\n",
      "  Training Loss: 0.5973796844482422, Training Accuracy: 0.7555954456329346\n",
      "  Validation Loss: 0.5294263958930969, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 653:\n",
      "  Training Loss: 0.5992587208747864, Training Accuracy: 0.7575485706329346\n",
      "  Validation Loss: 0.5526477694511414, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 654:\n",
      "  Training Loss: 0.6189903020858765, Training Accuracy: 0.7477301359176636\n",
      "  Validation Loss: 0.610247790813446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 655:\n",
      "  Training Loss: 0.5958430171012878, Training Accuracy: 0.753959059715271\n",
      "  Validation Loss: 0.564056932926178, Validation Accuracy: 0.7795608043670654\n",
      "Epoch 656:\n",
      "  Training Loss: 0.5881409645080566, Training Accuracy: 0.7560704946517944\n",
      "  Validation Loss: 0.5158697366714478, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 657:\n",
      "  Training Loss: 0.600006103515625, Training Accuracy: 0.7551731467247009\n",
      "  Validation Loss: 0.5207680463790894, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 658:\n",
      "  Training Loss: 0.6038903594017029, Training Accuracy: 0.7517420053482056\n",
      "  Validation Loss: 0.5162211060523987, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 659:\n",
      "  Training Loss: 0.5984481573104858, Training Accuracy: 0.7532728314399719\n",
      "  Validation Loss: 0.5161436200141907, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 660:\n",
      "  Training Loss: 0.5989632606506348, Training Accuracy: 0.7536423206329346\n",
      "  Validation Loss: 0.5184382200241089, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 661:\n",
      "  Training Loss: 0.6022274494171143, Training Accuracy: 0.7529560923576355\n",
      "  Validation Loss: 0.5482690334320068, Validation Accuracy: 0.7810388803482056\n",
      "Epoch 662:\n",
      "  Training Loss: 0.6008425951004028, Training Accuracy: 0.7523226141929626\n",
      "  Validation Loss: 0.5175149440765381, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 663:\n",
      "  Training Loss: 0.6004422307014465, Training Accuracy: 0.7582876086235046\n",
      "  Validation Loss: 0.6402524709701538, Validation Accuracy: 0.6868665814399719\n",
      "Epoch 664:\n",
      "  Training Loss: 0.5860853791236877, Training Accuracy: 0.7574430108070374\n",
      "  Validation Loss: 0.5532354116439819, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 665:\n",
      "  Training Loss: 0.6084755659103394, Training Accuracy: 0.7487331032752991\n",
      "  Validation Loss: 0.5333526134490967, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 666:\n",
      "  Training Loss: 0.6007195711135864, Training Accuracy: 0.7545396685600281\n",
      "  Validation Loss: 0.546196699142456, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 667:\n",
      "  Training Loss: 0.5883710980415344, Training Accuracy: 0.7565456032752991\n",
      "  Validation Loss: 0.5675154328346252, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 668:\n",
      "  Training Loss: 0.6029449105262756, Training Accuracy: 0.7523226141929626\n",
      "  Validation Loss: 0.7567232251167297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 669:\n",
      "  Training Loss: 0.590742826461792, Training Accuracy: 0.7548564076423645\n",
      "  Validation Loss: 0.5246400833129883, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 670:\n",
      "  Training Loss: 0.5993849039077759, Training Accuracy: 0.7542757391929626\n",
      "  Validation Loss: 0.5185471177101135, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 671:\n",
      "  Training Loss: 0.5964760184288025, Training Accuracy: 0.7570734620094299\n",
      "  Validation Loss: 0.5386166572570801, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 672:\n",
      "  Training Loss: 0.6059188842773438, Training Accuracy: 0.7517420053482056\n",
      "  Validation Loss: 0.5341691374778748, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 673:\n",
      "  Training Loss: 0.5952590703964233, Training Accuracy: 0.7554898858070374\n",
      "  Validation Loss: 0.725584864616394, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 674:\n",
      "  Training Loss: 0.600407600402832, Training Accuracy: 0.7540118098258972\n",
      "  Validation Loss: 0.5673535466194153, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 675:\n",
      "  Training Loss: 0.598423421382904, Training Accuracy: 0.7537478804588318\n",
      "  Validation Loss: 0.6011306643486023, Validation Accuracy: 0.7723817825317383\n",
      "Epoch 676:\n",
      "  Training Loss: 0.5987336039543152, Training Accuracy: 0.752005934715271\n",
      "  Validation Loss: 0.5553342700004578, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 677:\n",
      "  Training Loss: 0.5923452973365784, Training Accuracy: 0.7572846412658691\n",
      "  Validation Loss: 0.5166589617729187, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 678:\n",
      "  Training Loss: 0.5924817323684692, Training Accuracy: 0.7586570978164673\n",
      "  Validation Loss: 0.5453097224235535, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 679:\n",
      "  Training Loss: 0.6056022644042969, Training Accuracy: 0.7535895109176636\n",
      "  Validation Loss: 0.5158414840698242, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 680:\n",
      "  Training Loss: 0.5975423455238342, Training Accuracy: 0.7562288641929626\n",
      "  Validation Loss: 0.6338295936584473, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 681:\n",
      "  Training Loss: 0.5926466584205627, Training Accuracy: 0.7536951303482056\n",
      "  Validation Loss: 0.524947464466095, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 682:\n",
      "  Training Loss: 0.6031560301780701, Training Accuracy: 0.7494193315505981\n",
      "  Validation Loss: 0.5207698941230774, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 683:\n",
      "  Training Loss: 0.5938521027565002, Training Accuracy: 0.7574957609176636\n",
      "  Validation Loss: 0.5269773602485657, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 684:\n",
      "  Training Loss: 0.5907338857650757, Training Accuracy: 0.7610325217247009\n",
      "  Validation Loss: 0.5517088770866394, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 685:\n",
      "  Training Loss: 0.593048095703125, Training Accuracy: 0.7543285489082336\n",
      "  Validation Loss: 0.6080785989761353, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 686:\n",
      "  Training Loss: 0.5948879718780518, Training Accuracy: 0.7564927935600281\n",
      "  Validation Loss: 0.5540024042129517, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 687:\n",
      "  Training Loss: 0.6049444079399109, Training Accuracy: 0.7511085271835327\n",
      "  Validation Loss: 0.5223844647407532, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 688:\n",
      "  Training Loss: 0.609391987323761, Training Accuracy: 0.7502111196517944\n",
      "  Validation Loss: 0.542436957359314, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 689:\n",
      "  Training Loss: 0.5973143577575684, Training Accuracy: 0.7527977228164673\n",
      "  Validation Loss: 0.5406680107116699, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 690:\n",
      "  Training Loss: 0.5901709794998169, Training Accuracy: 0.7578125\n",
      "  Validation Loss: 0.775814414024353, Validation Accuracy: 0.43201014399528503\n",
      "Epoch 691:\n",
      "  Training Loss: 0.61046302318573, Training Accuracy: 0.7498416304588318\n",
      "  Validation Loss: 0.5835279226303101, Validation Accuracy: 0.7785050868988037\n",
      "Epoch 692:\n",
      "  Training Loss: 0.6104905605316162, Training Accuracy: 0.7491025924682617\n",
      "  Validation Loss: 0.5888921022415161, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 693:\n",
      "  Training Loss: 0.5922290086746216, Training Accuracy: 0.753853440284729\n",
      "  Validation Loss: 0.5175963640213013, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 694:\n",
      "  Training Loss: 0.5948115587234497, Training Accuracy: 0.7533255815505981\n",
      "  Validation Loss: 0.5714446902275085, Validation Accuracy: 0.7791385054588318\n",
      "Epoch 695:\n",
      "  Training Loss: 0.5997728705406189, Training Accuracy: 0.755806565284729\n",
      "  Validation Loss: 0.5786581635475159, Validation Accuracy: 0.7787162065505981\n",
      "Epoch 696:\n",
      "  Training Loss: 0.5959293842315674, Training Accuracy: 0.7514780163764954\n",
      "  Validation Loss: 0.6693286299705505, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 697:\n",
      "  Training Loss: 0.597683310508728, Training Accuracy: 0.7515308260917664\n",
      "  Validation Loss: 0.5722030997276306, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 698:\n",
      "  Training Loss: 0.5899606943130493, Training Accuracy: 0.7553315162658691\n",
      "  Validation Loss: 0.5398960709571838, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 699:\n",
      "  Training Loss: 0.5890215635299683, Training Accuracy: 0.7540118098258972\n",
      "  Validation Loss: 0.5173050761222839, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 700:\n",
      "  Training Loss: 0.5894728302955627, Training Accuracy: 0.7560704946517944\n",
      "  Validation Loss: 0.5322511196136475, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 701:\n",
      "  Training Loss: 0.591020405292511, Training Accuracy: 0.7556482553482056\n",
      "  Validation Loss: 0.9725316166877747, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 702:\n",
      "  Training Loss: 0.5900585055351257, Training Accuracy: 0.7555426359176636\n",
      "  Validation Loss: 0.6224695444107056, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 703:\n",
      "  Training Loss: 0.5797701478004456, Training Accuracy: 0.7609269618988037\n",
      "  Validation Loss: 0.5155333280563354, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 704:\n",
      "  Training Loss: 0.5999108552932739, Training Accuracy: 0.7531144618988037\n",
      "  Validation Loss: 0.7644355297088623, Validation Accuracy: 0.4480574429035187\n",
      "Epoch 705:\n",
      "  Training Loss: 0.5836765766143799, Training Accuracy: 0.7567039728164673\n",
      "  Validation Loss: 0.6653338670730591, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 706:\n",
      "  Training Loss: 0.5971418619155884, Training Accuracy: 0.7542229890823364\n",
      "  Validation Loss: 0.5797548890113831, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 707:\n",
      "  Training Loss: 0.5870160460472107, Training Accuracy: 0.7570734620094299\n",
      "  Validation Loss: 0.5425962209701538, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 708:\n",
      "  Training Loss: 0.5985350608825684, Training Accuracy: 0.7548564076423645\n",
      "  Validation Loss: 0.6759432554244995, Validation Accuracy: 0.5897381901741028\n",
      "Epoch 709:\n",
      "  Training Loss: 0.5914406180381775, Training Accuracy: 0.7543813586235046\n",
      "  Validation Loss: 0.6945741176605225, Validation Accuracy: 0.5572212934494019\n",
      "Epoch 710:\n",
      "  Training Loss: 0.5887579917907715, Training Accuracy: 0.7573902010917664\n",
      "  Validation Loss: 0.8463961482048035, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 711:\n",
      "  Training Loss: 0.6000306606292725, Training Accuracy: 0.753959059715271\n",
      "  Validation Loss: 0.5297067165374756, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 712:\n",
      "  Training Loss: 0.6035734415054321, Training Accuracy: 0.7530088424682617\n",
      "  Validation Loss: 0.5463659167289734, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 713:\n",
      "  Training Loss: 0.5858678817749023, Training Accuracy: 0.7575485706329346\n",
      "  Validation Loss: 0.5230201482772827, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 714:\n",
      "  Training Loss: 0.6001975536346436, Training Accuracy: 0.7555426359176636\n",
      "  Validation Loss: 0.5381268858909607, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 715:\n",
      "  Training Loss: 0.5953646302223206, Training Accuracy: 0.7536951303482056\n",
      "  Validation Loss: 0.5377662777900696, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 716:\n",
      "  Training Loss: 0.600308358669281, Training Accuracy: 0.7512140870094299\n",
      "  Validation Loss: 0.5307708382606506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 717:\n",
      "  Training Loss: 0.5879621505737305, Training Accuracy: 0.7554898858070374\n",
      "  Validation Loss: 0.5149071216583252, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 718:\n",
      "  Training Loss: 0.5854484438896179, Training Accuracy: 0.7587626576423645\n",
      "  Validation Loss: 0.5444372296333313, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 719:\n",
      "  Training Loss: 0.5850380659103394, Training Accuracy: 0.7570207118988037\n",
      "  Validation Loss: 0.5559719204902649, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 720:\n",
      "  Training Loss: 0.5864353179931641, Training Accuracy: 0.7563344836235046\n",
      "  Validation Loss: 0.5221138596534729, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 721:\n",
      "  Training Loss: 0.5893247723579407, Training Accuracy: 0.7577069401741028\n",
      "  Validation Loss: 0.5238614082336426, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 722:\n",
      "  Training Loss: 0.587181806564331, Training Accuracy: 0.7573373913764954\n",
      "  Validation Loss: 0.5367174744606018, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 723:\n",
      "  Training Loss: 0.5887672305107117, Training Accuracy: 0.7578125\n",
      "  Validation Loss: 0.5238466858863831, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 724:\n",
      "  Training Loss: 0.5964435935020447, Training Accuracy: 0.7538006901741028\n",
      "  Validation Loss: 0.5347223281860352, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 725:\n",
      "  Training Loss: 0.5894498229026794, Training Accuracy: 0.7543813586235046\n",
      "  Validation Loss: 0.5163758397102356, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 726:\n",
      "  Training Loss: 0.5921047329902649, Training Accuracy: 0.7583931684494019\n",
      "  Validation Loss: 0.5149350762367249, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 727:\n",
      "  Training Loss: 0.5939920544624329, Training Accuracy: 0.7576013803482056\n",
      "  Validation Loss: 0.5770456790924072, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 728:\n",
      "  Training Loss: 0.5787405371665955, Training Accuracy: 0.7589738368988037\n",
      "  Validation Loss: 0.5623254179954529, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 729:\n",
      "  Training Loss: 0.5966577529907227, Training Accuracy: 0.7573902010917664\n",
      "  Validation Loss: 0.5148544311523438, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 730:\n",
      "  Training Loss: 0.5879027843475342, Training Accuracy: 0.7548036575317383\n",
      "  Validation Loss: 0.8721433281898499, Validation Accuracy: 0.37647804617881775\n",
      "Epoch 731:\n",
      "  Training Loss: 0.587282121181488, Training Accuracy: 0.7583403587341309\n",
      "  Validation Loss: 0.5501726269721985, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 732:\n",
      "  Training Loss: 0.5889315605163574, Training Accuracy: 0.7550675868988037\n",
      "  Validation Loss: 0.6908959150314331, Validation Accuracy: 0.5631334185600281\n",
      "Epoch 733:\n",
      "  Training Loss: 0.5934225916862488, Training Accuracy: 0.7567567825317383\n",
      "  Validation Loss: 0.5187620520591736, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 734:\n",
      "  Training Loss: 0.5825431942939758, Training Accuracy: 0.7579708695411682\n",
      "  Validation Loss: 0.5263888835906982, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 735:\n",
      "  Training Loss: 0.5829161405563354, Training Accuracy: 0.7598711848258972\n",
      "  Validation Loss: 0.5458219647407532, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 736:\n",
      "  Training Loss: 0.5899222493171692, Training Accuracy: 0.7563344836235046\n",
      "  Validation Loss: 0.5807523727416992, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 737:\n",
      "  Training Loss: 0.5869356989860535, Training Accuracy: 0.7567039728164673\n",
      "  Validation Loss: 0.5205812454223633, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 738:\n",
      "  Training Loss: 0.5873830318450928, Training Accuracy: 0.7580236196517944\n",
      "  Validation Loss: 0.5661625266075134, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 739:\n",
      "  Training Loss: 0.5908778309822083, Training Accuracy: 0.7555954456329346\n",
      "  Validation Loss: 0.5418451428413391, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 740:\n",
      "  Training Loss: 0.5961529016494751, Training Accuracy: 0.7525865435600281\n",
      "  Validation Loss: 0.5344334840774536, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 741:\n",
      "  Training Loss: 0.600379467010498, Training Accuracy: 0.7514252662658691\n",
      "  Validation Loss: 0.5147741436958313, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 742:\n",
      "  Training Loss: 0.6024249196052551, Training Accuracy: 0.7546452879905701\n",
      "  Validation Loss: 0.8946383595466614, Validation Accuracy: 0.3684543967247009\n",
      "Epoch 743:\n",
      "  Training Loss: 0.5986453294754028, Training Accuracy: 0.7547508478164673\n",
      "  Validation Loss: 0.5365289449691772, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 744:\n",
      "  Training Loss: 0.5937873721122742, Training Accuracy: 0.7535367608070374\n",
      "  Validation Loss: 0.6059900522232056, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 745:\n",
      "  Training Loss: 0.585902214050293, Training Accuracy: 0.7563344836235046\n",
      "  Validation Loss: 0.5524618029594421, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 746:\n",
      "  Training Loss: 0.590573787689209, Training Accuracy: 0.7568623423576355\n",
      "  Validation Loss: 0.5159651637077332, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 747:\n",
      "  Training Loss: 0.5870727896690369, Training Accuracy: 0.7565984129905701\n",
      "  Validation Loss: 0.5334782004356384, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 748:\n",
      "  Training Loss: 0.5817968249320984, Training Accuracy: 0.7573902010917664\n",
      "  Validation Loss: 0.5203390121459961, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 749:\n",
      "  Training Loss: 0.5908781886100769, Training Accuracy: 0.7576541304588318\n",
      "  Validation Loss: 0.5417413711547852, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 750:\n",
      "  Training Loss: 0.5906042456626892, Training Accuracy: 0.7572318315505981\n",
      "  Validation Loss: 0.9191042184829712, Validation Accuracy: 0.36169764399528503\n",
      "Epoch 751:\n",
      "  Training Loss: 0.5860645771026611, Training Accuracy: 0.7579708695411682\n",
      "  Validation Loss: 0.6054575443267822, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 752:\n",
      "  Training Loss: 0.584930419921875, Training Accuracy: 0.7564927935600281\n",
      "  Validation Loss: 0.5555256605148315, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 753:\n",
      "  Training Loss: 0.6033799648284912, Training Accuracy: 0.7488914728164673\n",
      "  Validation Loss: 0.6898003220558167, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 754:\n",
      "  Training Loss: 0.5884691476821899, Training Accuracy: 0.7568623423576355\n",
      "  Validation Loss: 0.5145757794380188, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 755:\n",
      "  Training Loss: 0.5873638987541199, Training Accuracy: 0.75390625\n",
      "  Validation Loss: 0.5757893919944763, Validation Accuracy: 0.7791385054588318\n",
      "Epoch 756:\n",
      "  Training Loss: 0.5927252173423767, Training Accuracy: 0.7560704946517944\n",
      "  Validation Loss: 0.5272437930107117, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 757:\n",
      "  Training Loss: 0.5923951268196106, Training Accuracy: 0.7555426359176636\n",
      "  Validation Loss: 0.5152429342269897, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 758:\n",
      "  Training Loss: 0.5855672955513, Training Accuracy: 0.7574957609176636\n",
      "  Validation Loss: 0.5275533199310303, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 759:\n",
      "  Training Loss: 0.5824882984161377, Training Accuracy: 0.7591849565505981\n",
      "  Validation Loss: 0.5576903223991394, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 760:\n",
      "  Training Loss: 0.5786415934562683, Training Accuracy: 0.759712815284729\n",
      "  Validation Loss: 0.6368762850761414, Validation Accuracy: 0.689400315284729\n",
      "Epoch 761:\n",
      "  Training Loss: 0.5761856436729431, Training Accuracy: 0.761665940284729\n",
      "  Validation Loss: 0.5494145154953003, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 762:\n",
      "  Training Loss: 0.5884225964546204, Training Accuracy: 0.7548564076423645\n",
      "  Validation Loss: 0.5157729387283325, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 763:\n",
      "  Training Loss: 0.5879713892936707, Training Accuracy: 0.7572846412658691\n",
      "  Validation Loss: 0.6582207083702087, Validation Accuracy: 0.6315456032752991\n",
      "Epoch 764:\n",
      "  Training Loss: 0.581538200378418, Training Accuracy: 0.7584987282752991\n",
      "  Validation Loss: 0.5415734052658081, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 765:\n",
      "  Training Loss: 0.600294291973114, Training Accuracy: 0.7545396685600281\n",
      "  Validation Loss: 0.5676738619804382, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 766:\n",
      "  Training Loss: 0.5890512466430664, Training Accuracy: 0.7560177445411682\n",
      "  Validation Loss: 0.5178577303886414, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 767:\n",
      "  Training Loss: 0.5739886164665222, Training Accuracy: 0.761665940284729\n",
      "  Validation Loss: 0.5416220426559448, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 768:\n",
      "  Training Loss: 0.5917115211486816, Training Accuracy: 0.7542757391929626\n",
      "  Validation Loss: 0.5760147571563721, Validation Accuracy: 0.7795608043670654\n",
      "Epoch 769:\n",
      "  Training Loss: 0.5948275327682495, Training Accuracy: 0.7550147771835327\n",
      "  Validation Loss: 0.6481911540031433, Validation Accuracy: 0.6613175868988037\n",
      "Epoch 770:\n",
      "  Training Loss: 0.587405264377594, Training Accuracy: 0.7576013803482056\n",
      "  Validation Loss: 0.5256208181381226, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 771:\n",
      "  Training Loss: 0.5857356190681458, Training Accuracy: 0.7568623423576355\n",
      "  Validation Loss: 0.5146066546440125, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 772:\n",
      "  Training Loss: 0.5934876203536987, Training Accuracy: 0.7543813586235046\n",
      "  Validation Loss: 0.8150211572647095, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 773:\n",
      "  Training Loss: 0.5880429744720459, Training Accuracy: 0.7570207118988037\n",
      "  Validation Loss: 0.5621302127838135, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 774:\n",
      "  Training Loss: 0.585620641708374, Training Accuracy: 0.7567567825317383\n",
      "  Validation Loss: 0.5560023188591003, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 775:\n",
      "  Training Loss: 0.581813633441925, Training Accuracy: 0.7591322064399719\n",
      "  Validation Loss: 0.5155418515205383, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 776:\n",
      "  Training Loss: 0.5925138592720032, Training Accuracy: 0.7571790814399719\n",
      "  Validation Loss: 0.5330932140350342, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 777:\n",
      "  Training Loss: 0.5865864157676697, Training Accuracy: 0.7580764293670654\n",
      "  Validation Loss: 0.5416545867919922, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 778:\n",
      "  Training Loss: 0.5899057388305664, Training Accuracy: 0.7569679021835327\n",
      "  Validation Loss: 0.5698714852333069, Validation Accuracy: 0.7795608043670654\n",
      "Epoch 779:\n",
      "  Training Loss: 0.5864084362983704, Training Accuracy: 0.7576013803482056\n",
      "  Validation Loss: 0.5402400493621826, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 780:\n",
      "  Training Loss: 0.6007418036460876, Training Accuracy: 0.7511085271835327\n",
      "  Validation Loss: 0.5145238637924194, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 781:\n",
      "  Training Loss: 0.5804189443588257, Training Accuracy: 0.761665940284729\n",
      "  Validation Loss: 0.5474873185157776, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 782:\n",
      "  Training Loss: 0.5867314338684082, Training Accuracy: 0.7572318315505981\n",
      "  Validation Loss: 0.5148148536682129, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 783:\n",
      "  Training Loss: 0.5976487994194031, Training Accuracy: 0.7552259564399719\n",
      "  Validation Loss: 0.5315174460411072, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 784:\n",
      "  Training Loss: 0.5830733776092529, Training Accuracy: 0.7583403587341309\n",
      "  Validation Loss: 0.5355451107025146, Validation Accuracy: 0.7820945978164673\n",
      "Epoch 785:\n",
      "  Training Loss: 0.5885111093521118, Training Accuracy: 0.7590265870094299\n",
      "  Validation Loss: 0.5472986102104187, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 786:\n",
      "  Training Loss: 0.5937185883522034, Training Accuracy: 0.7560704946517944\n",
      "  Validation Loss: 0.5145164728164673, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 787:\n",
      "  Training Loss: 0.5848023891448975, Training Accuracy: 0.7592905163764954\n",
      "  Validation Loss: 0.5237062573432922, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 788:\n",
      "  Training Loss: 0.5858020782470703, Training Accuracy: 0.7537478804588318\n",
      "  Validation Loss: 0.6811704039573669, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 789:\n",
      "  Training Loss: 0.591303825378418, Training Accuracy: 0.7554898858070374\n",
      "  Validation Loss: 0.5748417973518372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 790:\n",
      "  Training Loss: 0.5743714570999146, Training Accuracy: 0.7598711848258972\n",
      "  Validation Loss: 0.5169693231582642, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 791:\n",
      "  Training Loss: 0.584119975566864, Training Accuracy: 0.7571262717247009\n",
      "  Validation Loss: 0.5310823321342468, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 792:\n",
      "  Training Loss: 0.5853484869003296, Training Accuracy: 0.7591322064399719\n",
      "  Validation Loss: 0.656658947467804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 793:\n",
      "  Training Loss: 0.5816628336906433, Training Accuracy: 0.7562816739082336\n",
      "  Validation Loss: 0.5879971385002136, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 794:\n",
      "  Training Loss: 0.5822306871414185, Training Accuracy: 0.7565456032752991\n",
      "  Validation Loss: 0.5227124691009521, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 795:\n",
      "  Training Loss: 0.5871787071228027, Training Accuracy: 0.7543813586235046\n",
      "  Validation Loss: 0.5151916146278381, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 796:\n",
      "  Training Loss: 0.5860264897346497, Training Accuracy: 0.7569150924682617\n",
      "  Validation Loss: 0.5310852527618408, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 797:\n",
      "  Training Loss: 0.5788337588310242, Training Accuracy: 0.7583931684494019\n",
      "  Validation Loss: 0.5334206223487854, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 798:\n",
      "  Training Loss: 0.5804151296615601, Training Accuracy: 0.7569679021835327\n",
      "  Validation Loss: 0.5232998132705688, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 799:\n",
      "  Training Loss: 0.5829947590827942, Training Accuracy: 0.7607157826423645\n",
      "  Validation Loss: 0.5146507024765015, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 800:\n",
      "  Training Loss: 0.5760743618011475, Training Accuracy: 0.7627744674682617\n",
      "  Validation Loss: 0.532497763633728, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 801:\n",
      "  Training Loss: 0.5790605545043945, Training Accuracy: 0.7583931684494019\n",
      "  Validation Loss: 0.5169336795806885, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 802:\n",
      "  Training Loss: 0.5820226669311523, Training Accuracy: 0.7573373913764954\n",
      "  Validation Loss: 0.5363538861274719, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 803:\n",
      "  Training Loss: 0.5795091986656189, Training Accuracy: 0.7624049782752991\n",
      "  Validation Loss: 0.6038639545440674, Validation Accuracy: 0.7630912065505981\n",
      "Epoch 804:\n",
      "  Training Loss: 0.5825709104537964, Training Accuracy: 0.7583931684494019\n",
      "  Validation Loss: 0.5363062024116516, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 805:\n",
      "  Training Loss: 0.6005849242210388, Training Accuracy: 0.7535367608070374\n",
      "  Validation Loss: 0.6074058413505554, Validation Accuracy: 0.7576013803482056\n",
      "Epoch 806:\n",
      "  Training Loss: 0.5839608907699585, Training Accuracy: 0.7608213424682617\n",
      "  Validation Loss: 0.5154446363449097, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 807:\n",
      "  Training Loss: 0.5855419635772705, Training Accuracy: 0.7557010054588318\n",
      "  Validation Loss: 0.549387514591217, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 808:\n",
      "  Training Loss: 0.5911392569541931, Training Accuracy: 0.7537478804588318\n",
      "  Validation Loss: 0.5151228904724121, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 809:\n",
      "  Training Loss: 0.5928552746772766, Training Accuracy: 0.7562816739082336\n",
      "  Validation Loss: 0.5142377614974976, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 810:\n",
      "  Training Loss: 0.5927073955535889, Training Accuracy: 0.755806565284729\n",
      "  Validation Loss: 0.533328115940094, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 811:\n",
      "  Training Loss: 0.5918084979057312, Training Accuracy: 0.7544869184494019\n",
      "  Validation Loss: 0.6723016500473022, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 812:\n",
      "  Training Loss: 0.5791223645210266, Training Accuracy: 0.7585515379905701\n",
      "  Validation Loss: 0.5624068975448608, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 813:\n",
      "  Training Loss: 0.5825698971748352, Training Accuracy: 0.7588154673576355\n",
      "  Validation Loss: 0.5260977745056152, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 814:\n",
      "  Training Loss: 0.5901209115982056, Training Accuracy: 0.7526921629905701\n",
      "  Validation Loss: 0.5295148491859436, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 815:\n",
      "  Training Loss: 0.584636390209198, Training Accuracy: 0.7572318315505981\n",
      "  Validation Loss: 0.5150136947631836, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 816:\n",
      "  Training Loss: 0.5940517783164978, Training Accuracy: 0.7578125\n",
      "  Validation Loss: 0.5247743725776672, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 817:\n",
      "  Training Loss: 0.5821840763092041, Training Accuracy: 0.7576013803482056\n",
      "  Validation Loss: 0.5501804947853088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 818:\n",
      "  Training Loss: 0.5742988586425781, Training Accuracy: 0.761771559715271\n",
      "  Validation Loss: 0.5140913128852844, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 819:\n",
      "  Training Loss: 0.5856900811195374, Training Accuracy: 0.7626689076423645\n",
      "  Validation Loss: 0.6169643998146057, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 820:\n",
      "  Training Loss: 0.5762388110160828, Training Accuracy: 0.7607157826423645\n",
      "  Validation Loss: 0.7506059408187866, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 821:\n",
      "  Training Loss: 0.589135468006134, Training Accuracy: 0.7551203370094299\n",
      "  Validation Loss: 0.5331253409385681, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 822:\n",
      "  Training Loss: 0.5710760951042175, Training Accuracy: 0.7628800868988037\n",
      "  Validation Loss: 0.519300639629364, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 823:\n",
      "  Training Loss: 0.5828031897544861, Training Accuracy: 0.7574957609176636\n",
      "  Validation Loss: 0.6118821501731873, Validation Accuracy: 0.7497888803482056\n",
      "Epoch 824:\n",
      "  Training Loss: 0.5782382488250732, Training Accuracy: 0.7594488859176636\n",
      "  Validation Loss: 0.6525558233261108, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 825:\n",
      "  Training Loss: 0.5864793658256531, Training Accuracy: 0.7582876086235046\n",
      "  Validation Loss: 0.5150360465049744, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 826:\n",
      "  Training Loss: 0.5788947343826294, Training Accuracy: 0.7610853314399719\n",
      "  Validation Loss: 0.5143144726753235, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 827:\n",
      "  Training Loss: 0.5776533484458923, Training Accuracy: 0.7596600651741028\n",
      "  Validation Loss: 0.515163004398346, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 828:\n",
      "  Training Loss: 0.589180052280426, Training Accuracy: 0.7570734620094299\n",
      "  Validation Loss: 0.5378258228302002, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 829:\n",
      "  Training Loss: 0.5831366777420044, Training Accuracy: 0.7579708695411682\n",
      "  Validation Loss: 0.6234889030456543, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 830:\n",
      "  Training Loss: 0.580561637878418, Training Accuracy: 0.7582876086235046\n",
      "  Validation Loss: 0.5542701482772827, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 831:\n",
      "  Training Loss: 0.575306236743927, Training Accuracy: 0.7582876086235046\n",
      "  Validation Loss: 0.6753127574920654, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 832:\n",
      "  Training Loss: 0.5779470801353455, Training Accuracy: 0.7580236196517944\n",
      "  Validation Loss: 0.5331518054008484, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 833:\n",
      "  Training Loss: 0.5928419828414917, Training Accuracy: 0.7535367608070374\n",
      "  Validation Loss: 0.5603092312812805, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 834:\n",
      "  Training Loss: 0.5882430076599121, Training Accuracy: 0.7591849565505981\n",
      "  Validation Loss: 0.583806037902832, Validation Accuracy: 0.779349684715271\n",
      "Epoch 835:\n",
      "  Training Loss: 0.5975019931793213, Training Accuracy: 0.755859375\n",
      "  Validation Loss: 0.5669991374015808, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 836:\n",
      "  Training Loss: 0.5882360935211182, Training Accuracy: 0.7568095326423645\n",
      "  Validation Loss: 0.5237955451011658, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 837:\n",
      "  Training Loss: 0.5850214958190918, Training Accuracy: 0.7573373913764954\n",
      "  Validation Loss: 0.5174840092658997, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 838:\n",
      "  Training Loss: 0.594806432723999, Training Accuracy: 0.7544341087341309\n",
      "  Validation Loss: 0.5392628908157349, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 839:\n",
      "  Training Loss: 0.5834314227104187, Training Accuracy: 0.7585515379905701\n",
      "  Validation Loss: 0.5543950200080872, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 840:\n",
      "  Training Loss: 0.5791759490966797, Training Accuracy: 0.757865309715271\n",
      "  Validation Loss: 0.5853251814842224, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 841:\n",
      "  Training Loss: 0.5744420886039734, Training Accuracy: 0.7639886140823364\n",
      "  Validation Loss: 0.5148664116859436, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 842:\n",
      "  Training Loss: 0.5793232321739197, Training Accuracy: 0.7609797120094299\n",
      "  Validation Loss: 0.5916661024093628, Validation Accuracy: 0.7776604890823364\n",
      "Epoch 843:\n",
      "  Training Loss: 0.584235668182373, Training Accuracy: 0.7589738368988037\n",
      "  Validation Loss: 0.5436290502548218, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 844:\n",
      "  Training Loss: 0.5832230448722839, Training Accuracy: 0.7600823640823364\n",
      "  Validation Loss: 0.5811201333999634, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 845:\n",
      "  Training Loss: 0.5801230669021606, Training Accuracy: 0.7602407336235046\n",
      "  Validation Loss: 0.5397817492485046, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 846:\n",
      "  Training Loss: 0.5825314521789551, Training Accuracy: 0.7581819891929626\n",
      "  Validation Loss: 0.573245108127594, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 847:\n",
      "  Training Loss: 0.5871378779411316, Training Accuracy: 0.7552787065505981\n",
      "  Validation Loss: 0.5159277319908142, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 848:\n",
      "  Training Loss: 0.5813126564025879, Training Accuracy: 0.757865309715271\n",
      "  Validation Loss: 0.5183222889900208, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 849:\n",
      "  Training Loss: 0.5746985077857971, Training Accuracy: 0.7585515379905701\n",
      "  Validation Loss: 0.5952410101890564, Validation Accuracy: 0.7747043967247009\n",
      "Epoch 850:\n",
      "  Training Loss: 0.5784010887145996, Training Accuracy: 0.7593961358070374\n",
      "  Validation Loss: 0.5336688756942749, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 851:\n",
      "  Training Loss: 0.5906672477722168, Training Accuracy: 0.7560704946517944\n",
      "  Validation Loss: 0.541422963142395, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 852:\n",
      "  Training Loss: 0.5905097126960754, Training Accuracy: 0.7546452879905701\n",
      "  Validation Loss: 0.5194315314292908, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 853:\n",
      "  Training Loss: 0.5846134424209595, Training Accuracy: 0.7608213424682617\n",
      "  Validation Loss: 0.5205843448638916, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 854:\n",
      "  Training Loss: 0.5859452486038208, Training Accuracy: 0.7559649348258972\n",
      "  Validation Loss: 1.0881234407424927, Validation Accuracy: 0.3243243098258972\n",
      "Epoch 855:\n",
      "  Training Loss: 0.5811524391174316, Training Accuracy: 0.7564927935600281\n",
      "  Validation Loss: 0.7218576073646545, Validation Accuracy: 0.5099239945411682\n",
      "Epoch 856:\n",
      "  Training Loss: 0.5764531493186951, Training Accuracy: 0.7618771195411682\n",
      "  Validation Loss: 0.513771653175354, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 857:\n",
      "  Training Loss: 0.5809851884841919, Training Accuracy: 0.7618243098258972\n",
      "  Validation Loss: 0.572198748588562, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 858:\n",
      "  Training Loss: 0.5823334455490112, Training Accuracy: 0.7588154673576355\n",
      "  Validation Loss: 0.5141193866729736, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 859:\n",
      "  Training Loss: 0.5805422067642212, Training Accuracy: 0.7593433260917664\n",
      "  Validation Loss: 0.5139262676239014, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 860:\n",
      "  Training Loss: 0.5799873471260071, Training Accuracy: 0.7604518532752991\n",
      "  Validation Loss: 0.5142897367477417, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 861:\n",
      "  Training Loss: 0.5824756622314453, Training Accuracy: 0.7589210271835327\n",
      "  Validation Loss: 0.5137331485748291, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 862:\n",
      "  Training Loss: 0.576210081577301, Training Accuracy: 0.7608213424682617\n",
      "  Validation Loss: 0.5148510932922363, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 863:\n",
      "  Training Loss: 0.5877127647399902, Training Accuracy: 0.7585515379905701\n",
      "  Validation Loss: 0.5732201337814331, Validation Accuracy: 0.7797719836235046\n",
      "Epoch 864:\n",
      "  Training Loss: 0.5894812345504761, Training Accuracy: 0.759712815284729\n",
      "  Validation Loss: 0.5262817740440369, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 865:\n",
      "  Training Loss: 0.5807580351829529, Training Accuracy: 0.7579708695411682\n",
      "  Validation Loss: 0.9295006990432739, Validation Accuracy: 0.36190879344940186\n",
      "Epoch 866:\n",
      "  Training Loss: 0.5796483755111694, Training Accuracy: 0.757759690284729\n",
      "  Validation Loss: 0.5532602071762085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 867:\n",
      "  Training Loss: 0.5746847987174988, Training Accuracy: 0.759818434715271\n",
      "  Validation Loss: 0.5477110147476196, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 868:\n",
      "  Training Loss: 0.5802538990974426, Training Accuracy: 0.7561761140823364\n",
      "  Validation Loss: 0.5138681530952454, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 869:\n",
      "  Training Loss: 0.5815392732620239, Training Accuracy: 0.7586042881011963\n",
      "  Validation Loss: 0.528647780418396, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 870:\n",
      "  Training Loss: 0.5835068821907043, Training Accuracy: 0.7587626576423645\n",
      "  Validation Loss: 0.5657271146774292, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 871:\n",
      "  Training Loss: 0.5787630677223206, Training Accuracy: 0.7587626576423645\n",
      "  Validation Loss: 0.5297141671180725, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 872:\n",
      "  Training Loss: 0.5744830369949341, Training Accuracy: 0.7610853314399719\n",
      "  Validation Loss: 0.5213029980659485, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 873:\n",
      "  Training Loss: 0.5719237923622131, Training Accuracy: 0.7625633478164673\n",
      "  Validation Loss: 0.5594781637191772, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 874:\n",
      "  Training Loss: 0.5836737751960754, Training Accuracy: 0.7563344836235046\n",
      "  Validation Loss: 0.5325538516044617, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 875:\n",
      "  Training Loss: 0.5747815370559692, Training Accuracy: 0.7581819891929626\n",
      "  Validation Loss: 0.535153329372406, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 876:\n",
      "  Training Loss: 0.5808029174804688, Training Accuracy: 0.7598711848258972\n",
      "  Validation Loss: 0.5199151039123535, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 877:\n",
      "  Training Loss: 0.5848118662834167, Training Accuracy: 0.7571262717247009\n",
      "  Validation Loss: 0.5145061612129211, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 878:\n",
      "  Training Loss: 0.5780500173568726, Training Accuracy: 0.7587626576423645\n",
      "  Validation Loss: 0.7847110629081726, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 879:\n",
      "  Training Loss: 0.5817146897315979, Training Accuracy: 0.7567039728164673\n",
      "  Validation Loss: 0.5361306071281433, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 880:\n",
      "  Training Loss: 0.5739030838012695, Training Accuracy: 0.7606630325317383\n",
      "  Validation Loss: 0.519096314907074, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 881:\n",
      "  Training Loss: 0.5749711990356445, Training Accuracy: 0.7610325217247009\n",
      "  Validation Loss: 0.5153757333755493, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 882:\n",
      "  Training Loss: 0.5773869752883911, Training Accuracy: 0.7586570978164673\n",
      "  Validation Loss: 0.5138864517211914, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 883:\n",
      "  Training Loss: 0.576865553855896, Training Accuracy: 0.7591849565505981\n",
      "  Validation Loss: 0.5136265158653259, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 884:\n",
      "  Training Loss: 0.5790789723396301, Training Accuracy: 0.7582876086235046\n",
      "  Validation Loss: 0.5469232201576233, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 885:\n",
      "  Training Loss: 0.587922990322113, Training Accuracy: 0.7546452879905701\n",
      "  Validation Loss: 0.577915370464325, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 886:\n",
      "  Training Loss: 0.5824903845787048, Training Accuracy: 0.7607685923576355\n",
      "  Validation Loss: 0.5235044360160828, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 887:\n",
      "  Training Loss: 0.5829218029975891, Training Accuracy: 0.7599767446517944\n",
      "  Validation Loss: 0.887313961982727, Validation Accuracy: 0.3781672418117523\n",
      "Epoch 888:\n",
      "  Training Loss: 0.5814948678016663, Training Accuracy: 0.7572846412658691\n",
      "  Validation Loss: 0.5138547420501709, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 889:\n",
      "  Training Loss: 0.5722762942314148, Training Accuracy: 0.7628800868988037\n",
      "  Validation Loss: 0.5185251235961914, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 890:\n",
      "  Training Loss: 0.5865799188613892, Training Accuracy: 0.7589210271835327\n",
      "  Validation Loss: 0.5434603095054626, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 891:\n",
      "  Training Loss: 0.5767520070075989, Training Accuracy: 0.7587099075317383\n",
      "  Validation Loss: 0.698399543762207, Validation Accuracy: 0.5494087934494019\n",
      "Epoch 892:\n",
      "  Training Loss: 0.5739351511001587, Training Accuracy: 0.7599767446517944\n",
      "  Validation Loss: 0.518090546131134, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 893:\n",
      "  Training Loss: 0.5759634375572205, Training Accuracy: 0.7585515379905701\n",
      "  Validation Loss: 0.5255429148674011, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 894:\n",
      "  Training Loss: 0.5782015323638916, Training Accuracy: 0.7592905163764954\n",
      "  Validation Loss: 0.5261617302894592, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 895:\n",
      "  Training Loss: 0.5823566317558289, Training Accuracy: 0.7582347989082336\n",
      "  Validation Loss: 0.6371810436248779, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 896:\n",
      "  Training Loss: 0.5834415555000305, Training Accuracy: 0.7595545053482056\n",
      "  Validation Loss: 0.6773704290390015, Validation Accuracy: 0.5884712934494019\n",
      "Epoch 897:\n",
      "  Training Loss: 0.57779860496521, Training Accuracy: 0.7593961358070374\n",
      "  Validation Loss: 0.5360184907913208, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 898:\n",
      "  Training Loss: 0.5814745426177979, Training Accuracy: 0.7608213424682617\n",
      "  Validation Loss: 0.5354524850845337, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 899:\n",
      "  Training Loss: 0.5749226808547974, Training Accuracy: 0.759818434715271\n",
      "  Validation Loss: 0.5214892029762268, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 900:\n",
      "  Training Loss: 0.5820729732513428, Training Accuracy: 0.7565984129905701\n",
      "  Validation Loss: 0.513608455657959, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 901:\n",
      "  Training Loss: 0.5933002233505249, Training Accuracy: 0.75390625\n",
      "  Validation Loss: 0.5305835008621216, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 902:\n",
      "  Training Loss: 0.5878816246986389, Training Accuracy: 0.7575485706329346\n",
      "  Validation Loss: 0.5295775532722473, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 903:\n",
      "  Training Loss: 0.575142502784729, Training Accuracy: 0.7587626576423645\n",
      "  Validation Loss: 0.6439714431762695, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 904:\n",
      "  Training Loss: 0.5848726630210876, Training Accuracy: 0.7577069401741028\n",
      "  Validation Loss: 0.5258657932281494, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 905:\n",
      "  Training Loss: 0.5783132314682007, Training Accuracy: 0.7592905163764954\n",
      "  Validation Loss: 0.5146342515945435, Validation Accuracy: 0.7829391956329346\n",
      "Epoch 906:\n",
      "  Training Loss: 0.5791559815406799, Training Accuracy: 0.7587099075317383\n",
      "  Validation Loss: 0.5135092735290527, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 907:\n",
      "  Training Loss: 0.5751977562904358, Training Accuracy: 0.7604518532752991\n",
      "  Validation Loss: 0.5213103890419006, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 908:\n",
      "  Training Loss: 0.5700556039810181, Training Accuracy: 0.7630912065505981\n",
      "  Validation Loss: 0.8353585004806519, Validation Accuracy: 0.39801520109176636\n",
      "Epoch 909:\n",
      "  Training Loss: 0.5771200656890869, Training Accuracy: 0.7563872337341309\n",
      "  Validation Loss: 0.707677960395813, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 910:\n",
      "  Training Loss: 0.5836156606674194, Training Accuracy: 0.7579180598258972\n",
      "  Validation Loss: 0.663399875164032, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 911:\n",
      "  Training Loss: 0.5794132947921753, Training Accuracy: 0.7574957609176636\n",
      "  Validation Loss: 0.8300551772117615, Validation Accuracy: 0.4035050570964813\n",
      "Epoch 912:\n",
      "  Training Loss: 0.5712498426437378, Training Accuracy: 0.7633023858070374\n",
      "  Validation Loss: 0.5795765519142151, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 913:\n",
      "  Training Loss: 0.5871517062187195, Training Accuracy: 0.7588154673576355\n",
      "  Validation Loss: 0.530523955821991, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 914:\n",
      "  Training Loss: 0.5745841264724731, Training Accuracy: 0.7614020109176636\n",
      "  Validation Loss: 0.5406660437583923, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 915:\n",
      "  Training Loss: 0.5793886184692383, Training Accuracy: 0.7592377662658691\n",
      "  Validation Loss: 0.534330427646637, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 916:\n",
      "  Training Loss: 0.5742897987365723, Training Accuracy: 0.7594488859176636\n",
      "  Validation Loss: 0.5231119394302368, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 917:\n",
      "  Training Loss: 0.5770267844200134, Training Accuracy: 0.7601351141929626\n",
      "  Validation Loss: 0.5157186388969421, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 918:\n",
      "  Training Loss: 0.5798218846321106, Training Accuracy: 0.7593433260917664\n",
      "  Validation Loss: 0.5137211084365845, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 919:\n",
      "  Training Loss: 0.5861021280288696, Training Accuracy: 0.7559649348258972\n",
      "  Validation Loss: 0.713172197341919, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 920:\n",
      "  Training Loss: 0.5721787214279175, Training Accuracy: 0.7586570978164673\n",
      "  Validation Loss: 0.5427475571632385, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 921:\n",
      "  Training Loss: 0.5693694353103638, Training Accuracy: 0.763671875\n",
      "  Validation Loss: 0.5251975059509277, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 922:\n",
      "  Training Loss: 0.5756909847259521, Training Accuracy: 0.7594488859176636\n",
      "  Validation Loss: 0.5680707693099976, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 923:\n",
      "  Training Loss: 0.5769465565681458, Training Accuracy: 0.7589738368988037\n",
      "  Validation Loss: 0.5438012480735779, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 924:\n",
      "  Training Loss: 0.5802534222602844, Training Accuracy: 0.7626161575317383\n",
      "  Validation Loss: 0.563942551612854, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 925:\n",
      "  Training Loss: 0.5805608630180359, Training Accuracy: 0.7587626576423645\n",
      "  Validation Loss: 0.5440029501914978, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 926:\n",
      "  Training Loss: 0.5734617114067078, Training Accuracy: 0.7607157826423645\n",
      "  Validation Loss: 0.5146980285644531, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 927:\n",
      "  Training Loss: 0.5861779451370239, Training Accuracy: 0.7581292390823364\n",
      "  Validation Loss: 0.5763408541679382, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 928:\n",
      "  Training Loss: 0.5758410692214966, Training Accuracy: 0.7603990435600281\n",
      "  Validation Loss: 0.5134162902832031, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 929:\n",
      "  Training Loss: 0.5769364833831787, Training Accuracy: 0.7606630325317383\n",
      "  Validation Loss: 0.5582307577133179, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 930:\n",
      "  Training Loss: 0.5798267126083374, Training Accuracy: 0.7582347989082336\n",
      "  Validation Loss: 0.5268561840057373, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 931:\n",
      "  Training Loss: 0.5797626972198486, Training Accuracy: 0.7625633478164673\n",
      "  Validation Loss: 0.5318745374679565, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 932:\n",
      "  Training Loss: 0.5807649493217468, Training Accuracy: 0.7600295543670654\n",
      "  Validation Loss: 0.522373378276825, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 933:\n",
      "  Training Loss: 0.5725062489509583, Training Accuracy: 0.7601351141929626\n",
      "  Validation Loss: 0.5274046063423157, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 934:\n",
      "  Training Loss: 0.5726380944252014, Training Accuracy: 0.7615603804588318\n",
      "  Validation Loss: 0.6122888326644897, Validation Accuracy: 0.7451435923576355\n",
      "Epoch 935:\n",
      "  Training Loss: 0.5809096097946167, Training Accuracy: 0.7589738368988037\n",
      "  Validation Loss: 0.5476239919662476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 936:\n",
      "  Training Loss: 0.5756314992904663, Training Accuracy: 0.7630912065505981\n",
      "  Validation Loss: 0.8861516714096069, Validation Accuracy: 0.37922295928001404\n",
      "Epoch 937:\n",
      "  Training Loss: 0.5843803882598877, Training Accuracy: 0.7563872337341309\n",
      "  Validation Loss: 0.5796472430229187, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 938:\n",
      "  Training Loss: 0.5691567063331604, Training Accuracy: 0.7613492608070374\n",
      "  Validation Loss: 0.5752781629562378, Validation Accuracy: 0.7801942825317383\n",
      "Epoch 939:\n",
      "  Training Loss: 0.5689073801040649, Training Accuracy: 0.761771559715271\n",
      "  Validation Loss: 0.5137333273887634, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 940:\n",
      "  Training Loss: 0.5728982090950012, Training Accuracy: 0.7595545053482056\n",
      "  Validation Loss: 0.5563904047012329, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 941:\n",
      "  Training Loss: 0.5748075246810913, Training Accuracy: 0.7619826793670654\n",
      "  Validation Loss: 0.6275596618652344, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 942:\n",
      "  Training Loss: 0.5780224204063416, Training Accuracy: 0.7611908912658691\n",
      "  Validation Loss: 0.5170373916625977, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 943:\n",
      "  Training Loss: 0.5831164717674255, Training Accuracy: 0.7620354890823364\n",
      "  Validation Loss: 0.5260182023048401, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 944:\n",
      "  Training Loss: 0.5716878175735474, Training Accuracy: 0.7619298696517944\n",
      "  Validation Loss: 0.5412185788154602, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 945:\n",
      "  Training Loss: 0.5801839232444763, Training Accuracy: 0.7598711848258972\n",
      "  Validation Loss: 0.5337024331092834, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 946:\n",
      "  Training Loss: 0.577440619468689, Training Accuracy: 0.7595016956329346\n",
      "  Validation Loss: 0.549326479434967, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 947:\n",
      "  Training Loss: 0.5733432769775391, Training Accuracy: 0.7604518532752991\n",
      "  Validation Loss: 0.517797589302063, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 948:\n",
      "  Training Loss: 0.5638746619224548, Training Accuracy: 0.7646220326423645\n",
      "  Validation Loss: 0.5391181707382202, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 949:\n",
      "  Training Loss: 0.5822599530220032, Training Accuracy: 0.7595016956329346\n",
      "  Validation Loss: 0.5469943284988403, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 950:\n",
      "  Training Loss: 0.5749306678771973, Training Accuracy: 0.7606102228164673\n",
      "  Validation Loss: 0.5138514041900635, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 951:\n",
      "  Training Loss: 0.5674821734428406, Training Accuracy: 0.7662056684494019\n",
      "  Validation Loss: 0.5387066602706909, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 952:\n",
      "  Training Loss: 0.5844082236289978, Training Accuracy: 0.7540646195411682\n",
      "  Validation Loss: 0.5384463667869568, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 953:\n",
      "  Training Loss: 0.5798994302749634, Training Accuracy: 0.7592377662658691\n",
      "  Validation Loss: 0.6166195273399353, Validation Accuracy: 0.7407094836235046\n",
      "Epoch 954:\n",
      "  Training Loss: 0.5729953050613403, Training Accuracy: 0.7621410489082336\n",
      "  Validation Loss: 0.5670377612113953, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 955:\n",
      "  Training Loss: 0.5879342555999756, Training Accuracy: 0.7584987282752991\n",
      "  Validation Loss: 0.5256924033164978, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 956:\n",
      "  Training Loss: 0.5856081247329712, Training Accuracy: 0.755912184715271\n",
      "  Validation Loss: 0.5134382247924805, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 957:\n",
      "  Training Loss: 0.5868472456932068, Training Accuracy: 0.7577069401741028\n",
      "  Validation Loss: 0.5272712707519531, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 958:\n",
      "  Training Loss: 0.566547691822052, Training Accuracy: 0.7645692825317383\n",
      "  Validation Loss: 0.5133787393569946, Validation Accuracy: 0.7825168967247009\n",
      "Epoch 959:\n",
      "  Training Loss: 0.5735812187194824, Training Accuracy: 0.7599239945411682\n",
      "  Validation Loss: 0.5469457507133484, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 960:\n",
      "  Training Loss: 0.5704339146614075, Training Accuracy: 0.7570734620094299\n",
      "  Validation Loss: 0.6712273955345154, Validation Accuracy: 0.5952280163764954\n",
      "Epoch 961:\n",
      "  Training Loss: 0.5730682015419006, Training Accuracy: 0.7614548206329346\n",
      "  Validation Loss: 0.6103205680847168, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 962:\n",
      "  Training Loss: 0.5764405727386475, Training Accuracy: 0.7594488859176636\n",
      "  Validation Loss: 0.520159125328064, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 963:\n",
      "  Training Loss: 0.5801345109939575, Training Accuracy: 0.7608213424682617\n",
      "  Validation Loss: 1.5354366302490234, Validation Accuracy: 0.27597129344940186\n",
      "Epoch 964:\n",
      "  Training Loss: 0.5723600387573242, Training Accuracy: 0.7584459185600281\n",
      "  Validation Loss: 0.5138261318206787, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 965:\n",
      "  Training Loss: 0.5710039138793945, Training Accuracy: 0.7613492608070374\n",
      "  Validation Loss: 0.5143512487411499, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 966:\n",
      "  Training Loss: 0.5746157765388489, Training Accuracy: 0.7613492608070374\n",
      "  Validation Loss: 0.5775368809700012, Validation Accuracy: 0.7799831032752991\n",
      "Epoch 967:\n",
      "  Training Loss: 0.5820204019546509, Training Accuracy: 0.7565456032752991\n",
      "  Validation Loss: 0.5273738503456116, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 968:\n",
      "  Training Loss: 0.5654813647270203, Training Accuracy: 0.7607685923576355\n",
      "  Validation Loss: 0.5528504848480225, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 969:\n",
      "  Training Loss: 0.5711938142776489, Training Accuracy: 0.7629328370094299\n",
      "  Validation Loss: 0.7336686849594116, Validation Accuracy: 0.4900760054588318\n",
      "Epoch 970:\n",
      "  Training Loss: 0.5682638883590698, Training Accuracy: 0.763724684715271\n",
      "  Validation Loss: 0.5141870975494385, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 971:\n",
      "  Training Loss: 0.5747014880180359, Training Accuracy: 0.7611380815505981\n",
      "  Validation Loss: 0.7693487405776978, Validation Accuracy: 0.45080235600471497\n",
      "Epoch 972:\n",
      "  Training Loss: 0.5833627581596375, Training Accuracy: 0.7590265870094299\n",
      "  Validation Loss: 0.5591198801994324, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 973:\n",
      "  Training Loss: 0.589655876159668, Training Accuracy: 0.7548564076423645\n",
      "  Validation Loss: 0.5633240342140198, Validation Accuracy: 0.7814611196517944\n",
      "Epoch 974:\n",
      "  Training Loss: 0.5804557800292969, Training Accuracy: 0.7562288641929626\n",
      "  Validation Loss: 0.5748304128646851, Validation Accuracy: 0.7810388803482056\n",
      "Epoch 975:\n",
      "  Training Loss: 0.5736432671546936, Training Accuracy: 0.7635663151741028\n",
      "  Validation Loss: 0.5182148218154907, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 976:\n",
      "  Training Loss: 0.5698921084403992, Training Accuracy: 0.7638302445411682\n",
      "  Validation Loss: 0.5987264513969421, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 977:\n",
      "  Training Loss: 0.5699275135993958, Training Accuracy: 0.7606102228164673\n",
      "  Validation Loss: 0.529621422290802, Validation Accuracy: 0.7818834185600281\n",
      "Epoch 978:\n",
      "  Training Loss: 0.5658742189407349, Training Accuracy: 0.7621938586235046\n",
      "  Validation Loss: 0.6017842292785645, Validation Accuracy: 0.7662584185600281\n",
      "Epoch 979:\n",
      "  Training Loss: 0.567787766456604, Training Accuracy: 0.7628272771835327\n",
      "  Validation Loss: 0.5151491165161133, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 980:\n",
      "  Training Loss: 0.5805999040603638, Training Accuracy: 0.7601879239082336\n",
      "  Validation Loss: 0.5242971777915955, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 981:\n",
      "  Training Loss: 0.5780717730522156, Training Accuracy: 0.759712815284729\n",
      "  Validation Loss: 0.517311692237854, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 982:\n",
      "  Training Loss: 0.5745545029640198, Training Accuracy: 0.759765625\n",
      "  Validation Loss: 0.8142255544662476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 983:\n",
      "  Training Loss: 0.5772735476493835, Training Accuracy: 0.7602934837341309\n",
      "  Validation Loss: 0.6450168490409851, Validation Accuracy: 0.6632179021835327\n",
      "Epoch 984:\n",
      "  Training Loss: 0.5779077410697937, Training Accuracy: 0.7598711848258972\n",
      "  Validation Loss: 0.5599005818367004, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 985:\n",
      "  Training Loss: 0.5790557861328125, Training Accuracy: 0.7609797120094299\n",
      "  Validation Loss: 0.5166414380073547, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 986:\n",
      "  Training Loss: 0.5736363530158997, Training Accuracy: 0.7606102228164673\n",
      "  Validation Loss: 0.5379153490066528, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 987:\n",
      "  Training Loss: 0.5740692615509033, Training Accuracy: 0.7610853314399719\n",
      "  Validation Loss: 0.5134825110435486, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 988:\n",
      "  Training Loss: 0.5688834190368652, Training Accuracy: 0.761771559715271\n",
      "  Validation Loss: 0.513623833656311, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 989:\n",
      "  Training Loss: 0.5733410120010376, Training Accuracy: 0.7622466087341309\n",
      "  Validation Loss: 0.7425817251205444, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 990:\n",
      "  Training Loss: 0.5796830654144287, Training Accuracy: 0.7574957609176636\n",
      "  Validation Loss: 0.5434568524360657, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 991:\n",
      "  Training Loss: 0.5809773206710815, Training Accuracy: 0.7599239945411682\n",
      "  Validation Loss: 0.6530226469039917, Validation Accuracy: 0.6393581032752991\n",
      "Epoch 992:\n",
      "  Training Loss: 0.576688289642334, Training Accuracy: 0.7615603804588318\n",
      "  Validation Loss: 0.5260393619537354, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 993:\n",
      "  Training Loss: 0.5772446990013123, Training Accuracy: 0.7605046629905701\n",
      "  Validation Loss: 0.5744620561599731, Validation Accuracy: 0.78125\n",
      "Epoch 994:\n",
      "  Training Loss: 0.5801131725311279, Training Accuracy: 0.7593961358070374\n",
      "  Validation Loss: 0.5167980194091797, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 995:\n",
      "  Training Loss: 0.5707877278327942, Training Accuracy: 0.7630912065505981\n",
      "  Validation Loss: 0.5206329226493835, Validation Accuracy: 0.7823057174682617\n",
      "Epoch 996:\n",
      "  Training Loss: 0.5766580700874329, Training Accuracy: 0.7612964510917664\n",
      "  Validation Loss: 0.5680810809135437, Validation Accuracy: 0.7816722989082336\n",
      "Epoch 997:\n",
      "  Training Loss: 0.5663464665412903, Training Accuracy: 0.7624577879905701\n",
      "  Validation Loss: 0.5139645338058472, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 998:\n",
      "  Training Loss: 0.5730772018432617, Training Accuracy: 0.7614020109176636\n",
      "  Validation Loss: 0.5158413052558899, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 999:\n",
      "  Training Loss: 0.5738588571548462, Training Accuracy: 0.7611908912658691\n",
      "  Validation Loss: 0.5812580585479736, Validation Accuracy: 0.7804054021835327\n",
      "Epoch 1000:\n",
      "  Training Loss: 0.5761573910713196, Training Accuracy: 0.7586042881011963\n",
      "  Validation Loss: 0.5419520139694214, Validation Accuracy: 0.7816722989082336\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(len(train_loss)):\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Training Loss: {train_loss[epoch]}, Training Accuracy: {train_accuracy[epoch]}\")\n",
    "    print(f\"  Validation Loss: {valid_loss[epoch]}, Validation Accuracy: {valid_accuracy[epoch]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(5, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9,\n",
    "                                     beta_2=0.999)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "592/592 [==============================] - 2s 2ms/step - loss: 294.7612 - accuracy: 0.7291 - val_loss: 0.6120 - val_accuracy: 0.7663\n",
      "Epoch 2/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6819 - accuracy: 0.7075 - val_loss: 0.5894 - val_accuracy: 0.7827\n",
      "Epoch 3/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7737 - accuracy: 0.7243 - val_loss: 0.7860 - val_accuracy: 0.7827\n",
      "Epoch 4/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.7741 - accuracy: 0.7113 - val_loss: 0.7506 - val_accuracy: 0.7827\n",
      "Epoch 5/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6196 - accuracy: 0.7531 - val_loss: 0.6710 - val_accuracy: 0.7827\n",
      "Epoch 6/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6343 - accuracy: 0.7501 - val_loss: 0.5316 - val_accuracy: 0.7827\n",
      "Epoch 7/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.6294 - accuracy: 0.7503 - val_loss: 0.5114 - val_accuracy: 0.7827\n",
      "Epoch 8/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5722 - accuracy: 0.7668 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 9/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7723 - val_loss: 0.5088 - val_accuracy: 0.7827\n",
      "Epoch 10/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5265 - accuracy: 0.7759 - val_loss: 0.5129 - val_accuracy: 0.7827\n",
      "Epoch 11/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5218 - accuracy: 0.7759 - val_loss: 0.5146 - val_accuracy: 0.7827\n",
      "Epoch 12/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5241 - accuracy: 0.7759 - val_loss: 0.5090 - val_accuracy: 0.7827\n",
      "Epoch 13/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5223 - accuracy: 0.7759 - val_loss: 0.5147 - val_accuracy: 0.7827\n",
      "Epoch 14/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5245 - accuracy: 0.7759 - val_loss: 0.5085 - val_accuracy: 0.7827\n",
      "Epoch 15/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5271 - accuracy: 0.7759 - val_loss: 0.5280 - val_accuracy: 0.7827\n",
      "Epoch 16/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5327 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 17/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 18/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 19/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 20/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 21/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 22/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 23/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 24/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 25/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 26/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 27/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 28/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 29/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 30/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 31/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 32/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 33/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 34/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 35/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 36/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 37/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 38/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 39/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 40/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 41/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 42/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 43/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 44/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 45/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 46/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 47/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 48/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 49/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 50/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 51/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 52/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 53/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 54/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 55/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 56/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 57/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 58/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 59/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 60/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 61/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 62/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 63/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 64/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 65/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 66/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 67/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 68/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 69/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 70/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 71/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 72/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 73/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 74/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 75/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 76/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 77/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 78/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 79/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 80/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 81/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 82/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 83/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 84/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 85/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 86/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 87/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 88/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 89/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 90/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 91/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 92/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 93/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 94/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 95/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 96/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 97/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 98/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 99/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 100/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 101/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 102/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 103/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 104/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 105/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 106/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 107/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 108/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 109/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 110/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 111/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 112/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 113/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 114/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 115/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 116/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 117/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 118/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 119/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 120/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 121/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 122/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 123/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 124/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 125/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 126/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 127/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 128/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 129/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 130/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 131/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 132/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 133/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 134/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 135/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 136/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 137/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 138/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 139/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 140/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 141/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 142/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 143/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 144/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 145/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 146/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 147/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 148/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 149/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 150/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 151/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 152/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 153/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 154/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 155/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 156/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 157/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 158/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 159/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 160/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 161/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 162/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5240 - val_accuracy: 0.7827\n",
      "Epoch 163/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 164/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 165/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 166/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 167/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 168/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 169/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 170/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 171/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 172/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 173/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 174/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 175/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 176/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 177/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 178/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 179/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 180/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 181/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 182/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 183/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 184/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 185/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 186/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 187/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 188/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 189/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 190/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 191/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 192/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 193/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 194/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 195/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 196/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 197/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 198/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 199/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 200/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 201/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 202/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 203/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 204/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 205/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 206/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 207/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 208/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5239 - val_accuracy: 0.7827\n",
      "Epoch 209/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 210/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 211/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 212/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 213/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 214/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 215/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 216/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 217/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 218/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5239 - val_accuracy: 0.7827\n",
      "Epoch 219/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 220/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 221/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 222/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 223/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 224/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 225/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 226/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 227/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 228/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 229/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 230/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 231/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 232/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 233/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 234/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 235/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 236/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 237/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 238/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 239/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 240/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 241/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 242/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 243/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 244/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 245/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 246/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 247/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 248/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 249/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 250/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 251/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 252/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 253/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 254/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 255/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 256/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 257/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 258/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 259/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 260/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 261/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 262/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 263/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 264/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 265/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 266/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 267/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 268/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 269/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 270/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 271/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 272/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 273/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 274/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 275/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 276/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 277/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 278/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 279/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 280/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 281/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 282/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 283/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 284/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 285/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 286/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 287/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 288/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 289/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 290/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 291/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 292/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 293/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 294/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 295/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 296/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 297/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 298/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 299/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 300/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 301/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 302/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 303/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 304/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 305/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 306/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 307/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 308/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 309/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 310/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 311/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 312/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 313/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 314/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 315/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 316/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 317/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 318/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 319/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 320/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 321/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 322/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 323/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 324/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 325/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 326/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 327/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 328/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 329/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 330/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 331/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 332/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 333/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 334/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 335/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 336/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 337/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 338/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 339/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 340/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 341/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 342/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 343/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 344/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 345/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 346/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 347/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 348/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 349/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 350/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 351/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 352/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 353/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 354/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 355/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 356/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 357/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 358/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 359/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 360/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 361/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 362/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 363/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 364/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 365/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 366/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 367/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 368/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 369/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 370/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 371/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 372/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 373/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 374/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 375/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 376/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 377/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 378/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 379/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 380/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 381/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 382/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 383/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 384/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 385/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 386/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 387/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 388/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 389/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 390/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 391/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 392/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 393/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 394/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 395/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 396/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 397/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 398/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 399/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 400/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 401/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 402/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 403/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 404/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 405/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 406/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 407/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 408/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 409/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 410/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 411/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 412/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 413/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 414/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 415/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 416/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 417/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 418/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 419/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 420/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 421/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 422/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 423/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 424/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 425/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 426/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 427/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 428/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 429/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 430/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 431/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 432/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 433/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 434/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 435/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 436/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 437/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 438/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 439/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 440/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 441/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 442/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 443/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 444/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 445/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 446/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 447/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 448/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 449/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 450/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 451/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 452/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 453/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 454/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 455/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 456/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 457/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 458/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 459/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 460/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 461/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 462/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 463/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 464/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 465/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 466/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 467/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 468/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 469/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 470/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 471/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 472/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 473/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 474/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 475/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 476/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 477/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 478/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 479/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 480/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 481/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 482/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 483/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 484/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 485/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 486/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 487/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 488/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 489/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 490/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 491/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 492/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 493/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 494/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 495/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 496/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 497/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 498/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 499/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 500/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 501/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 502/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 503/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 504/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 505/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 506/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 507/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 508/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 509/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 510/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 511/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 512/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 513/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 514/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 515/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 516/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 517/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 518/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 519/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 520/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 521/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 522/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 523/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 524/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 525/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 526/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 527/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 528/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 529/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 530/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 531/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5239 - val_accuracy: 0.7827\n",
      "Epoch 532/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 533/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 534/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 535/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 536/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 537/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 538/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 539/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 540/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 541/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 542/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 543/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 544/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 545/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 546/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 547/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 548/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 549/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 550/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 551/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 552/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 553/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 554/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 555/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 556/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 557/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 558/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 559/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 560/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 561/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 562/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 563/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5320 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 564/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 565/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 566/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 567/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 568/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 569/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 570/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 571/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 572/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 573/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 574/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 575/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 576/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 577/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 578/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 579/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 580/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 581/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 582/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 583/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 584/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 585/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 586/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 587/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 588/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 589/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 590/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 591/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 592/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 593/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 594/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 595/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 596/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 597/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 598/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 599/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 600/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 601/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 602/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 603/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 604/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 605/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 606/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 607/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 608/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 609/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 610/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 611/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 612/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 613/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 614/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 615/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 616/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 617/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 618/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 619/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 620/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 621/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 622/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 623/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 624/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 625/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 626/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 627/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 628/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 629/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 630/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 631/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 632/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 633/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 634/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 635/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 636/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 637/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 638/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 639/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 640/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5239 - val_accuracy: 0.7827\n",
      "Epoch 641/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 642/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 643/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 644/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 645/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 646/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 647/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 648/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 649/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 650/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 651/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 652/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 653/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 654/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5239 - val_accuracy: 0.7827\n",
      "Epoch 655/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 656/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 657/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 658/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 659/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 660/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 661/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 662/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 663/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 664/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 665/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 666/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 667/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 668/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 669/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 670/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 671/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 672/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 673/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 674/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 675/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 676/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 677/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 678/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 679/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 680/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 681/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 682/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 683/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 684/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 685/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 686/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 687/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 688/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 689/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 690/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 691/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 692/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 693/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 694/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 695/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 696/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 697/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 698/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 699/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 700/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 701/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 702/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 703/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 704/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 705/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 706/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 707/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 708/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 709/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 710/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 711/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 712/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 713/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5239 - val_accuracy: 0.7827\n",
      "Epoch 714/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 715/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 716/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 717/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 718/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 719/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 720/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 721/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 722/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 723/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 724/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 725/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 726/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 727/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 728/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 729/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 730/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 731/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 732/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 733/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 734/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 735/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 736/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 737/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 738/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 739/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 740/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 741/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 742/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 743/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 744/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 745/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 746/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 747/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 748/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 749/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 750/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 751/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 752/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 753/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 754/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 755/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 756/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 757/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 758/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 759/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 760/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 761/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 762/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 763/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 764/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 765/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 766/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 767/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 768/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 769/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 770/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 771/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 772/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 773/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 774/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 775/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 776/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 777/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 778/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 779/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 780/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 781/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 782/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 783/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 784/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 785/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 786/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 787/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 788/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 789/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 790/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 791/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 792/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 793/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 794/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 795/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 796/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 797/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 798/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 799/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 800/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 801/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 802/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 803/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 804/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 805/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 806/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 807/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 808/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 809/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 810/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 811/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 812/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 813/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 814/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 815/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 816/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 817/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 818/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 819/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 820/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 821/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 822/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 823/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 824/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 825/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 826/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 827/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 828/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 829/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 830/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 831/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 832/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 833/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 834/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 835/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 836/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 837/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 838/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 839/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 840/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 841/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 842/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 843/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 844/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 845/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 846/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 847/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 848/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 849/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 850/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 851/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 852/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 853/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 854/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 855/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 856/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 857/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 858/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 859/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 860/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 861/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 862/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 863/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 864/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 865/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 866/1000\n",
      "592/592 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 867/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 868/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 869/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 870/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 871/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 872/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 873/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 874/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 875/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 876/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 877/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 878/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 879/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 880/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 881/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 882/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 883/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 884/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 885/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 886/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 887/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 888/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 889/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 890/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 891/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 892/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 893/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 894/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 895/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 896/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 897/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 898/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 899/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 900/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 901/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 902/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 903/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 904/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 905/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 906/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 907/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 908/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 909/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 910/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 911/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 912/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 913/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 914/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 915/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 916/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 917/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 918/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 919/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 920/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 921/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 922/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 923/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 924/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 925/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5238 - val_accuracy: 0.7827\n",
      "Epoch 926/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 927/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 928/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 929/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 930/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 931/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 932/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 933/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 934/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 935/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 936/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 937/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 938/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 939/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 940/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 941/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 942/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 943/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 944/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 945/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 946/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 947/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 948/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 949/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 950/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 951/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 952/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 953/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 954/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 955/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 956/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 957/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 958/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 959/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 960/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 961/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 962/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 963/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 964/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 965/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 966/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 967/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 968/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 969/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 970/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 971/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 972/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 973/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 974/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 975/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 976/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 977/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 978/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 979/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 980/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 981/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 982/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 983/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 984/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 985/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 986/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5320 - accuracy: 0.7759 - val_loss: 0.5234 - val_accuracy: 0.7827\n",
      "Epoch 987/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 988/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 989/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 990/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 991/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 992/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 993/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5237 - val_accuracy: 0.7827\n",
      "Epoch 994/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 995/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 996/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 997/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 998/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5236 - val_accuracy: 0.7827\n",
      "Epoch 999/1000\n",
      "592/592 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n",
      "Epoch 1000/1000\n",
      "592/592 [==============================] - 2s 3ms/step - loss: 0.5322 - accuracy: 0.7759 - val_loss: 0.5235 - val_accuracy: 0.7827\n"
     ]
    }
   ],
   "source": [
    "historyADM = model.fit(X_train, y_train, epochs=1000,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = historyADM.history['loss'] \n",
    "train_accuracy = historyADM.history['accuracy']  \n",
    "valid_loss = historyADM.history['val_loss'] \n",
    "valid_accuracy = historyADM.history['val_accuracy']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Training Loss: 294.76116943359375, Training Accuracy: 0.7290962934494019\n",
      "  Validation Loss: 0.6120092868804932, Validation Accuracy: 0.7662584185600281\n",
      "Epoch 2:\n",
      "  Training Loss: 0.6818750500679016, Training Accuracy: 0.7075063586235046\n",
      "  Validation Loss: 0.5894455313682556, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 3:\n",
      "  Training Loss: 0.7736547589302063, Training Accuracy: 0.7242926359176636\n",
      "  Validation Loss: 0.7860444188117981, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 4:\n",
      "  Training Loss: 0.7741307020187378, Training Accuracy: 0.7113069891929626\n",
      "  Validation Loss: 0.7505900263786316, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 5:\n",
      "  Training Loss: 0.6195554137229919, Training Accuracy: 0.7530616521835327\n",
      "  Validation Loss: 0.6710062026977539, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 6:\n",
      "  Training Loss: 0.6343453526496887, Training Accuracy: 0.7501055598258972\n",
      "  Validation Loss: 0.531593918800354, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 7:\n",
      "  Training Loss: 0.6293935179710388, Training Accuracy: 0.7503167390823364\n",
      "  Validation Loss: 0.5113924145698547, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 8:\n",
      "  Training Loss: 0.5721860527992249, Training Accuracy: 0.7667863368988037\n",
      "  Validation Loss: 0.523767352104187, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 9:\n",
      "  Training Loss: 0.5440412759780884, Training Accuracy: 0.7722761631011963\n",
      "  Validation Loss: 0.5088304281234741, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 10:\n",
      "  Training Loss: 0.5265399813652039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5129132866859436, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 11:\n",
      "  Training Loss: 0.5218430161476135, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5145673155784607, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 12:\n",
      "  Training Loss: 0.5241152048110962, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5090200901031494, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 13:\n",
      "  Training Loss: 0.5222571492195129, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.514670729637146, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 14:\n",
      "  Training Loss: 0.5244821310043335, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.508495032787323, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 15:\n",
      "  Training Loss: 0.5270892381668091, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5279829502105713, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 16:\n",
      "  Training Loss: 0.5326758027076721, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236677527427673, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 17:\n",
      "  Training Loss: 0.5321481823921204, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523592472076416, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 18:\n",
      "  Training Loss: 0.5321457386016846, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236655473709106, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 19:\n",
      "  Training Loss: 0.5321595668792725, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235634446144104, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 20:\n",
      "  Training Loss: 0.5321230888366699, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523493766784668, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 21:\n",
      "  Training Loss: 0.5321818590164185, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235629677772522, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 22:\n",
      "  Training Loss: 0.5321264266967773, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234712362289429, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 23:\n",
      "  Training Loss: 0.5321394205093384, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236409306526184, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 24:\n",
      "  Training Loss: 0.5321395397186279, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523676335811615, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 25:\n",
      "  Training Loss: 0.5321828126907349, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237368941307068, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 26:\n",
      "  Training Loss: 0.5321451425552368, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236420631408691, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 27:\n",
      "  Training Loss: 0.5321592688560486, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235307812690735, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 28:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235886573791504, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 29:\n",
      "  Training Loss: 0.5321484208106995, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235610008239746, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 30:\n",
      "  Training Loss: 0.5321279764175415, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235345959663391, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 31:\n",
      "  Training Loss: 0.5321636199951172, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523564338684082, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 32:\n",
      "  Training Loss: 0.5321472883224487, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236719250679016, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 33:\n",
      "  Training Loss: 0.5321241617202759, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235024690628052, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 34:\n",
      "  Training Loss: 0.5321531295776367, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235081315040588, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 35:\n",
      "  Training Loss: 0.532179057598114, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235170722007751, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 36:\n",
      "  Training Loss: 0.5321229696273804, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236482620239258, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 37:\n",
      "  Training Loss: 0.5321497917175293, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235236287117004, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 38:\n",
      "  Training Loss: 0.5321058630943298, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237472653388977, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 39:\n",
      "  Training Loss: 0.5321443676948547, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235639214515686, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 40:\n",
      "  Training Loss: 0.5321418642997742, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236471891403198, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 41:\n",
      "  Training Loss: 0.5321493744850159, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236402750015259, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 42:\n",
      "  Training Loss: 0.5321835279464722, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235762596130371, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 43:\n",
      "  Training Loss: 0.5320913195610046, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234355926513672, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 44:\n",
      "  Training Loss: 0.5321576595306396, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236063599586487, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 45:\n",
      "  Training Loss: 0.5321348309516907, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234934687614441, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 46:\n",
      "  Training Loss: 0.5321687459945679, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235663056373596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 47:\n",
      "  Training Loss: 0.5321243405342102, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235105752944946, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 48:\n",
      "  Training Loss: 0.5321398973464966, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235152244567871, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 49:\n",
      "  Training Loss: 0.532149076461792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235568284988403, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 50:\n",
      "  Training Loss: 0.5321522951126099, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235565900802612, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 51:\n",
      "  Training Loss: 0.5321457982063293, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235326290130615, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 52:\n",
      "  Training Loss: 0.5321421027183533, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236033797264099, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 53:\n",
      "  Training Loss: 0.5321491956710815, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235603451728821, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 54:\n",
      "  Training Loss: 0.532173752784729, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235362648963928, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 55:\n",
      "  Training Loss: 0.5321340560913086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235634446144104, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 56:\n",
      "  Training Loss: 0.5321428179740906, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235989689826965, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 57:\n",
      "  Training Loss: 0.5321224927902222, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235098600387573, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 58:\n",
      "  Training Loss: 0.5321478843688965, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235193371772766, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 59:\n",
      "  Training Loss: 0.532145619392395, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236523151397705, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 60:\n",
      "  Training Loss: 0.532180666923523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237018465995789, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 61:\n",
      "  Training Loss: 0.5321462750434875, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236364603042603, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 62:\n",
      "  Training Loss: 0.5321320295333862, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235640406608582, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 63:\n",
      "  Training Loss: 0.5321495532989502, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236272811889648, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 64:\n",
      "  Training Loss: 0.5321413278579712, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523487389087677, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 65:\n",
      "  Training Loss: 0.5321470499038696, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235905051231384, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 66:\n",
      "  Training Loss: 0.5321648120880127, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523632824420929, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 67:\n",
      "  Training Loss: 0.5321326851844788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523493766784668, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 68:\n",
      "  Training Loss: 0.5321292281150818, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237897634506226, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 69:\n",
      "  Training Loss: 0.5321729779243469, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235162377357483, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 70:\n",
      "  Training Loss: 0.5321168899536133, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234654545783997, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 71:\n",
      "  Training Loss: 0.5321681499481201, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235609412193298, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 72:\n",
      "  Training Loss: 0.5321581363677979, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52350252866745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 73:\n",
      "  Training Loss: 0.5321412086486816, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236045718193054, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 74:\n",
      "  Training Loss: 0.5321444869041443, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236301422119141, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 75:\n",
      "  Training Loss: 0.5320684313774109, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234440565109253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 76:\n",
      "  Training Loss: 0.5321661829948425, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235739946365356, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 77:\n",
      "  Training Loss: 0.5321511030197144, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236520767211914, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 78:\n",
      "  Training Loss: 0.5321463942527771, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235612988471985, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 79:\n",
      "  Training Loss: 0.5321504473686218, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237180590629578, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 80:\n",
      "  Training Loss: 0.5321605205535889, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523749828338623, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 81:\n",
      "  Training Loss: 0.532151997089386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234636664390564, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 82:\n",
      "  Training Loss: 0.5321733355522156, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235450267791748, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 83:\n",
      "  Training Loss: 0.5321598052978516, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235811471939087, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 84:\n",
      "  Training Loss: 0.5321323275566101, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235803723335266, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 85:\n",
      "  Training Loss: 0.5321413278579712, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235620141029358, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 86:\n",
      "  Training Loss: 0.5321217179298401, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237371921539307, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 87:\n",
      "  Training Loss: 0.5321615934371948, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234884023666382, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 88:\n",
      "  Training Loss: 0.5321550369262695, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234678387641907, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 89:\n",
      "  Training Loss: 0.5321127772331238, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523450493812561, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 90:\n",
      "  Training Loss: 0.5321438908576965, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236120820045471, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 91:\n",
      "  Training Loss: 0.5321367383003235, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236296057701111, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 92:\n",
      "  Training Loss: 0.5321564674377441, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235728621482849, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 93:\n",
      "  Training Loss: 0.5321435928344727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689878463745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 94:\n",
      "  Training Loss: 0.5321180820465088, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234883427619934, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 95:\n",
      "  Training Loss: 0.5321534276008606, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 96:\n",
      "  Training Loss: 0.5321680307388306, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236204266548157, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 97:\n",
      "  Training Loss: 0.532146155834198, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237352848052979, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 98:\n",
      "  Training Loss: 0.5321638584136963, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236295461654663, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 99:\n",
      "  Training Loss: 0.5321384072303772, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235902070999146, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 100:\n",
      "  Training Loss: 0.5321635007858276, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236086845397949, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 101:\n",
      "  Training Loss: 0.5321553945541382, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235859751701355, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 102:\n",
      "  Training Loss: 0.5321550369262695, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235578417778015, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 103:\n",
      "  Training Loss: 0.5321160554885864, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237442255020142, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 104:\n",
      "  Training Loss: 0.5321641564369202, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237721800804138, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 105:\n",
      "  Training Loss: 0.5321362614631653, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236612558364868, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 106:\n",
      "  Training Loss: 0.5321537852287292, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235605239868164, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 107:\n",
      "  Training Loss: 0.5321351885795593, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235162377357483, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 108:\n",
      "  Training Loss: 0.5321506857872009, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236640572547913, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 109:\n",
      "  Training Loss: 0.5321279764175415, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237215161323547, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 110:\n",
      "  Training Loss: 0.5321512222290039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236883759498596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 111:\n",
      "  Training Loss: 0.5321624279022217, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236356854438782, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 112:\n",
      "  Training Loss: 0.5321604609489441, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235208868980408, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 113:\n",
      "  Training Loss: 0.5321204662322998, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236161947250366, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 114:\n",
      "  Training Loss: 0.5321358442306519, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235278606414795, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 115:\n",
      "  Training Loss: 0.5321593880653381, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236343741416931, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 116:\n",
      "  Training Loss: 0.5321455001831055, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236166715621948, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 117:\n",
      "  Training Loss: 0.5321082472801208, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234889984130859, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 118:\n",
      "  Training Loss: 0.5321452021598816, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235375165939331, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 119:\n",
      "  Training Loss: 0.532163679599762, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235204696655273, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 120:\n",
      "  Training Loss: 0.5321664810180664, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523652195930481, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 121:\n",
      "  Training Loss: 0.5321744084358215, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235320925712585, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 122:\n",
      "  Training Loss: 0.5321373343467712, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236917734146118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 123:\n",
      "  Training Loss: 0.5321485996246338, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234996676445007, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 124:\n",
      "  Training Loss: 0.5321686863899231, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234994292259216, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 125:\n",
      "  Training Loss: 0.5321301221847534, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237441658973694, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 126:\n",
      "  Training Loss: 0.5321440696716309, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236914753913879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 127:\n",
      "  Training Loss: 0.5321524739265442, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236341953277588, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 128:\n",
      "  Training Loss: 0.5321725010871887, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236295461654663, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 129:\n",
      "  Training Loss: 0.5321592092514038, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235219597816467, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 130:\n",
      "  Training Loss: 0.5321469306945801, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235803723335266, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 131:\n",
      "  Training Loss: 0.5321465134620667, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235571265220642, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 132:\n",
      "  Training Loss: 0.532122790813446, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235041379928589, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 133:\n",
      "  Training Loss: 0.5321561694145203, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234991908073425, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 134:\n",
      "  Training Loss: 0.5321511030197144, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52361661195755, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 135:\n",
      "  Training Loss: 0.5321129560470581, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234598517417908, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 136:\n",
      "  Training Loss: 0.5321767926216125, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235100984573364, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 137:\n",
      "  Training Loss: 0.5321428775787354, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236788392066956, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 138:\n",
      "  Training Loss: 0.5321818590164185, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235593914985657, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 139:\n",
      "  Training Loss: 0.5321553349494934, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235558748245239, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 140:\n",
      "  Training Loss: 0.5321515798568726, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235324501991272, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 141:\n",
      "  Training Loss: 0.5321671962738037, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235065221786499, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 142:\n",
      "  Training Loss: 0.5321499705314636, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236240029335022, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 143:\n",
      "  Training Loss: 0.5321561694145203, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235368609428406, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 144:\n",
      "  Training Loss: 0.5321657657623291, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235907435417175, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 145:\n",
      "  Training Loss: 0.5321545600891113, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235142707824707, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 146:\n",
      "  Training Loss: 0.5321111679077148, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237691402435303, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 147:\n",
      "  Training Loss: 0.532180666923523, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236856937408447, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 148:\n",
      "  Training Loss: 0.5321333408355713, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234765410423279, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 149:\n",
      "  Training Loss: 0.5321440696716309, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523674488067627, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 150:\n",
      "  Training Loss: 0.5321559309959412, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235589742660522, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 151:\n",
      "  Training Loss: 0.532141387462616, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235468745231628, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 152:\n",
      "  Training Loss: 0.5321375131607056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235421061515808, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 153:\n",
      "  Training Loss: 0.5321366786956787, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234911441802979, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 154:\n",
      "  Training Loss: 0.5321444869041443, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234957337379456, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 155:\n",
      "  Training Loss: 0.5321514010429382, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236079096794128, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 156:\n",
      "  Training Loss: 0.5321646332740784, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236003994941711, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 157:\n",
      "  Training Loss: 0.5321424603462219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235471129417419, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 158:\n",
      "  Training Loss: 0.5321571826934814, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234985947608948, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 159:\n",
      "  Training Loss: 0.5321536660194397, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236144661903381, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 160:\n",
      "  Training Loss: 0.5321871042251587, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 161:\n",
      "  Training Loss: 0.5321502089500427, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236278176307678, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 162:\n",
      "  Training Loss: 0.5320735573768616, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5240328311920166, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 163:\n",
      "  Training Loss: 0.5321821570396423, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237724184989929, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 164:\n",
      "  Training Loss: 0.5321418046951294, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234697461128235, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 165:\n",
      "  Training Loss: 0.5321245193481445, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237043499946594, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 166:\n",
      "  Training Loss: 0.5321462750434875, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235269665718079, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 167:\n",
      "  Training Loss: 0.5321455597877502, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236237645149231, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 168:\n",
      "  Training Loss: 0.5321713089942932, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235656499862671, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 169:\n",
      "  Training Loss: 0.5321338176727295, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523499608039856, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 170:\n",
      "  Training Loss: 0.5321415066719055, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236612558364868, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 171:\n",
      "  Training Loss: 0.5321425199508667, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235466361045837, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 172:\n",
      "  Training Loss: 0.5321565270423889, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235437154769897, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 173:\n",
      "  Training Loss: 0.5321518778800964, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235426425933838, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 174:\n",
      "  Training Loss: 0.5321550369262695, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235915780067444, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 175:\n",
      "  Training Loss: 0.5321527719497681, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235167741775513, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 176:\n",
      "  Training Loss: 0.5321539044380188, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236423015594482, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 177:\n",
      "  Training Loss: 0.5321356058120728, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234886407852173, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 178:\n",
      "  Training Loss: 0.5320955514907837, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237703323364258, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 179:\n",
      "  Training Loss: 0.5321546792984009, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236634612083435, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 180:\n",
      "  Training Loss: 0.5321508049964905, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236220955848694, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 181:\n",
      "  Training Loss: 0.532146692276001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237029790878296, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 182:\n",
      "  Training Loss: 0.5321049690246582, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523475706577301, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 183:\n",
      "  Training Loss: 0.5321614146232605, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235293507575989, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 184:\n",
      "  Training Loss: 0.5321725010871887, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235624313354492, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 185:\n",
      "  Training Loss: 0.5321342945098877, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235938429832458, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 186:\n",
      "  Training Loss: 0.5321361422538757, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235093235969543, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 187:\n",
      "  Training Loss: 0.5321422815322876, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234889984130859, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 188:\n",
      "  Training Loss: 0.5321356654167175, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236638784408569, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 189:\n",
      "  Training Loss: 0.5321609377861023, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236278176307678, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 190:\n",
      "  Training Loss: 0.5321307182312012, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234823226928711, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 191:\n",
      "  Training Loss: 0.5321471095085144, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234823822975159, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 192:\n",
      "  Training Loss: 0.5321531295776367, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235317945480347, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 193:\n",
      "  Training Loss: 0.5321456789970398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235159993171692, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 194:\n",
      "  Training Loss: 0.532137393951416, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234720706939697, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 195:\n",
      "  Training Loss: 0.5321515798568726, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235236883163452, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 196:\n",
      "  Training Loss: 0.5321363806724548, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236225128173828, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 197:\n",
      "  Training Loss: 0.5321725010871887, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523595929145813, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 198:\n",
      "  Training Loss: 0.5321439504623413, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236443877220154, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 199:\n",
      "  Training Loss: 0.5321691632270813, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235525369644165, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 200:\n",
      "  Training Loss: 0.5321406722068787, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523711085319519, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 201:\n",
      "  Training Loss: 0.5321410298347473, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235214829444885, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 202:\n",
      "  Training Loss: 0.5321455001831055, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236417055130005, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 203:\n",
      "  Training Loss: 0.5321706533432007, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236333608627319, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 204:\n",
      "  Training Loss: 0.5321508049964905, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235849022865295, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 205:\n",
      "  Training Loss: 0.532112181186676, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523463249206543, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 206:\n",
      "  Training Loss: 0.5321261286735535, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237196087837219, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 207:\n",
      "  Training Loss: 0.5321653485298157, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237094163894653, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 208:\n",
      "  Training Loss: 0.5321280360221863, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238843560218811, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 209:\n",
      "  Training Loss: 0.5321672558784485, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235493183135986, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 210:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236780643463135, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 211:\n",
      "  Training Loss: 0.5321508646011353, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235548615455627, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 212:\n",
      "  Training Loss: 0.5321207046508789, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523520827293396, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 213:\n",
      "  Training Loss: 0.5321255922317505, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237001776695251, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 214:\n",
      "  Training Loss: 0.5321722030639648, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236552953720093, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 215:\n",
      "  Training Loss: 0.5321367383003235, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236198902130127, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 216:\n",
      "  Training Loss: 0.5321360230445862, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52353435754776, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 217:\n",
      "  Training Loss: 0.5321375131607056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523673951625824, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 218:\n",
      "  Training Loss: 0.5320931673049927, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238999724388123, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 219:\n",
      "  Training Loss: 0.5321358442306519, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234735608100891, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 220:\n",
      "  Training Loss: 0.5321772694587708, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235241651535034, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 221:\n",
      "  Training Loss: 0.5321549773216248, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235041379928589, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 222:\n",
      "  Training Loss: 0.5321422219276428, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523618221282959, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 223:\n",
      "  Training Loss: 0.5321424603462219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235478281974792, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 224:\n",
      "  Training Loss: 0.5321537852287292, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236181020736694, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 225:\n",
      "  Training Loss: 0.5321586728096008, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235923528671265, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 226:\n",
      "  Training Loss: 0.5321488380432129, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234971046447754, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 227:\n",
      "  Training Loss: 0.5321518778800964, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235368609428406, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 228:\n",
      "  Training Loss: 0.5321351885795593, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237045288085938, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 229:\n",
      "  Training Loss: 0.5321175456047058, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238150358200073, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 230:\n",
      "  Training Loss: 0.5321662425994873, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237116813659668, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 231:\n",
      "  Training Loss: 0.5321558713912964, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52360600233078, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 232:\n",
      "  Training Loss: 0.5321593284606934, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235068798065186, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 233:\n",
      "  Training Loss: 0.5321561098098755, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235421657562256, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 234:\n",
      "  Training Loss: 0.5321500897407532, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235148072242737, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 235:\n",
      "  Training Loss: 0.5321429371833801, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236372351646423, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 236:\n",
      "  Training Loss: 0.5321362018585205, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236582159996033, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 237:\n",
      "  Training Loss: 0.5321647524833679, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235796570777893, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 238:\n",
      "  Training Loss: 0.5321645736694336, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236727595329285, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 239:\n",
      "  Training Loss: 0.5321363210678101, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236029624938965, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 240:\n",
      "  Training Loss: 0.5321224331855774, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235024690628052, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 241:\n",
      "  Training Loss: 0.5321388840675354, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236337780952454, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 242:\n",
      "  Training Loss: 0.5321611166000366, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236148238182068, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 243:\n",
      "  Training Loss: 0.5321341156959534, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235319137573242, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 244:\n",
      "  Training Loss: 0.5321291089057922, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236244797706604, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 245:\n",
      "  Training Loss: 0.5321537256240845, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236166715621948, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 246:\n",
      "  Training Loss: 0.5321304202079773, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235280990600586, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 247:\n",
      "  Training Loss: 0.5321318507194519, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523478627204895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 248:\n",
      "  Training Loss: 0.5321311354637146, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523658275604248, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 249:\n",
      "  Training Loss: 0.532156229019165, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235986113548279, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 250:\n",
      "  Training Loss: 0.532140851020813, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523532509803772, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 251:\n",
      "  Training Loss: 0.5321400761604309, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235463976860046, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 252:\n",
      "  Training Loss: 0.5321264266967773, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234720706939697, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 253:\n",
      "  Training Loss: 0.5321460962295532, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234721302986145, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 254:\n",
      "  Training Loss: 0.532182514667511, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234574675559998, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 255:\n",
      "  Training Loss: 0.532151997089386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236520767211914, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 256:\n",
      "  Training Loss: 0.5321577191352844, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236393213272095, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 257:\n",
      "  Training Loss: 0.5321534276008606, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237714052200317, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 258:\n",
      "  Training Loss: 0.5321460366249084, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234928131103516, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 259:\n",
      "  Training Loss: 0.5321617126464844, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235012173652649, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 260:\n",
      "  Training Loss: 0.5321716666221619, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235304236412048, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 261:\n",
      "  Training Loss: 0.5321491360664368, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235543251037598, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 262:\n",
      "  Training Loss: 0.532149076461792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236354470252991, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 263:\n",
      "  Training Loss: 0.5321642160415649, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235620141029358, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 264:\n",
      "  Training Loss: 0.5321530103683472, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236059427261353, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 265:\n",
      "  Training Loss: 0.5321354269981384, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236057639122009, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 266:\n",
      "  Training Loss: 0.5321521759033203, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235595703125, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 267:\n",
      "  Training Loss: 0.5321311354637146, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235452055931091, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 268:\n",
      "  Training Loss: 0.5321506261825562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 269:\n",
      "  Training Loss: 0.5321443676948547, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236707925796509, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 270:\n",
      "  Training Loss: 0.5321435928344727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235994458198547, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 271:\n",
      "  Training Loss: 0.532131552696228, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235036015510559, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 272:\n",
      "  Training Loss: 0.5321553945541382, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235408544540405, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 273:\n",
      "  Training Loss: 0.5321485996246338, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236535668373108, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 274:\n",
      "  Training Loss: 0.5321184396743774, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234708786010742, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 275:\n",
      "  Training Loss: 0.532153844833374, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523521363735199, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 276:\n",
      "  Training Loss: 0.5321598649024963, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523607075214386, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 277:\n",
      "  Training Loss: 0.532151997089386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235165953636169, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 278:\n",
      "  Training Loss: 0.5321224331855774, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234693884849548, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 279:\n",
      "  Training Loss: 0.5321435928344727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235854387283325, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 280:\n",
      "  Training Loss: 0.5321556329727173, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234827995300293, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 281:\n",
      "  Training Loss: 0.5321729779243469, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235324501991272, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 282:\n",
      "  Training Loss: 0.5321031808853149, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234423279762268, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 283:\n",
      "  Training Loss: 0.5321749448776245, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235154628753662, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 284:\n",
      "  Training Loss: 0.5321415662765503, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235326886177063, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 285:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236443877220154, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 286:\n",
      "  Training Loss: 0.5321341156959534, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234922170639038, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 287:\n",
      "  Training Loss: 0.5320940017700195, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238453149795532, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 288:\n",
      "  Training Loss: 0.5321062207221985, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523472785949707, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 289:\n",
      "  Training Loss: 0.5321362018585205, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523449182510376, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 290:\n",
      "  Training Loss: 0.5321842432022095, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234931707382202, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 291:\n",
      "  Training Loss: 0.5321283936500549, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523472785949707, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 292:\n",
      "  Training Loss: 0.5321252942085266, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235885977745056, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 293:\n",
      "  Training Loss: 0.5321413278579712, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235173106193542, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 294:\n",
      "  Training Loss: 0.532139778137207, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235199928283691, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 295:\n",
      "  Training Loss: 0.5321572422981262, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235527753829956, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 296:\n",
      "  Training Loss: 0.5321700572967529, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236743688583374, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 297:\n",
      "  Training Loss: 0.5321452617645264, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236531496047974, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 298:\n",
      "  Training Loss: 0.532133162021637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236612558364868, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 299:\n",
      "  Training Loss: 0.532151997089386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235291719436646, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 300:\n",
      "  Training Loss: 0.5321325063705444, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237330794334412, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 301:\n",
      "  Training Loss: 0.5321515202522278, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235390067100525, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 302:\n",
      "  Training Loss: 0.5321523547172546, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523531436920166, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 303:\n",
      "  Training Loss: 0.532136082649231, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237670540809631, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 304:\n",
      "  Training Loss: 0.5321338176727295, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237191319465637, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 305:\n",
      "  Training Loss: 0.5321730375289917, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235481858253479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 306:\n",
      "  Training Loss: 0.5321388840675354, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236449241638184, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 307:\n",
      "  Training Loss: 0.5321618914604187, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235289335250854, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 308:\n",
      "  Training Loss: 0.5321425795555115, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236707329750061, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 309:\n",
      "  Training Loss: 0.5321497917175293, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236009955406189, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 310:\n",
      "  Training Loss: 0.5321162939071655, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523483157157898, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 311:\n",
      "  Training Loss: 0.5321340560913086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236702561378479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 312:\n",
      "  Training Loss: 0.532164454460144, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235551595687866, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 313:\n",
      "  Training Loss: 0.5321317911148071, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234730839729309, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 314:\n",
      "  Training Loss: 0.5321352481842041, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236936807632446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 315:\n",
      "  Training Loss: 0.5321350693702698, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234635472297668, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 316:\n",
      "  Training Loss: 0.5321657061576843, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236244201660156, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 317:\n",
      "  Training Loss: 0.5321477651596069, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235311388969421, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 318:\n",
      "  Training Loss: 0.5321577787399292, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235508680343628, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 319:\n",
      "  Training Loss: 0.5321531295776367, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235906839370728, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 320:\n",
      "  Training Loss: 0.5321542620658875, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234953165054321, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 321:\n",
      "  Training Loss: 0.5321534872055054, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235933065414429, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 322:\n",
      "  Training Loss: 0.5321508646011353, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235458612442017, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 323:\n",
      "  Training Loss: 0.5321450233459473, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236845016479492, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 324:\n",
      "  Training Loss: 0.5321511626243591, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235010385513306, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 325:\n",
      "  Training Loss: 0.5321547985076904, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523546576499939, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 326:\n",
      "  Training Loss: 0.5321353077888489, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235815644264221, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 327:\n",
      "  Training Loss: 0.5321314334869385, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235026478767395, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 328:\n",
      "  Training Loss: 0.5321524143218994, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523545503616333, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 329:\n",
      "  Training Loss: 0.5321347117424011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235961079597473, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 330:\n",
      "  Training Loss: 0.5321353077888489, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236078500747681, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 331:\n",
      "  Training Loss: 0.5321348309516907, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237276554107666, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 332:\n",
      "  Training Loss: 0.5321168303489685, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234735608100891, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 333:\n",
      "  Training Loss: 0.5321667194366455, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234677195549011, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 334:\n",
      "  Training Loss: 0.5321522951126099, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234838128089905, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 335:\n",
      "  Training Loss: 0.5321522951126099, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235894918441772, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 336:\n",
      "  Training Loss: 0.5321274995803833, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235826373100281, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 337:\n",
      "  Training Loss: 0.532158374786377, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234991312026978, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 338:\n",
      "  Training Loss: 0.5321365594863892, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235550999641418, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 339:\n",
      "  Training Loss: 0.532150149345398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236088633537292, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 340:\n",
      "  Training Loss: 0.5321475267410278, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235307812690735, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 341:\n",
      "  Training Loss: 0.5321477651596069, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235704183578491, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 342:\n",
      "  Training Loss: 0.532096266746521, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234358906745911, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 343:\n",
      "  Training Loss: 0.5321500301361084, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236154794692993, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 344:\n",
      "  Training Loss: 0.5321441292762756, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236917734146118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 345:\n",
      "  Training Loss: 0.5321547985076904, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235528349876404, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 346:\n",
      "  Training Loss: 0.5321342349052429, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235223770141602, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 347:\n",
      "  Training Loss: 0.5321466326713562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523496687412262, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 348:\n",
      "  Training Loss: 0.5321405529975891, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235170722007751, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 349:\n",
      "  Training Loss: 0.5321422815322876, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523556649684906, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 350:\n",
      "  Training Loss: 0.5321331024169922, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237657427787781, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 351:\n",
      "  Training Loss: 0.5321372747421265, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236068964004517, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 352:\n",
      "  Training Loss: 0.5320989489555359, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238458514213562, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 353:\n",
      "  Training Loss: 0.5321574211120605, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234774351119995, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 354:\n",
      "  Training Loss: 0.5321594476699829, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234947204589844, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 355:\n",
      "  Training Loss: 0.5321438908576965, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236735939979553, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 356:\n",
      "  Training Loss: 0.5321475267410278, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236529111862183, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 357:\n",
      "  Training Loss: 0.5321460366249084, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235326886177063, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 358:\n",
      "  Training Loss: 0.5321285724639893, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237364172935486, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 359:\n",
      "  Training Loss: 0.532168984413147, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235123634338379, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 360:\n",
      "  Training Loss: 0.5321233868598938, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236427783966064, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 361:\n",
      "  Training Loss: 0.5321506261825562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237273573875427, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 362:\n",
      "  Training Loss: 0.5321414470672607, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236108303070068, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 363:\n",
      "  Training Loss: 0.5321570038795471, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234830975532532, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 364:\n",
      "  Training Loss: 0.5321445465087891, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523529052734375, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 365:\n",
      "  Training Loss: 0.5321446657180786, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236230492591858, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 366:\n",
      "  Training Loss: 0.5321466326713562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236081480979919, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 367:\n",
      "  Training Loss: 0.532148540019989, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236429572105408, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 368:\n",
      "  Training Loss: 0.5321463346481323, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236177444458008, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 369:\n",
      "  Training Loss: 0.5321394205093384, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236446857452393, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 370:\n",
      "  Training Loss: 0.532160758972168, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235907435417175, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 371:\n",
      "  Training Loss: 0.5321603417396545, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236429572105408, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 372:\n",
      "  Training Loss: 0.5321376323699951, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235123038291931, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 373:\n",
      "  Training Loss: 0.5321424007415771, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236086249351501, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 374:\n",
      "  Training Loss: 0.5321524143218994, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523543655872345, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 375:\n",
      "  Training Loss: 0.532148540019989, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234612822532654, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 376:\n",
      "  Training Loss: 0.5321478247642517, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236078500747681, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 377:\n",
      "  Training Loss: 0.5321492552757263, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235180258750916, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 378:\n",
      "  Training Loss: 0.532150387763977, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235327482223511, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 379:\n",
      "  Training Loss: 0.5321276187896729, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234782695770264, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 380:\n",
      "  Training Loss: 0.532147228717804, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235826373100281, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 381:\n",
      "  Training Loss: 0.5321502089500427, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235458016395569, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 382:\n",
      "  Training Loss: 0.5321341156959534, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523493230342865, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 383:\n",
      "  Training Loss: 0.5321467518806458, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234928727149963, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 384:\n",
      "  Training Loss: 0.5321527123451233, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235611796379089, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 385:\n",
      "  Training Loss: 0.5321385264396667, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235551595687866, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 386:\n",
      "  Training Loss: 0.5321547389030457, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236548781394958, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 387:\n",
      "  Training Loss: 0.5321642160415649, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236107707023621, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 388:\n",
      "  Training Loss: 0.5321439504623413, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235248804092407, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 389:\n",
      "  Training Loss: 0.5321378111839294, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234782099723816, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 390:\n",
      "  Training Loss: 0.5321659445762634, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234975814819336, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 391:\n",
      "  Training Loss: 0.5321120619773865, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237028002738953, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 392:\n",
      "  Training Loss: 0.5321512222290039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235556960105896, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 393:\n",
      "  Training Loss: 0.5321228504180908, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234531164169312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 394:\n",
      "  Training Loss: 0.5321412086486816, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237151384353638, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 395:\n",
      "  Training Loss: 0.5321528315544128, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235199332237244, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 396:\n",
      "  Training Loss: 0.5321449637413025, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235320925712585, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 397:\n",
      "  Training Loss: 0.5321665406227112, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523603081703186, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 398:\n",
      "  Training Loss: 0.532141923904419, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236324667930603, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 399:\n",
      "  Training Loss: 0.532123327255249, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234652757644653, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 400:\n",
      "  Training Loss: 0.532156229019165, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235060453414917, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 401:\n",
      "  Training Loss: 0.5321608781814575, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236167311668396, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 402:\n",
      "  Training Loss: 0.5321367383003235, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236353278160095, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 403:\n",
      "  Training Loss: 0.5321706533432007, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235260128974915, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 404:\n",
      "  Training Loss: 0.5321430563926697, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235994458198547, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 405:\n",
      "  Training Loss: 0.5321295857429504, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234718322753906, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 406:\n",
      "  Training Loss: 0.5321683883666992, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236223936080933, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 407:\n",
      "  Training Loss: 0.5321512222290039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235792994499207, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 408:\n",
      "  Training Loss: 0.5321447253227234, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235296487808228, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 409:\n",
      "  Training Loss: 0.5321484208106995, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234684944152832, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 410:\n",
      "  Training Loss: 0.5321650505065918, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235446095466614, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 411:\n",
      "  Training Loss: 0.532160758972168, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236788392066956, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 412:\n",
      "  Training Loss: 0.5321332812309265, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237085223197937, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 413:\n",
      "  Training Loss: 0.5321712493896484, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236523151397705, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 414:\n",
      "  Training Loss: 0.5321687459945679, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235551595687866, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 415:\n",
      "  Training Loss: 0.5321404933929443, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235550999641418, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 416:\n",
      "  Training Loss: 0.5321437120437622, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235176682472229, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 417:\n",
      "  Training Loss: 0.5321325063705444, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234781503677368, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 418:\n",
      "  Training Loss: 0.5321475863456726, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716700553894, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 419:\n",
      "  Training Loss: 0.532157301902771, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235626697540283, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 420:\n",
      "  Training Loss: 0.5321455001831055, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235186815261841, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 421:\n",
      "  Training Loss: 0.5321171879768372, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234594345092773, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 422:\n",
      "  Training Loss: 0.5321171879768372, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237722396850586, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 423:\n",
      "  Training Loss: 0.5322136878967285, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236527323722839, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 424:\n",
      "  Training Loss: 0.5321191549301147, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236544013023376, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 425:\n",
      "  Training Loss: 0.532148003578186, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235128402709961, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 426:\n",
      "  Training Loss: 0.5321333408355713, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235134959220886, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 427:\n",
      "  Training Loss: 0.5321547985076904, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235527157783508, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 428:\n",
      "  Training Loss: 0.532163143157959, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236127376556396, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 429:\n",
      "  Training Loss: 0.5321359634399414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234864950180054, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 430:\n",
      "  Training Loss: 0.532124936580658, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235931873321533, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 431:\n",
      "  Training Loss: 0.5321576595306396, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235853791236877, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 432:\n",
      "  Training Loss: 0.5321424603462219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236685872077942, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 433:\n",
      "  Training Loss: 0.5321416854858398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237060189247131, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 434:\n",
      "  Training Loss: 0.5321577191352844, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 435:\n",
      "  Training Loss: 0.5321417450904846, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235105752944946, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 436:\n",
      "  Training Loss: 0.5320852398872375, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238000750541687, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 437:\n",
      "  Training Loss: 0.5321649312973022, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236364603042603, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 438:\n",
      "  Training Loss: 0.5321492552757263, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235875844955444, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 439:\n",
      "  Training Loss: 0.532149076461792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236063599586487, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 440:\n",
      "  Training Loss: 0.532131552696228, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236008763313293, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 441:\n",
      "  Training Loss: 0.5321367979049683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234645009040833, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 442:\n",
      "  Training Loss: 0.5321323275566101, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523766279220581, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 443:\n",
      "  Training Loss: 0.5321604609489441, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236079692840576, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 444:\n",
      "  Training Loss: 0.5321440100669861, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235742926597595, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 445:\n",
      "  Training Loss: 0.5321165919303894, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234500765800476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 446:\n",
      "  Training Loss: 0.5321487784385681, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234804153442383, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 447:\n",
      "  Training Loss: 0.5321473479270935, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234897136688232, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 448:\n",
      "  Training Loss: 0.5321621298789978, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234938859939575, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 449:\n",
      "  Training Loss: 0.5321658849716187, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235614776611328, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 450:\n",
      "  Training Loss: 0.5321667194366455, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234923362731934, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 451:\n",
      "  Training Loss: 0.5321434736251831, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236766338348389, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 452:\n",
      "  Training Loss: 0.5321410298347473, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236826539039612, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 453:\n",
      "  Training Loss: 0.5321542620658875, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236294865608215, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 454:\n",
      "  Training Loss: 0.5321248769760132, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235008597373962, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 455:\n",
      "  Training Loss: 0.5321241617202759, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236654877662659, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 456:\n",
      "  Training Loss: 0.5321332812309265, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236337184906006, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 457:\n",
      "  Training Loss: 0.5321478843688965, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234932899475098, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 458:\n",
      "  Training Loss: 0.532134473323822, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234676599502563, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 459:\n",
      "  Training Loss: 0.5321595072746277, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235621929168701, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 460:\n",
      "  Training Loss: 0.5321556925773621, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236400365829468, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 461:\n",
      "  Training Loss: 0.5321382880210876, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235594511032104, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 462:\n",
      "  Training Loss: 0.5321393609046936, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235063433647156, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 463:\n",
      "  Training Loss: 0.532145082950592, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523679792881012, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 464:\n",
      "  Training Loss: 0.5321531295776367, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236023664474487, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 465:\n",
      "  Training Loss: 0.5321456789970398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234787464141846, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 466:\n",
      "  Training Loss: 0.5321335792541504, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236276984214783, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 467:\n",
      "  Training Loss: 0.5321136713027954, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237156748771667, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 468:\n",
      "  Training Loss: 0.5321357846260071, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235246419906616, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 469:\n",
      "  Training Loss: 0.532151460647583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234958529472351, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 470:\n",
      "  Training Loss: 0.5321119427680969, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234610438346863, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 471:\n",
      "  Training Loss: 0.5321484208106995, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235780477523804, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 472:\n",
      "  Training Loss: 0.5321423411369324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235443115234375, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 473:\n",
      "  Training Loss: 0.5321100354194641, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237600803375244, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 474:\n",
      "  Training Loss: 0.5321836471557617, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235533118247986, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 475:\n",
      "  Training Loss: 0.5321444272994995, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235912203788757, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 476:\n",
      "  Training Loss: 0.5321626663208008, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 477:\n",
      "  Training Loss: 0.532164454460144, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236053466796875, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 478:\n",
      "  Training Loss: 0.5321419835090637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236838459968567, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 479:\n",
      "  Training Loss: 0.5321720242500305, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235764384269714, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 480:\n",
      "  Training Loss: 0.5321527123451233, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235809087753296, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 481:\n",
      "  Training Loss: 0.5321511626243591, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235745906829834, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 482:\n",
      "  Training Loss: 0.532132089138031, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236333608627319, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 483:\n",
      "  Training Loss: 0.5321428179740906, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235171318054199, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 484:\n",
      "  Training Loss: 0.5321540832519531, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235508680343628, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 485:\n",
      "  Training Loss: 0.5321238040924072, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523442268371582, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 486:\n",
      "  Training Loss: 0.5321635007858276, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523453950881958, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 487:\n",
      "  Training Loss: 0.5321490168571472, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236120820045471, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 488:\n",
      "  Training Loss: 0.5321105718612671, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234975814819336, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 489:\n",
      "  Training Loss: 0.5321389436721802, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235268473625183, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 490:\n",
      "  Training Loss: 0.5321033000946045, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237492322921753, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 491:\n",
      "  Training Loss: 0.532156765460968, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234881043434143, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 492:\n",
      "  Training Loss: 0.5321672558784485, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234796404838562, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 493:\n",
      "  Training Loss: 0.5321511030197144, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235663056373596, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 494:\n",
      "  Training Loss: 0.5321354269981384, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523472011089325, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 495:\n",
      "  Training Loss: 0.5321652293205261, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236119627952576, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 496:\n",
      "  Training Loss: 0.5321010947227478, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234509110450745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 497:\n",
      "  Training Loss: 0.5321900248527527, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235077142715454, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 498:\n",
      "  Training Loss: 0.5321429371833801, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234922170639038, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 499:\n",
      "  Training Loss: 0.5321683883666992, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235775113105774, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 500:\n",
      "  Training Loss: 0.532119870185852, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234825015068054, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 501:\n",
      "  Training Loss: 0.5321665406227112, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234716534614563, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 502:\n",
      "  Training Loss: 0.532099723815918, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234421491622925, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 503:\n",
      "  Training Loss: 0.5321568250656128, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237112641334534, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 504:\n",
      "  Training Loss: 0.5321623086929321, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235791206359863, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 505:\n",
      "  Training Loss: 0.5321649312973022, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523540198802948, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 506:\n",
      "  Training Loss: 0.5321033000946045, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234768390655518, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 507:\n",
      "  Training Loss: 0.5321281552314758, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236527323722839, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 508:\n",
      "  Training Loss: 0.5321652889251709, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236405730247498, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 509:\n",
      "  Training Loss: 0.5321491956710815, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236080288887024, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 510:\n",
      "  Training Loss: 0.5321323275566101, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235215425491333, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 511:\n",
      "  Training Loss: 0.5321443676948547, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236602425575256, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 512:\n",
      "  Training Loss: 0.5321485996246338, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235641598701477, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 513:\n",
      "  Training Loss: 0.5321367979049683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234963297843933, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 514:\n",
      "  Training Loss: 0.5321564674377441, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523563027381897, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 515:\n",
      "  Training Loss: 0.5321387648582458, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523652970790863, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 516:\n",
      "  Training Loss: 0.5321635603904724, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236557722091675, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 517:\n",
      "  Training Loss: 0.5321565866470337, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235307812690735, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 518:\n",
      "  Training Loss: 0.5321391820907593, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234942436218262, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 519:\n",
      "  Training Loss: 0.5321317315101624, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234947204589844, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 520:\n",
      "  Training Loss: 0.532135546207428, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236719250679016, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 521:\n",
      "  Training Loss: 0.5321496725082397, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676765441895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 522:\n",
      "  Training Loss: 0.5321612358093262, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235624313354492, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 523:\n",
      "  Training Loss: 0.5321405529975891, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236448049545288, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 524:\n",
      "  Training Loss: 0.5321512222290039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236196517944336, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 525:\n",
      "  Training Loss: 0.5321490168571472, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235462188720703, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 526:\n",
      "  Training Loss: 0.5321498513221741, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235303044319153, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 527:\n",
      "  Training Loss: 0.5321396589279175, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236480832099915, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 528:\n",
      "  Training Loss: 0.5321550965309143, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236917734146118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 529:\n",
      "  Training Loss: 0.5321511626243591, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523526668548584, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 530:\n",
      "  Training Loss: 0.5321468710899353, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236125588417053, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 531:\n",
      "  Training Loss: 0.5321013331413269, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238761901855469, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 532:\n",
      "  Training Loss: 0.5321841239929199, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 533:\n",
      "  Training Loss: 0.532127857208252, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236902832984924, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 534:\n",
      "  Training Loss: 0.5321472883224487, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235956907272339, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 535:\n",
      "  Training Loss: 0.532142698764801, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235260725021362, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 536:\n",
      "  Training Loss: 0.5321184396743774, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523715615272522, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 537:\n",
      "  Training Loss: 0.5321551561355591, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235596895217896, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 538:\n",
      "  Training Loss: 0.53214430809021, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235001444816589, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 539:\n",
      "  Training Loss: 0.5321420431137085, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235494375228882, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 540:\n",
      "  Training Loss: 0.5321523547172546, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235427618026733, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 541:\n",
      "  Training Loss: 0.5321333408355713, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234537720680237, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 542:\n",
      "  Training Loss: 0.5321890115737915, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235610008239746, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 543:\n",
      "  Training Loss: 0.5321442484855652, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236628651618958, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 544:\n",
      "  Training Loss: 0.5321308374404907, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235803723335266, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 545:\n",
      "  Training Loss: 0.5321409702301025, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234957933425903, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 546:\n",
      "  Training Loss: 0.5321217775344849, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234782099723816, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 547:\n",
      "  Training Loss: 0.5321553349494934, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235645174980164, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 548:\n",
      "  Training Loss: 0.5321298837661743, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234699249267578, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 549:\n",
      "  Training Loss: 0.5321147441864014, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237412452697754, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 550:\n",
      "  Training Loss: 0.5321490168571472, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52359539270401, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 551:\n",
      "  Training Loss: 0.5321436524391174, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235409736633301, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 552:\n",
      "  Training Loss: 0.5321441888809204, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236227512359619, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 553:\n",
      "  Training Loss: 0.5321846008300781, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235943794250488, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 554:\n",
      "  Training Loss: 0.5321431756019592, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236307978630066, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 555:\n",
      "  Training Loss: 0.5321536660194397, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235494375228882, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 556:\n",
      "  Training Loss: 0.5321409702301025, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235376358032227, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 557:\n",
      "  Training Loss: 0.5321347117424011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236071348190308, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 558:\n",
      "  Training Loss: 0.5321340560913086, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 559:\n",
      "  Training Loss: 0.5321379899978638, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235307812690735, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 560:\n",
      "  Training Loss: 0.532140851020813, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235675573348999, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 561:\n",
      "  Training Loss: 0.532151460647583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523504912853241, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 562:\n",
      "  Training Loss: 0.5321696996688843, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234783291816711, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 563:\n",
      "  Training Loss: 0.5320379137992859, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238472819328308, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 564:\n",
      "  Training Loss: 0.5321976542472839, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236093401908875, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 565:\n",
      "  Training Loss: 0.532151997089386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235291719436646, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 566:\n",
      "  Training Loss: 0.5321493148803711, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236727595329285, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 567:\n",
      "  Training Loss: 0.5321469306945801, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235939025878906, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 568:\n",
      "  Training Loss: 0.5321409106254578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235754251480103, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 569:\n",
      "  Training Loss: 0.5321236252784729, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236265063285828, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 570:\n",
      "  Training Loss: 0.5321539640426636, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236818194389343, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 571:\n",
      "  Training Loss: 0.5321725606918335, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523626446723938, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 572:\n",
      "  Training Loss: 0.5321515202522278, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 573:\n",
      "  Training Loss: 0.5321730375289917, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234706401824951, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 574:\n",
      "  Training Loss: 0.5321610569953918, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235134959220886, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 575:\n",
      "  Training Loss: 0.5321475863456726, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236542224884033, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 576:\n",
      "  Training Loss: 0.5321124792098999, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234705209732056, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 577:\n",
      "  Training Loss: 0.5321218371391296, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236630439758301, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 578:\n",
      "  Training Loss: 0.5321391224861145, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234991312026978, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 579:\n",
      "  Training Loss: 0.5321597456932068, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235536694526672, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 580:\n",
      "  Training Loss: 0.532130241394043, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236015915870667, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 581:\n",
      "  Training Loss: 0.5321327447891235, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236511826515198, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 582:\n",
      "  Training Loss: 0.5321177840232849, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238324403762817, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 583:\n",
      "  Training Loss: 0.5321493744850159, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523552417755127, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 584:\n",
      "  Training Loss: 0.5321410298347473, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523616373538971, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 585:\n",
      "  Training Loss: 0.5321351289749146, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235036611557007, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 586:\n",
      "  Training Loss: 0.5321165323257446, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234506130218506, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 587:\n",
      "  Training Loss: 0.5321660041809082, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235624313354492, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 588:\n",
      "  Training Loss: 0.532166063785553, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523643970489502, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 589:\n",
      "  Training Loss: 0.5321431159973145, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235535502433777, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 590:\n",
      "  Training Loss: 0.5321111679077148, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237815976142883, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 591:\n",
      "  Training Loss: 0.5321979522705078, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236152410507202, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 592:\n",
      "  Training Loss: 0.5321317315101624, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234786868095398, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 593:\n",
      "  Training Loss: 0.5321295857429504, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234664082527161, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 594:\n",
      "  Training Loss: 0.5321314334869385, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237149596214294, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 595:\n",
      "  Training Loss: 0.5321661233901978, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236087441444397, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 596:\n",
      "  Training Loss: 0.5321661233901978, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236279368400574, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 597:\n",
      "  Training Loss: 0.5321456789970398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235159397125244, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 598:\n",
      "  Training Loss: 0.5321286916732788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237237811088562, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 599:\n",
      "  Training Loss: 0.5322058796882629, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235409140586853, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 600:\n",
      "  Training Loss: 0.5321424007415771, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234903693199158, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 601:\n",
      "  Training Loss: 0.5321720242500305, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235770344734192, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 602:\n",
      "  Training Loss: 0.5321400165557861, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523662805557251, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 603:\n",
      "  Training Loss: 0.5321187376976013, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523473858833313, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 604:\n",
      "  Training Loss: 0.5321616530418396, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235590934753418, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 605:\n",
      "  Training Loss: 0.5321493744850159, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235797166824341, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 606:\n",
      "  Training Loss: 0.5321375131607056, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235635638237, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 607:\n",
      "  Training Loss: 0.5321828126907349, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235159397125244, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 608:\n",
      "  Training Loss: 0.5321359634399414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235209465026855, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 609:\n",
      "  Training Loss: 0.5321642756462097, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235541462898254, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 610:\n",
      "  Training Loss: 0.5321418046951294, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523547351360321, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 611:\n",
      "  Training Loss: 0.5321384072303772, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236086845397949, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 612:\n",
      "  Training Loss: 0.5321511030197144, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236047506332397, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 613:\n",
      "  Training Loss: 0.532146155834198, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234991312026978, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 614:\n",
      "  Training Loss: 0.532128632068634, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234736204147339, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 615:\n",
      "  Training Loss: 0.5321516394615173, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694050788879, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 616:\n",
      "  Training Loss: 0.5321305990219116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236071348190308, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 617:\n",
      "  Training Loss: 0.5321484208106995, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236296653747559, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 618:\n",
      "  Training Loss: 0.532143771648407, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235999822616577, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 619:\n",
      "  Training Loss: 0.5321223735809326, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236973166465759, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 620:\n",
      "  Training Loss: 0.5321462750434875, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235956311225891, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 621:\n",
      "  Training Loss: 0.532131016254425, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236608386039734, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 622:\n",
      "  Training Loss: 0.5321668386459351, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235776305198669, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 623:\n",
      "  Training Loss: 0.5321369767189026, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237008929252625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 624:\n",
      "  Training Loss: 0.5321329236030579, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237517356872559, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 625:\n",
      "  Training Loss: 0.5321435928344727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236606597900391, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 626:\n",
      "  Training Loss: 0.5321581363677979, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235748291015625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 627:\n",
      "  Training Loss: 0.5321473479270935, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236386060714722, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 628:\n",
      "  Training Loss: 0.5321556329727173, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235342979431152, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 629:\n",
      "  Training Loss: 0.5321389436721802, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236290097236633, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 630:\n",
      "  Training Loss: 0.5321523547172546, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235258936882019, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 631:\n",
      "  Training Loss: 0.5321633219718933, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236724615097046, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 632:\n",
      "  Training Loss: 0.5321516394615173, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235226154327393, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 633:\n",
      "  Training Loss: 0.5321327447891235, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234735608100891, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 634:\n",
      "  Training Loss: 0.5321728587150574, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235536694526672, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 635:\n",
      "  Training Loss: 0.5321458578109741, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236110091209412, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 636:\n",
      "  Training Loss: 0.5321565270423889, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235878229141235, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 637:\n",
      "  Training Loss: 0.5321246981620789, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234774351119995, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 638:\n",
      "  Training Loss: 0.5321426391601562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237182378768921, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 639:\n",
      "  Training Loss: 0.5321457982063293, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236977934837341, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 640:\n",
      "  Training Loss: 0.532106339931488, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238593220710754, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 641:\n",
      "  Training Loss: 0.532173752784729, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235368609428406, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 642:\n",
      "  Training Loss: 0.5321559906005859, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236100554466248, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 643:\n",
      "  Training Loss: 0.5321415662765503, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235006213188171, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 644:\n",
      "  Training Loss: 0.5321033596992493, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234392285346985, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 645:\n",
      "  Training Loss: 0.5321840047836304, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 646:\n",
      "  Training Loss: 0.5321494936943054, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236120223999023, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 647:\n",
      "  Training Loss: 0.5321451425552368, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235904455184937, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 648:\n",
      "  Training Loss: 0.5321459770202637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235116481781006, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 649:\n",
      "  Training Loss: 0.5321385264396667, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523590087890625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 650:\n",
      "  Training Loss: 0.5321435928344727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236968398094177, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 651:\n",
      "  Training Loss: 0.5321592688560486, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523603618144989, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 652:\n",
      "  Training Loss: 0.5321362614631653, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234944820404053, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 653:\n",
      "  Training Loss: 0.5321246385574341, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237026810646057, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 654:\n",
      "  Training Loss: 0.5321289300918579, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238811373710632, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 655:\n",
      "  Training Loss: 0.5321646928787231, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235761404037476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 656:\n",
      "  Training Loss: 0.5321596264839172, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235368609428406, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 657:\n",
      "  Training Loss: 0.5321506261825562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236448049545288, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 658:\n",
      "  Training Loss: 0.5321362018585205, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235477089881897, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 659:\n",
      "  Training Loss: 0.5321651101112366, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523664653301239, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 660:\n",
      "  Training Loss: 0.5321548581123352, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235776901245117, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 661:\n",
      "  Training Loss: 0.5321347117424011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234765410423279, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 662:\n",
      "  Training Loss: 0.5321394801139832, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523649275302887, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 663:\n",
      "  Training Loss: 0.5321493148803711, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236141681671143, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 664:\n",
      "  Training Loss: 0.5321540236473083, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235162973403931, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 665:\n",
      "  Training Loss: 0.5321596264839172, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235267281532288, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 666:\n",
      "  Training Loss: 0.5321517586708069, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235993266105652, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 667:\n",
      "  Training Loss: 0.5321460366249084, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236137509346008, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 668:\n",
      "  Training Loss: 0.5321448445320129, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235902667045593, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 669:\n",
      "  Training Loss: 0.5321475863456726, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523764967918396, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 670:\n",
      "  Training Loss: 0.5321558117866516, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235277414321899, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 671:\n",
      "  Training Loss: 0.5321720242500305, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235038995742798, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 672:\n",
      "  Training Loss: 0.5321542620658875, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235278606414795, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 673:\n",
      "  Training Loss: 0.5321359634399414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235395431518555, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 674:\n",
      "  Training Loss: 0.5321344137191772, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236833691596985, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 675:\n",
      "  Training Loss: 0.5321490168571472, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 676:\n",
      "  Training Loss: 0.5321423411369324, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236278176307678, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 677:\n",
      "  Training Loss: 0.5321393013000488, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235137343406677, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 678:\n",
      "  Training Loss: 0.5321214199066162, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236409902572632, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 679:\n",
      "  Training Loss: 0.5321387648582458, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236709117889404, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 680:\n",
      "  Training Loss: 0.5321579575538635, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235552191734314, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 681:\n",
      "  Training Loss: 0.5321601629257202, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235311388969421, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 682:\n",
      "  Training Loss: 0.5321275591850281, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234600305557251, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 683:\n",
      "  Training Loss: 0.5321500897407532, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236530303955078, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 684:\n",
      "  Training Loss: 0.5321435332298279, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235163569450378, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 685:\n",
      "  Training Loss: 0.5321369767189026, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235236883163452, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 686:\n",
      "  Training Loss: 0.5321546792984009, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235509276390076, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 687:\n",
      "  Training Loss: 0.532112181186676, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523460865020752, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 688:\n",
      "  Training Loss: 0.5321542024612427, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235708355903625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 689:\n",
      "  Training Loss: 0.5321351885795593, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236092209815979, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 690:\n",
      "  Training Loss: 0.5321545004844666, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235185623168945, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 691:\n",
      "  Training Loss: 0.5321521162986755, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236086249351501, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 692:\n",
      "  Training Loss: 0.5321621894836426, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235422849655151, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 693:\n",
      "  Training Loss: 0.532145082950592, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234747529029846, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 694:\n",
      "  Training Loss: 0.5321454405784607, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235037207603455, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 695:\n",
      "  Training Loss: 0.5321739315986633, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235527753829956, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 696:\n",
      "  Training Loss: 0.5321451425552368, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236116051673889, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 697:\n",
      "  Training Loss: 0.5321447253227234, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236550569534302, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 698:\n",
      "  Training Loss: 0.532110333442688, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234451293945312, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 699:\n",
      "  Training Loss: 0.5321581363677979, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235416293144226, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 700:\n",
      "  Training Loss: 0.5321158170700073, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234440565109253, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 701:\n",
      "  Training Loss: 0.5321547389030457, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235365033149719, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 702:\n",
      "  Training Loss: 0.5321850180625916, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 703:\n",
      "  Training Loss: 0.532139241695404, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523567795753479, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 704:\n",
      "  Training Loss: 0.5321438908576965, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235608816146851, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 705:\n",
      "  Training Loss: 0.5321250557899475, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235311388969421, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 706:\n",
      "  Training Loss: 0.5321486592292786, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235295295715332, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 707:\n",
      "  Training Loss: 0.5321566462516785, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236536264419556, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 708:\n",
      "  Training Loss: 0.5321131944656372, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237428545951843, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 709:\n",
      "  Training Loss: 0.5321676135063171, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523628294467926, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 710:\n",
      "  Training Loss: 0.5321409106254578, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235966444015503, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 711:\n",
      "  Training Loss: 0.5321608781814575, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236018896102905, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 712:\n",
      "  Training Loss: 0.5321347713470459, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235340595245361, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 713:\n",
      "  Training Loss: 0.5320848226547241, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238842368125916, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 714:\n",
      "  Training Loss: 0.5321691632270813, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235777497291565, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 715:\n",
      "  Training Loss: 0.5321531891822815, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235610008239746, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 716:\n",
      "  Training Loss: 0.5321459770202637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523592472076416, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 717:\n",
      "  Training Loss: 0.5321744084358215, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738158226013, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 718:\n",
      "  Training Loss: 0.5321627259254456, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236440896987915, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 719:\n",
      "  Training Loss: 0.5321096181869507, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234647989273071, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 720:\n",
      "  Training Loss: 0.532143235206604, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235463380813599, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 721:\n",
      "  Training Loss: 0.5321512222290039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235830545425415, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 722:\n",
      "  Training Loss: 0.5321440696716309, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235342383384705, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 723:\n",
      "  Training Loss: 0.5321564674377441, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235565304756165, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 724:\n",
      "  Training Loss: 0.5321601629257202, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235894918441772, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 725:\n",
      "  Training Loss: 0.532142698764801, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235689282417297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 726:\n",
      "  Training Loss: 0.5321309566497803, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236631631851196, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 727:\n",
      "  Training Loss: 0.5321633219718933, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235644578933716, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 728:\n",
      "  Training Loss: 0.5321439504623413, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234805941581726, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 729:\n",
      "  Training Loss: 0.5321173071861267, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237134695053101, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 730:\n",
      "  Training Loss: 0.5321611166000366, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237028002738953, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 731:\n",
      "  Training Loss: 0.5321428775787354, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236037969589233, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 732:\n",
      "  Training Loss: 0.5321772694587708, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235734581947327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 733:\n",
      "  Training Loss: 0.532151460647583, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523525595664978, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 734:\n",
      "  Training Loss: 0.5321450233459473, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523551881313324, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 735:\n",
      "  Training Loss: 0.5321236252784729, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523462176322937, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 736:\n",
      "  Training Loss: 0.5321386456489563, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523638129234314, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 737:\n",
      "  Training Loss: 0.5321558713912964, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235375761985779, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 738:\n",
      "  Training Loss: 0.532146155834198, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236210823059082, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 739:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237070918083191, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 740:\n",
      "  Training Loss: 0.5321518778800964, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235641598701477, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 741:\n",
      "  Training Loss: 0.5321320295333862, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238131284713745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 742:\n",
      "  Training Loss: 0.5321429371833801, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236126184463501, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 743:\n",
      "  Training Loss: 0.5321545600891113, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235617160797119, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 744:\n",
      "  Training Loss: 0.5321156978607178, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236109495162964, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 745:\n",
      "  Training Loss: 0.532168984413147, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235395431518555, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 746:\n",
      "  Training Loss: 0.5321416854858398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235006213188171, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 747:\n",
      "  Training Loss: 0.5321530103683472, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234552621841431, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 748:\n",
      "  Training Loss: 0.5321665406227112, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523597776889801, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 749:\n",
      "  Training Loss: 0.5321137309074402, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238066911697388, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 750:\n",
      "  Training Loss: 0.5321649312973022, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235376358032227, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 751:\n",
      "  Training Loss: 0.5321416258811951, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523620069026947, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 752:\n",
      "  Training Loss: 0.5321508646011353, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234783887863159, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 753:\n",
      "  Training Loss: 0.5321634411811829, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235048532485962, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 754:\n",
      "  Training Loss: 0.5321493744850159, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235986113548279, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 755:\n",
      "  Training Loss: 0.5321583151817322, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235617160797119, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 756:\n",
      "  Training Loss: 0.5321518182754517, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523552417755127, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 757:\n",
      "  Training Loss: 0.5321488976478577, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236736536026001, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 758:\n",
      "  Training Loss: 0.5321577191352844, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236353874206543, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 759:\n",
      "  Training Loss: 0.532147228717804, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236299633979797, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 760:\n",
      "  Training Loss: 0.532153308391571, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236791372299194, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 761:\n",
      "  Training Loss: 0.5321572422981262, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235577821731567, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 762:\n",
      "  Training Loss: 0.5321382284164429, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523509681224823, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 763:\n",
      "  Training Loss: 0.5321488380432129, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235162377357483, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 764:\n",
      "  Training Loss: 0.5321523547172546, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 765:\n",
      "  Training Loss: 0.5321099758148193, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237640142440796, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 766:\n",
      "  Training Loss: 0.5321024060249329, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234743356704712, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 767:\n",
      "  Training Loss: 0.5321574211120605, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236371159553528, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 768:\n",
      "  Training Loss: 0.5321515798568726, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235329866409302, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 769:\n",
      "  Training Loss: 0.5321666598320007, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52347731590271, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 770:\n",
      "  Training Loss: 0.5321570634841919, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52350252866745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 771:\n",
      "  Training Loss: 0.5321497917175293, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236071944236755, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 772:\n",
      "  Training Loss: 0.5321424603462219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235722661018372, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 773:\n",
      "  Training Loss: 0.5321751832962036, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236416459083557, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 774:\n",
      "  Training Loss: 0.5321301817893982, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235427021980286, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 775:\n",
      "  Training Loss: 0.5321653485298157, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235721468925476, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 776:\n",
      "  Training Loss: 0.5321664810180664, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234735608100891, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 777:\n",
      "  Training Loss: 0.5321473479270935, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236376523971558, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 778:\n",
      "  Training Loss: 0.5320947766304016, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234548449516296, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 779:\n",
      "  Training Loss: 0.5321489572525024, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234922170639038, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 780:\n",
      "  Training Loss: 0.5321324467658997, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236386656761169, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 781:\n",
      "  Training Loss: 0.5321334600448608, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238285660743713, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 782:\n",
      "  Training Loss: 0.5321571826934814, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235648155212402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 783:\n",
      "  Training Loss: 0.5321609973907471, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236448049545288, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 784:\n",
      "  Training Loss: 0.5320913195610046, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52350252866745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 785:\n",
      "  Training Loss: 0.5320836305618286, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523702085018158, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 786:\n",
      "  Training Loss: 0.5321508049964905, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235859155654907, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 787:\n",
      "  Training Loss: 0.5321223139762878, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234782695770264, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 788:\n",
      "  Training Loss: 0.5321579575538635, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234748721122742, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 789:\n",
      "  Training Loss: 0.5321528911590576, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235318541526794, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 790:\n",
      "  Training Loss: 0.5321589708328247, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235635042190552, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 791:\n",
      "  Training Loss: 0.5321135520935059, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234577059745789, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 792:\n",
      "  Training Loss: 0.5321611166000366, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235053896903992, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 793:\n",
      "  Training Loss: 0.5321680307388306, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235694646835327, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 794:\n",
      "  Training Loss: 0.5321245789527893, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236378312110901, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 795:\n",
      "  Training Loss: 0.532111406326294, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235753059387207, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 796:\n",
      "  Training Loss: 0.5321451425552368, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235695242881775, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 797:\n",
      "  Training Loss: 0.5321541428565979, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234764814376831, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 798:\n",
      "  Training Loss: 0.5321638584136963, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236377716064453, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 799:\n",
      "  Training Loss: 0.5321583151817322, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236008763313293, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 800:\n",
      "  Training Loss: 0.5321374535560608, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523695170879364, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 801:\n",
      "  Training Loss: 0.5321663022041321, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523637056350708, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 802:\n",
      "  Training Loss: 0.5321475863456726, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236263275146484, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 803:\n",
      "  Training Loss: 0.5321473479270935, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235236883163452, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 804:\n",
      "  Training Loss: 0.532153844833374, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235962867736816, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 805:\n",
      "  Training Loss: 0.5321467518806458, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235422849655151, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 806:\n",
      "  Training Loss: 0.5321443676948547, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235691070556641, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 807:\n",
      "  Training Loss: 0.5321260690689087, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236612558364868, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 808:\n",
      "  Training Loss: 0.5321544408798218, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236635208129883, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 809:\n",
      "  Training Loss: 0.532138466835022, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234869122505188, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 810:\n",
      "  Training Loss: 0.5321637392044067, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235731601715088, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 811:\n",
      "  Training Loss: 0.5321357846260071, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235605239868164, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 812:\n",
      "  Training Loss: 0.5321458578109741, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235950350761414, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 813:\n",
      "  Training Loss: 0.5321332812309265, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235676765441895, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 814:\n",
      "  Training Loss: 0.5321589708328247, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235466361045837, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 815:\n",
      "  Training Loss: 0.5321547985076904, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523505449295044, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 816:\n",
      "  Training Loss: 0.5321351289749146, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234919190406799, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 817:\n",
      "  Training Loss: 0.532139778137207, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523631751537323, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 818:\n",
      "  Training Loss: 0.5321182012557983, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238263010978699, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 819:\n",
      "  Training Loss: 0.5321516394615173, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236068964004517, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 820:\n",
      "  Training Loss: 0.5321601629257202, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235716104507446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 821:\n",
      "  Training Loss: 0.532143235206604, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523644745349884, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 822:\n",
      "  Training Loss: 0.532167911529541, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235307812690735, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 823:\n",
      "  Training Loss: 0.5321335792541504, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236164331436157, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 824:\n",
      "  Training Loss: 0.532143235206604, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237670540809631, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 825:\n",
      "  Training Loss: 0.5321447253227234, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52351313829422, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 826:\n",
      "  Training Loss: 0.5321151614189148, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237550735473633, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 827:\n",
      "  Training Loss: 0.5321643948554993, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235714912414551, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 828:\n",
      "  Training Loss: 0.5321477055549622, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235280990600586, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 829:\n",
      "  Training Loss: 0.5321452021598816, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235868692398071, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 830:\n",
      "  Training Loss: 0.5321316123008728, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523735523223877, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 831:\n",
      "  Training Loss: 0.5321493148803711, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234823226928711, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 832:\n",
      "  Training Loss: 0.5321352481842041, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234576463699341, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 833:\n",
      "  Training Loss: 0.5321714282035828, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523544430732727, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 834:\n",
      "  Training Loss: 0.53214430809021, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235315561294556, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 835:\n",
      "  Training Loss: 0.5321328043937683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237454175949097, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 836:\n",
      "  Training Loss: 0.532150387763977, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234922170639038, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 837:\n",
      "  Training Loss: 0.53212571144104, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236826539039612, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 838:\n",
      "  Training Loss: 0.5321329832077026, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235709547996521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 839:\n",
      "  Training Loss: 0.5321239829063416, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523748517036438, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 840:\n",
      "  Training Loss: 0.532180905342102, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235688090324402, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 841:\n",
      "  Training Loss: 0.5321458578109741, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523514449596405, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 842:\n",
      "  Training Loss: 0.5321301817893982, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235252380371094, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 843:\n",
      "  Training Loss: 0.532130777835846, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237002968788147, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 844:\n",
      "  Training Loss: 0.5321661829948425, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236752033233643, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 845:\n",
      "  Training Loss: 0.5321508646011353, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235331058502197, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 846:\n",
      "  Training Loss: 0.5321460366249084, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729217529297, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 847:\n",
      "  Training Loss: 0.5321345925331116, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234836339950562, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 848:\n",
      "  Training Loss: 0.5321551561355591, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235275030136108, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 849:\n",
      "  Training Loss: 0.532137930393219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236794948577881, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 850:\n",
      "  Training Loss: 0.5321319699287415, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235509276390076, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 851:\n",
      "  Training Loss: 0.532151997089386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523590087890625, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 852:\n",
      "  Training Loss: 0.5321053266525269, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238180160522461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 853:\n",
      "  Training Loss: 0.5321840643882751, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236297249794006, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 854:\n",
      "  Training Loss: 0.5321685671806335, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235771536827087, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 855:\n",
      "  Training Loss: 0.5321477651596069, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236153602600098, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 856:\n",
      "  Training Loss: 0.5321000814437866, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523836612701416, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 857:\n",
      "  Training Loss: 0.5321882963180542, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236361026763916, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 858:\n",
      "  Training Loss: 0.5321450233459473, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523552656173706, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 859:\n",
      "  Training Loss: 0.5321593880653381, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235407948493958, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 860:\n",
      "  Training Loss: 0.5321686863899231, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235401391983032, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 861:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236100554466248, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 862:\n",
      "  Training Loss: 0.532092273235321, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234716534614563, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 863:\n",
      "  Training Loss: 0.5321552157402039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235479474067688, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 864:\n",
      "  Training Loss: 0.532155454158783, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237299203872681, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 865:\n",
      "  Training Loss: 0.5321463942527771, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235434174537659, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 866:\n",
      "  Training Loss: 0.5321453809738159, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234779119491577, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 867:\n",
      "  Training Loss: 0.5321531295776367, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235615968704224, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 868:\n",
      "  Training Loss: 0.5321276783943176, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236011147499084, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 869:\n",
      "  Training Loss: 0.5321481823921204, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235070586204529, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 870:\n",
      "  Training Loss: 0.5321279764175415, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234751105308533, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 871:\n",
      "  Training Loss: 0.5321720242500305, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235389471054077, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 872:\n",
      "  Training Loss: 0.5321552157402039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235764384269714, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 873:\n",
      "  Training Loss: 0.5321416854858398, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235395431518555, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 874:\n",
      "  Training Loss: 0.5321476459503174, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235544443130493, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 875:\n",
      "  Training Loss: 0.5321429371833801, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523603618144989, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 876:\n",
      "  Training Loss: 0.5321506857872009, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235593914985657, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 877:\n",
      "  Training Loss: 0.5321420431137085, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235841870307922, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 878:\n",
      "  Training Loss: 0.5321337580680847, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236225724220276, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 879:\n",
      "  Training Loss: 0.5321641564369202, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236597061157227, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 880:\n",
      "  Training Loss: 0.5321328043937683, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236469507217407, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 881:\n",
      "  Training Loss: 0.5321171283721924, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523457944393158, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 882:\n",
      "  Training Loss: 0.5321325659751892, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236775279045105, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 883:\n",
      "  Training Loss: 0.5321522355079651, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234785079956055, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 884:\n",
      "  Training Loss: 0.5321577787399292, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236057639122009, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 885:\n",
      "  Training Loss: 0.5321483612060547, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235479474067688, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 886:\n",
      "  Training Loss: 0.5321599245071411, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235166549682617, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 887:\n",
      "  Training Loss: 0.5321500897407532, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235753655433655, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 888:\n",
      "  Training Loss: 0.5321502685546875, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235933065414429, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 889:\n",
      "  Training Loss: 0.5321435928344727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235149264335632, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 890:\n",
      "  Training Loss: 0.5321429371833801, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236006379127502, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 891:\n",
      "  Training Loss: 0.532121479511261, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234639644622803, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 892:\n",
      "  Training Loss: 0.5320965051651001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5238286256790161, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 893:\n",
      "  Training Loss: 0.5321449637413025, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237687826156616, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 894:\n",
      "  Training Loss: 0.5321432948112488, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235061049461365, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 895:\n",
      "  Training Loss: 0.5321575999259949, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234978795051575, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 896:\n",
      "  Training Loss: 0.5321435928344727, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234602093696594, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 897:\n",
      "  Training Loss: 0.532145082950592, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237488746643066, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 898:\n",
      "  Training Loss: 0.5321642160415649, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.52359938621521, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 899:\n",
      "  Training Loss: 0.5321466326713562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236612558364868, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 900:\n",
      "  Training Loss: 0.5321238040924072, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234706997871399, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 901:\n",
      "  Training Loss: 0.5321476459503174, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235773324966431, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 902:\n",
      "  Training Loss: 0.5321326851844788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235849022865295, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 903:\n",
      "  Training Loss: 0.532134473323822, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236532688140869, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 904:\n",
      "  Training Loss: 0.5321445465087891, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523674726486206, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 905:\n",
      "  Training Loss: 0.5321424603462219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234581828117371, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 906:\n",
      "  Training Loss: 0.5321596264839172, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234903693199158, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 907:\n",
      "  Training Loss: 0.5321376323699951, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237470865249634, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 908:\n",
      "  Training Loss: 0.5321657061576843, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235463976860046, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 909:\n",
      "  Training Loss: 0.5321358442306519, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235478281974792, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 910:\n",
      "  Training Loss: 0.532131016254425, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523719847202301, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 911:\n",
      "  Training Loss: 0.5321425199508667, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234687328338623, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 912:\n",
      "  Training Loss: 0.5321528315544128, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235115885734558, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 913:\n",
      "  Training Loss: 0.5321545004844666, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235703587532043, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 914:\n",
      "  Training Loss: 0.5321062803268433, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234502553939819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 915:\n",
      "  Training Loss: 0.5321909785270691, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235467553138733, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 916:\n",
      "  Training Loss: 0.5321464538574219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236179828643799, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 917:\n",
      "  Training Loss: 0.5321142077445984, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234591364860535, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 918:\n",
      "  Training Loss: 0.5321419835090637, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234981775283813, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 919:\n",
      "  Training Loss: 0.5321625471115112, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234997272491455, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 920:\n",
      "  Training Loss: 0.5321627855300903, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235657095909119, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 921:\n",
      "  Training Loss: 0.5321494936943054, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234691500663757, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 922:\n",
      "  Training Loss: 0.5321716666221619, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236074924468994, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 923:\n",
      "  Training Loss: 0.5321519374847412, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235193371772766, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 924:\n",
      "  Training Loss: 0.5321518778800964, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234760642051697, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 925:\n",
      "  Training Loss: 0.5321058034896851, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523771345615387, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 926:\n",
      "  Training Loss: 0.5321811437606812, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236013531684875, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 927:\n",
      "  Training Loss: 0.5321450233459473, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235890746116638, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 928:\n",
      "  Training Loss: 0.5321305394172668, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236555337905884, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 929:\n",
      "  Training Loss: 0.5321254134178162, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234535336494446, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 930:\n",
      "  Training Loss: 0.5321722030639648, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523568868637085, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 931:\n",
      "  Training Loss: 0.532149076461792, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236337184906006, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 932:\n",
      "  Training Loss: 0.532146692276001, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235206484794617, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 933:\n",
      "  Training Loss: 0.5321511626243591, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234799385070801, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 934:\n",
      "  Training Loss: 0.5321522355079651, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523655891418457, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 935:\n",
      "  Training Loss: 0.5321252942085266, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234643816947937, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 936:\n",
      "  Training Loss: 0.5321463942527771, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235738754272461, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 937:\n",
      "  Training Loss: 0.5321498513221741, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235760807991028, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 938:\n",
      "  Training Loss: 0.5321343541145325, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234634876251221, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 939:\n",
      "  Training Loss: 0.5321540832519531, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236051678657532, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 940:\n",
      "  Training Loss: 0.5321373343467712, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234777331352234, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 941:\n",
      "  Training Loss: 0.5321053266525269, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523713231086731, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 942:\n",
      "  Training Loss: 0.5321471095085144, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523474931716919, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 943:\n",
      "  Training Loss: 0.532131016254425, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234643816947937, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 944:\n",
      "  Training Loss: 0.5321722626686096, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235193371772766, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 945:\n",
      "  Training Loss: 0.5321430563926697, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234912633895874, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 946:\n",
      "  Training Loss: 0.5321506261825562, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235145092010498, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 947:\n",
      "  Training Loss: 0.532142698764801, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237407684326172, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 948:\n",
      "  Training Loss: 0.5321590900421143, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236291885375977, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 949:\n",
      "  Training Loss: 0.5321572422981262, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236523151397705, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 950:\n",
      "  Training Loss: 0.5321338772773743, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237321853637695, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 951:\n",
      "  Training Loss: 0.5321328639984131, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235179662704468, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 952:\n",
      "  Training Loss: 0.5321560502052307, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235223770141602, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 953:\n",
      "  Training Loss: 0.5321351885795593, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235937237739563, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 954:\n",
      "  Training Loss: 0.5321474075317383, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235563516616821, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 955:\n",
      "  Training Loss: 0.5321406722068787, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235441327095032, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 956:\n",
      "  Training Loss: 0.5321372151374817, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235921740531921, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 957:\n",
      "  Training Loss: 0.5321458578109741, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234971642494202, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 958:\n",
      "  Training Loss: 0.532115638256073, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236154794692993, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 959:\n",
      "  Training Loss: 0.5321468710899353, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523513674736023, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 960:\n",
      "  Training Loss: 0.5321561098098755, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234800577163696, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 961:\n",
      "  Training Loss: 0.5321663022041321, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234741568565369, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 962:\n",
      "  Training Loss: 0.5321723818778992, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236696004867554, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 963:\n",
      "  Training Loss: 0.532151997089386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235424041748047, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 964:\n",
      "  Training Loss: 0.5321447253227234, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235303044319153, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 965:\n",
      "  Training Loss: 0.5321387052536011, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236033201217651, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 966:\n",
      "  Training Loss: 0.5321403741836548, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236726999282837, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 967:\n",
      "  Training Loss: 0.5321329236030579, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235106945037842, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 968:\n",
      "  Training Loss: 0.5321583151817322, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234760046005249, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 969:\n",
      "  Training Loss: 0.5321682095527649, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235630869865417, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 970:\n",
      "  Training Loss: 0.5321637988090515, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236186385154724, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 971:\n",
      "  Training Loss: 0.5321399569511414, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236602425575256, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 972:\n",
      "  Training Loss: 0.5321376919746399, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236376523971558, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 973:\n",
      "  Training Loss: 0.5321512222290039, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235595107078552, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 974:\n",
      "  Training Loss: 0.5321390628814697, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236243605613708, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 975:\n",
      "  Training Loss: 0.5321453213691711, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235196948051453, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 976:\n",
      "  Training Loss: 0.5321360230445862, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235089659690857, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 977:\n",
      "  Training Loss: 0.5321499109268188, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523735761642456, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 978:\n",
      "  Training Loss: 0.5321668386459351, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235723257064819, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 979:\n",
      "  Training Loss: 0.5321455597877502, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235729813575745, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 980:\n",
      "  Training Loss: 0.5321417450904846, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235196948051453, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 981:\n",
      "  Training Loss: 0.5321425795555115, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523496150970459, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 982:\n",
      "  Training Loss: 0.532151997089386, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236917734146118, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 983:\n",
      "  Training Loss: 0.5321431159973145, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235345959663391, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 984:\n",
      "  Training Loss: 0.5321308970451355, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523592472076416, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 985:\n",
      "  Training Loss: 0.5321413278579712, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235770344734192, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 986:\n",
      "  Training Loss: 0.5320404171943665, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234346389770508, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 987:\n",
      "  Training Loss: 0.5321941375732422, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236614942550659, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 988:\n",
      "  Training Loss: 0.5321527719497681, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236278772354126, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 989:\n",
      "  Training Loss: 0.5321518182754517, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235884189605713, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 990:\n",
      "  Training Loss: 0.532152533531189, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5236766338348389, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 991:\n",
      "  Training Loss: 0.5321286916732788, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234925150871277, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 992:\n",
      "  Training Loss: 0.5321520566940308, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235322713851929, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 993:\n",
      "  Training Loss: 0.532119631767273, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5237426161766052, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 994:\n",
      "  Training Loss: 0.5321441292762756, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235825181007385, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 995:\n",
      "  Training Loss: 0.532137930393219, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234938859939575, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 996:\n",
      "  Training Loss: 0.53214031457901, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5234886407852173, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 997:\n",
      "  Training Loss: 0.5321677923202515, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235438942909241, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 998:\n",
      "  Training Loss: 0.5321358442306519, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235558152198792, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 999:\n",
      "  Training Loss: 0.5321440100669861, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.523459792137146, Validation Accuracy: 0.7827280163764954\n",
      "Epoch 1000:\n",
      "  Training Loss: 0.5321654081344604, Training Accuracy: 0.7758657336235046\n",
      "  Validation Loss: 0.5235269665718079, Validation Accuracy: 0.7827280163764954\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(len(train_loss)):\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Training Loss: {train_loss[epoch]}, Training Accuracy: {train_accuracy[epoch]}\")\n",
    "    print(f\"  Validation Loss: {valid_loss[epoch]}, Validation Accuracy: {valid_accuracy[epoch]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for loss in (\"loss\", \"val_loss\"):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    opt_names = \"SGD Momentum AdaGrad Adam\"\n",
    "    for history, opt_name in zip((historySGD, historySGDM,\n",
    "                                  historyADA, historyADM,\n",
    "                                 ),\n",
    "                                 opt_names.split()):\n",
    "        plt.plot(history.history[loss], label=f\"{opt_name}\", linewidth=3)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel({\"loss\": \"Training loss\", \"val_loss\": \"Validation loss\"}[loss])\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.axis([0, 10, 0.5, 1])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
